{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment: AlphaSnake 10x10 Vast.ai Paper-Fidelity\n",
        "\n",
        "## Objetivo\n",
        "Entrenar un agente Snake estilo AlphaZero (MCTS + Policy/Value Net) reproduciendo condiciones del paper para 10x10, y exportar ONNX para integración directa con `games/snake/ai.html`.\n",
        "\n",
        "## Criterios de éxito\n",
        "- Contrato de entorno paper: 10x10, rewards `+1/-1/0`, sin shaping.\n",
        "- Champion update por `head-to-head MCTS > 55%`.\n",
        "- Estrategia faseada en Vast.ai (warm-up -> paper) con resume por checkpoint.\n",
        "- Export ONNX verificado para uso en navegador.\n",
        "\n",
        "**Takeaway:** este notebook es el artefacto reproducible end-to-end para entrenar y desplegar AlphaSnake 10x10.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# Cell 1: Setup — Vast.ai / Colab / local\n",
        "# ============================================================\n",
        "import os\n",
        "import torch\n",
        "\n",
        "ROOT_DIR = os.getcwd()\n",
        "if os.path.exists('/workspace'):\n",
        "    # Vast.ai (persistente)\n",
        "    SAVE_DIR = '/workspace/alphasnake'\n",
        "elif os.path.exists('/content'):\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        SAVE_DIR = '/content/drive/MyDrive/alphasnake'\n",
        "    except ImportError:\n",
        "        SAVE_DIR = '/content/alphasnake'\n",
        "else:\n",
        "    SAVE_DIR = os.path.join(ROOT_DIR, 'alphasnake')\n",
        "\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "print(f'PyTorch {torch.__version__} | CUDA: {torch.cuda.is_available()} | Checkpoints: {SAVE_DIR}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup completado\n",
        "Se detecta entorno (`/workspace`, Colab o local) y se define `SAVE_DIR` persistente para checkpoints.\n",
        "\n",
        "**Takeaway:** todos los artefactos de entrenamiento quedan centralizados y resumibles.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# Cell 2: Imports y Configuracion (paper-fidelity + faseado)\n",
        "# ============================================================\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import pickle\n",
        "import subprocess\n",
        "import threading\n",
        "import multiprocessing\n",
        "import dataclasses\n",
        "from dataclasses import dataclass\n",
        "from collections import deque\n",
        "from queue import Empty\n",
        "from typing import Optional\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Perfil de corrida (Vast.ai RTX 4060 Ti + 4.8 vCPU)\n",
        "# -----------------------------------------------------------\n",
        "PRESET = 'vastai_phased'  # 'paper_only' | 'vastai_phased' | 'smoke'\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # Entorno\n",
        "    board_size: int = 10\n",
        "    max_steps: int = 1000\n",
        "\n",
        "    # Red\n",
        "    net_channels: int = 64\n",
        "    net_blocks: int = 6\n",
        "\n",
        "    # MCTS (paper)\n",
        "    num_simulations: int = 400\n",
        "    c_puct: float = 1.0\n",
        "    dir_alpha: float = 0.03\n",
        "    dir_eps: float = 0.25\n",
        "    temp_init: float = 1.0\n",
        "    temp_final: float = 0.0\n",
        "    temp_decay_move: int = 30\n",
        "    food_samples: int = 8\n",
        "\n",
        "    # Training\n",
        "    lr: float = 1e-3\n",
        "    weight_decay: float = 1e-4\n",
        "    batch_size: int = 128\n",
        "    buffer_size: int = 200_000\n",
        "    epochs_per_iter: int = 10\n",
        "\n",
        "    # Self-play / evaluación efectiva por iteración\n",
        "    games_per_iter: int = 1000\n",
        "    eval_games: int = 200\n",
        "\n",
        "    # Iteraciones\n",
        "    max_iterations: int = 37\n",
        "    warmup_iterations: int = 12\n",
        "\n",
        "    # Fase A (warm-up)\n",
        "    warmup_num_simulations: int = 220\n",
        "    warmup_games_per_iter: int = 360\n",
        "    warmup_eval_games: int = 80\n",
        "    warmup_food_samples: int = 6\n",
        "\n",
        "    # Fase B (paper)\n",
        "    paper_num_simulations: int = 400\n",
        "    paper_games_per_iter: int = 1000\n",
        "    paper_eval_games: int = 200\n",
        "    paper_food_samples: int = 8\n",
        "\n",
        "    # Champion / evaluación\n",
        "    accept_threshold: float = 0.55  # new model must win >55% head-to-head\n",
        "    target_win_rate: float = 0.94\n",
        "    target_win_rate_patience: int = 3\n",
        "\n",
        "    # Infra\n",
        "    save_dir: str = SAVE_DIR\n",
        "    checkpoint_interval: int = 1\n",
        "    selfplay_workers: int = 4\n",
        "    selfplay_backend: str = 'process'  # 'process' recomendado para evitar GIL\n",
        "    inference_batch_size: int = 128\n",
        "    inference_timeout_ms: int = 8\n",
        "    use_amp: bool = True\n",
        "\n",
        "    # Logging\n",
        "    verbose_game_updates: bool = False\n",
        "    game_log_interval: int = 1\n",
        "    game_tick_moves: int = 100\n",
        "    heartbeat_seconds: int = 30\n",
        "\n",
        "\n",
        "def build_config(preset: str) -> Config:\n",
        "    if preset == 'paper_only':\n",
        "        return Config(\n",
        "            max_iterations=37,\n",
        "            warmup_iterations=0,\n",
        "            warmup_num_simulations=400,\n",
        "            warmup_games_per_iter=1000,\n",
        "            warmup_eval_games=200,\n",
        "            warmup_food_samples=8,\n",
        "            paper_num_simulations=400,\n",
        "            paper_games_per_iter=1000,\n",
        "            paper_eval_games=200,\n",
        "            paper_food_samples=8,\n",
        "        )\n",
        "\n",
        "    if preset == 'smoke':\n",
        "        return Config(\n",
        "            max_iterations=2,\n",
        "            warmup_iterations=2,\n",
        "            warmup_num_simulations=50,\n",
        "            warmup_games_per_iter=20,\n",
        "            warmup_eval_games=10,\n",
        "            warmup_food_samples=2,\n",
        "            paper_num_simulations=50,\n",
        "            paper_games_per_iter=20,\n",
        "            paper_eval_games=10,\n",
        "            paper_food_samples=2,\n",
        "            buffer_size=25_000,\n",
        "            inference_batch_size=32,\n",
        "        )\n",
        "\n",
        "    # vastai_phased (recomendado)\n",
        "    return Config()\n",
        "\n",
        "\n",
        "config = build_config(PRESET)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "\n",
        "def get_iteration_config(cfg: Config, iteration: int) -> Config:\n",
        "    # Devuelve config efectiva para la iteración (1-based).\n",
        "    if iteration <= cfg.warmup_iterations:\n",
        "        return dataclasses.replace(\n",
        "            cfg,\n",
        "            num_simulations=cfg.warmup_num_simulations,\n",
        "            games_per_iter=cfg.warmup_games_per_iter,\n",
        "            eval_games=cfg.warmup_eval_games,\n",
        "            food_samples=cfg.warmup_food_samples,\n",
        "        )\n",
        "\n",
        "    return dataclasses.replace(\n",
        "        cfg,\n",
        "        num_simulations=cfg.paper_num_simulations,\n",
        "        games_per_iter=cfg.paper_games_per_iter,\n",
        "        eval_games=cfg.paper_eval_games,\n",
        "        food_samples=cfg.paper_food_samples,\n",
        "    )\n",
        "\n",
        "print(f'Device: {device}')\n",
        "print(f'Preset: {PRESET}')\n",
        "print('Config base:')\n",
        "print(dataclasses.asdict(config))\n",
        "print('Iter 1 cfg:', dataclasses.asdict(get_iteration_config(config, 1)))\n",
        "print('Iter final cfg:', dataclasses.asdict(get_iteration_config(config, config.max_iterations)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Entorno Snake 10x10\n",
        "Contrato exacto: acciones discretas 4, reversa prohibida, observación `(4,10,10)` y rewards `+1/-1/0`.\n",
        "\n",
        "**Takeaway:** mantener este contrato es clave para reproducir comportamiento tipo paper.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 3: Entorno Snake\n",
        "# ============================================================\n",
        "\n",
        "class SnakeEnv:\n",
        "    \"\"\"\n",
        "    Snake environment estilo AlphaSnake.\n",
        "    Grid board_size x board_size, 4 acciones, observacion (4, H, W).\n",
        "    \"\"\"\n",
        "    # 0=UP, 1=DOWN, 2=LEFT, 3=RIGHT\n",
        "    ACTIONS = {0: (0, -1), 1: (0, 1), 2: (-1, 0), 3: (1, 0)}\n",
        "    OPPOSITES = {0: 1, 1: 0, 2: 3, 3: 2}\n",
        "    ACTION_NAMES = {0: 'UP', 1: 'DOWN', 2: 'LEFT', 3: 'RIGHT'}\n",
        "\n",
        "    def __init__(self, board_size=10, max_steps=1000):\n",
        "        self.board_size = board_size\n",
        "        self.max_steps = max_steps\n",
        "        self.max_score = board_size * board_size - 3  # 97 para 10x10\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        mid = self.board_size // 2\n",
        "        # Snake empieza en el centro, apuntando a la derecha\n",
        "        self.snake = deque([(mid, mid), (mid - 1, mid), (mid - 2, mid)])\n",
        "        self.direction = 3  # RIGHT\n",
        "        self.food = None\n",
        "        self.done = False\n",
        "        self.steps = 0\n",
        "        self.score = 0\n",
        "        self._place_food()\n",
        "        return self.get_state()\n",
        "\n",
        "    def _get_free_cells(self):\n",
        "        snake_set = set(self.snake)\n",
        "        return [(x, y) for y in range(self.board_size)\n",
        "                for x in range(self.board_size)\n",
        "                if (x, y) not in snake_set]\n",
        "\n",
        "    def _place_food(self):\n",
        "        free = self._get_free_cells()\n",
        "        if free:\n",
        "            self.food = random.choice(free)\n",
        "        else:\n",
        "            # Tablero lleno = victoria\n",
        "            self.food = None\n",
        "            self.done = True\n",
        "\n",
        "    def get_state(self):\n",
        "        \"\"\"Retorna observacion (4, board_size, board_size) float32.\"\"\"\n",
        "        s = np.zeros((4, self.board_size, self.board_size), dtype=np.float32)\n",
        "        # Canal 0: cuerpo\n",
        "        for x, y in self.snake:\n",
        "            s[0, y, x] = 1.0\n",
        "        # Canal 1: cabeza\n",
        "        hx, hy = self.snake[0]\n",
        "        s[1, hy, hx] = 1.0\n",
        "        # Canal 2: comida\n",
        "        if self.food:\n",
        "            fx, fy = self.food\n",
        "            s[2, fy, fx] = 1.0\n",
        "        # Canal 3: direccion (valor constante normalizado)\n",
        "        # UP=0.25, DOWN=0.5, LEFT=0.75, RIGHT=1.0\n",
        "        s[3, :, :] = (self.direction + 1) / 4.0\n",
        "        return s\n",
        "\n",
        "    def valid_actions(self):\n",
        "        \"\"\"Acciones validas (excluye reversa directa).\"\"\"\n",
        "        rev = self.OPPOSITES[self.direction]\n",
        "        return [a for a in range(4) if a != rev]\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Ejecuta accion. Retorna (reward, done).\n",
        "        reward: +1.0 comida, -1.0 muerte, 0.0 otro.\n",
        "        \"\"\"\n",
        "        if self.done:\n",
        "            return 0.0, True\n",
        "\n",
        "        # Prohibir reversa directa: si intenta reversa, mantener direccion\n",
        "        if action == self.OPPOSITES[self.direction]:\n",
        "            action = self.direction\n",
        "\n",
        "        self.direction = action\n",
        "        dx, dy = self.ACTIONS[action]\n",
        "        hx, hy = self.snake[0]\n",
        "        nx, ny = hx + dx, hy + dy\n",
        "\n",
        "        # Colision con pared\n",
        "        if nx < 0 or nx >= self.board_size or ny < 0 or ny >= self.board_size:\n",
        "            self.done = True\n",
        "            return -1.0, True\n",
        "\n",
        "        # Verificar si comera comida (antes de mover)\n",
        "        ate_food = self.food is not None and (nx, ny) == self.food\n",
        "\n",
        "        # Colision con cuerpo\n",
        "        # Si NO come comida, la cola se movera, asi que es valido ir a la pos actual de la cola\n",
        "        body_set = set(self.snake)\n",
        "        if not ate_food:\n",
        "            body_set.discard(self.snake[-1])\n",
        "        if (nx, ny) in body_set:\n",
        "            self.done = True\n",
        "            return -1.0, True\n",
        "\n",
        "        # Mover\n",
        "        self.snake.appendleft((nx, ny))\n",
        "        if ate_food:\n",
        "            self.score += 1\n",
        "            self._place_food()\n",
        "            if self.done:  # _place_food puso done=True si el tablero esta lleno\n",
        "                return 1.0, True\n",
        "        else:\n",
        "            self.snake.pop()\n",
        "\n",
        "        self.steps += 1\n",
        "        if self.steps >= self.max_steps:\n",
        "            self.done = True\n",
        "            return 0.0, True\n",
        "\n",
        "        return (1.0 if ate_food else 0.0), False\n",
        "\n",
        "    def is_win(self):\n",
        "        \"\"\"True si la serpiente lleno todo el tablero.\"\"\"\n",
        "        return len(self.snake) >= self.board_size ** 2\n",
        "\n",
        "    def clone(self):\n",
        "        \"\"\"Copia eficiente del entorno.\"\"\"\n",
        "        env = SnakeEnv.__new__(SnakeEnv)\n",
        "        env.board_size = self.board_size\n",
        "        env.max_steps = self.max_steps\n",
        "        env.max_score = self.max_score\n",
        "        env.snake = deque(self.snake)\n",
        "        env.direction = self.direction\n",
        "        env.food = self.food\n",
        "        env.done = self.done\n",
        "        env.steps = self.steps\n",
        "        env.score = self.score\n",
        "        return env\n",
        "\n",
        "# --- Test rapido ---\n",
        "env = SnakeEnv(10, 1000)\n",
        "s = env.reset()\n",
        "print(f\"State shape: {s.shape}\")\n",
        "print(f\"Snake length: {len(env.snake)}, food: {env.food}\")\n",
        "print(f\"Valid actions: {[env.ACTION_NAMES[a] for a in env.valid_actions()]}\")\n",
        "\n",
        "# Jugar unos pasos random\n",
        "for _ in range(20):\n",
        "    a = random.choice(env.valid_actions())\n",
        "    r, done = env.step(a)\n",
        "    if done:\n",
        "        print(f\"Game over! Score: {env.score}, Win: {env.is_win()}\")\n",
        "        break\n",
        "else:\n",
        "    print(f\"After 20 steps: score={env.score}, length={len(env.snake)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Red Policy/Value\n",
        "ResNet 6 bloques (64 canales) con cabezas policy (4 acciones) y value `tanh`.\n",
        "\n",
        "**Takeaway:** arquitectura compacta y estable para 10x10, consistente con configuración paper reducida.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 4: Red Neuronal — AlphaSnakeNet\n",
        "# ============================================================\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    \"\"\"Bloque residual: Conv→BN→ReLU→Conv→BN→skip→ReLU\"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(channels)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out = F.relu(out + residual)\n",
        "        return out\n",
        "\n",
        "\n",
        "class AlphaSnakeNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Policy + Value network estilo AlphaZero.\n",
        "    Input:  (batch, 4, board_size, board_size)\n",
        "    Output: policy (batch, 4), value (batch, 1)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=4, num_blocks=6, channels=64, board_size=10):\n",
        "        super().__init__()\n",
        "        self.board_size = board_size\n",
        "\n",
        "        # Stem\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, channels, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Residual tower\n",
        "        self.res_tower = nn.Sequential(*[ResBlock(channels) for _ in range(num_blocks)])\n",
        "\n",
        "        # Policy head\n",
        "        self.policy_conv = nn.Conv2d(channels, 2, 1, bias=False)\n",
        "        self.policy_bn = nn.BatchNorm2d(2)\n",
        "        self.policy_fc = nn.Linear(2 * board_size * board_size, 4)\n",
        "\n",
        "        # Value head\n",
        "        self.value_conv = nn.Conv2d(channels, 1, 1, bias=False)\n",
        "        self.value_bn = nn.BatchNorm2d(1)\n",
        "        self.value_fc1 = nn.Linear(board_size * board_size, 64)\n",
        "        self.value_fc2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shared trunk\n",
        "        x = self.stem(x)\n",
        "        x = self.res_tower(x)\n",
        "\n",
        "        # Policy\n",
        "        p = F.relu(self.policy_bn(self.policy_conv(x)))\n",
        "        p = p.view(p.size(0), -1)\n",
        "        p = F.softmax(self.policy_fc(p), dim=1)\n",
        "\n",
        "        # Value\n",
        "        v = F.relu(self.value_bn(self.value_conv(x)))\n",
        "        v = v.view(v.size(0), -1)\n",
        "        v = F.relu(self.value_fc1(v))\n",
        "        v = torch.tanh(self.value_fc2(v))\n",
        "\n",
        "        return p, v\n",
        "\n",
        "# --- Test ---\n",
        "net = AlphaSnakeNet(\n",
        "    in_channels=4,\n",
        "    num_blocks=config.net_blocks,\n",
        "    channels=config.net_channels,\n",
        "    board_size=config.board_size\n",
        ").to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in net.parameters())\n",
        "print(f\"AlphaSnakeNet: {total_params:,} parametros\")\n",
        "\n",
        "dummy = torch.randn(1, 4, config.board_size, config.board_size).to(device)\n",
        "pol, val = net(dummy)\n",
        "print(f\"Policy shape: {pol.shape}, sum={pol.sum().item():.4f}\")\n",
        "print(f\"Value shape: {val.shape}, val={val.item():.4f}\")\n",
        "del net, dummy  # liberar memoria\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MCTS con estocasticidad de comida\n",
        "UCB clásico AlphaZero, Dirichlet en raíz y `food_samples` para robustez ante spawn aleatorio.\n",
        "\n",
        "**Takeaway:** esta parte determina gran parte del rendimiento final (>90%).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 5: MCTS — Monte Carlo Tree Search\n",
        "# ============================================================\n",
        "\n",
        "class MCTSNode:\n",
        "    \"\"\"Nodo del arbol MCTS.\"\"\"\n",
        "    __slots__ = ['prior', 'visit_count', 'value_sum', 'children',\n",
        "                 'is_expanded', 'env', 'food_eaten']\n",
        "\n",
        "    def __init__(self, prior=0.0):\n",
        "        self.prior = prior\n",
        "        self.visit_count = 0\n",
        "        self.value_sum = 0.0\n",
        "        self.children = {}        # action (int) -> MCTSNode\n",
        "        self.is_expanded = False\n",
        "        self.env = None           # SnakeEnv snapshot\n",
        "        self.food_eaten = False   # True si la transicion a este nodo comio comida\n",
        "\n",
        "    def value(self):\n",
        "        if self.visit_count == 0:\n",
        "            return 0.0\n",
        "        return self.value_sum / self.visit_count\n",
        "\n",
        "\n",
        "class MCTS:\n",
        "    \"\"\"\n",
        "    Monte Carlo Tree Search estilo AlphaZero con soporte\n",
        "    para estocasticidad de comida (food sampling).\n",
        "    Acepta predict_fn y predict_batch_fn para permitir\n",
        "    batching de inferencia desde multiples hilos.\n",
        "    \"\"\"\n",
        "    def __init__(self, predict_fn, cfg, predict_batch_fn=None):\n",
        "        self.predict_fn = predict_fn\n",
        "        self.predict_batch_fn = predict_batch_fn or (lambda states: [predict_fn(s) for s in states])\n",
        "        self.c_puct = cfg.c_puct\n",
        "        self.num_simulations = cfg.num_simulations\n",
        "        self.dir_alpha = cfg.dir_alpha\n",
        "        self.dir_eps = cfg.dir_eps\n",
        "        self.food_samples = cfg.food_samples\n",
        "\n",
        "    def _predict(self, state_np):\n",
        "        \"\"\"Inferir policy y value.\"\"\"\n",
        "        return self.predict_fn(state_np)\n",
        "\n",
        "    def _ucb_score(self, parent, child):\n",
        "        \"\"\"UCB score clasico AlphaZero.\"\"\"\n",
        "        q = child.value()\n",
        "        u = self.c_puct * child.prior * math.sqrt(parent.visit_count) / (1 + child.visit_count)\n",
        "        return q + u\n",
        "\n",
        "    def _select_child(self, node):\n",
        "        \"\"\"Seleccionar hijo con mayor UCB score.\"\"\"\n",
        "        best_score = -float('inf')\n",
        "        best_action = -1\n",
        "        best_child = None\n",
        "        for action, child in node.children.items():\n",
        "            score = self._ucb_score(node, child)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_action = action\n",
        "                best_child = child\n",
        "        return best_action, best_child\n",
        "\n",
        "    def _expand(self, node):\n",
        "        \"\"\"\n",
        "        Expandir nodo hoja: evaluar con la red, crear hijos.\n",
        "        Maneja estocasticidad de comida promediando valores.\n",
        "        Retorna el valor del nodo.\n",
        "        \"\"\"\n",
        "        env = node.env\n",
        "        if env.done:\n",
        "            return 1.0 if env.is_win() else -1.0\n",
        "\n",
        "        policy, value = self._predict(env.get_state())\n",
        "\n",
        "        # --- Food stochasticity (batched via predict_batch_fn) ---\n",
        "        if node.food_eaten and self.food_samples > 1:\n",
        "            snake_set = set(env.snake)\n",
        "            free = [(x, y) for y in range(env.board_size)\n",
        "                    for x in range(env.board_size)\n",
        "                    if (x, y) not in snake_set and (x, y) != env.food]\n",
        "            k = min(self.food_samples - 1, len(free))\n",
        "            if k > 0:\n",
        "                sampled_positions = random.sample(free, k)\n",
        "                food_states = []\n",
        "                for food_pos in sampled_positions:\n",
        "                    env_copy = env.clone()\n",
        "                    env_copy.food = food_pos\n",
        "                    food_states.append(env_copy.get_state())\n",
        "                results = self.predict_batch_fn(food_states)\n",
        "                food_values = [r[1] for r in results]\n",
        "                value = (value + sum(food_values)) / (1 + len(food_values))\n",
        "\n",
        "        # Mascara de acciones validas y renormalizacion\n",
        "        valid = env.valid_actions()\n",
        "        mask = np.zeros(4, dtype=np.float32)\n",
        "        for a in valid:\n",
        "            mask[a] = 1.0\n",
        "        policy = policy * mask\n",
        "        total = policy.sum()\n",
        "        if total > 0:\n",
        "            policy /= total\n",
        "        else:\n",
        "            policy = mask / mask.sum()\n",
        "\n",
        "        # Crear hijos\n",
        "        for a in valid:\n",
        "            node.children[a] = MCTSNode(prior=policy[a])\n",
        "\n",
        "        node.is_expanded = True\n",
        "        return value\n",
        "\n",
        "    def _add_dirichlet_noise(self, node):\n",
        "        \"\"\"Agregar Dirichlet noise al root para exploracion.\"\"\"\n",
        "        actions = list(node.children.keys())\n",
        "        if not actions:\n",
        "            return\n",
        "        noise = np.random.dirichlet([self.dir_alpha] * len(actions))\n",
        "        for i, a in enumerate(actions):\n",
        "            node.children[a].prior = (\n",
        "                (1 - self.dir_eps) * node.children[a].prior +\n",
        "                self.dir_eps * noise[i]\n",
        "            )\n",
        "\n",
        "    def search(self, root_env, temperature=1.0):\n",
        "        \"\"\"\n",
        "        Ejecutar MCTS completo desde un estado raiz.\n",
        "        Retorna distribucion de probabilidad sobre acciones (4,).\n",
        "        \"\"\"\n",
        "        root = MCTSNode()\n",
        "        root.env = root_env.clone()\n",
        "\n",
        "        # Expandir raiz\n",
        "        self._expand(root)\n",
        "        self._add_dirichlet_noise(root)\n",
        "\n",
        "        # Simulaciones\n",
        "        for _ in range(self.num_simulations):\n",
        "            node = root\n",
        "            path = [node]\n",
        "\n",
        "            # --- SELECT ---\n",
        "            while node.is_expanded and node.children:\n",
        "                action, child = self._select_child(node)\n",
        "\n",
        "                # Computacion lazy del estado del hijo\n",
        "                if child.env is None:\n",
        "                    env_copy = node.env.clone()\n",
        "                    old_score = env_copy.score\n",
        "                    env_copy.step(action)\n",
        "                    child.env = env_copy\n",
        "                    child.food_eaten = (env_copy.score > old_score and not env_copy.done)\n",
        "\n",
        "                node = child\n",
        "                path.append(node)\n",
        "\n",
        "            # --- EXPAND & EVALUATE ---\n",
        "            if not node.is_expanded:\n",
        "                value = self._expand(node)\n",
        "            else:\n",
        "                # Nodo terminal (expanded pero sin hijos)\n",
        "                value = 1.0 if node.env.is_win() else -1.0\n",
        "\n",
        "            # --- BACKUP ---\n",
        "            for n in reversed(path):\n",
        "                n.visit_count += 1\n",
        "                n.value_sum += value\n",
        "\n",
        "        # --- Construir distribucion de acciones ---\n",
        "        visits = np.zeros(4, dtype=np.float32)\n",
        "        for a, child in root.children.items():\n",
        "            visits[a] = child.visit_count\n",
        "\n",
        "        if temperature == 0:\n",
        "            # Greedy\n",
        "            best = np.argmax(visits)\n",
        "            probs = np.zeros(4, dtype=np.float32)\n",
        "            probs[best] = 1.0\n",
        "        else:\n",
        "            # Con temperatura\n",
        "            visits_temp = np.power(visits, 1.0 / temperature)\n",
        "            total = visits_temp.sum()\n",
        "            if total > 0:\n",
        "                probs = visits_temp / total\n",
        "            else:\n",
        "                probs = np.ones(4, dtype=np.float32) / 4.0\n",
        "\n",
        "        return probs\n",
        "\n",
        "print(\"MCTS implementado correctamente.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batching de inferencia para self-play paralelo\n",
        "Se agrupan requests de múltiples workers para aprovechar mejor GPU.\n",
        "\n",
        "**Takeaway:** batching mejora throughput en RTX 4060 Ti y reduce cuello de botella en inferencia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 5b: InferenceBatcher — Batching de GPU para self-play paralelo\n",
        "# ============================================================\n",
        "import queue as queue_module\n",
        "import concurrent.futures\n",
        "\n",
        "class InferenceBatcher:\n",
        "    \"\"\"\n",
        "    Recolecta requests de inferencia de multiples hilos y los\n",
        "    procesa en batches en la GPU. Esto maximiza la utilizacion\n",
        "    de la GPU durante self-play paralelo.\n",
        "    \"\"\"\n",
        "    def __init__(self, net, device, max_batch=64, timeout_ms=3):\n",
        "        self.net = net\n",
        "        self.device = device\n",
        "        self.max_batch = max_batch\n",
        "        self.timeout = timeout_ms / 1000.0\n",
        "        self._queue = queue_module.Queue()\n",
        "        self._running = False\n",
        "        self._thread = None\n",
        "        self._total_calls = 0\n",
        "        self._total_batches = 0\n",
        "\n",
        "    def start(self):\n",
        "        self._running = True\n",
        "        self._total_calls = 0\n",
        "        self._total_batches = 0\n",
        "        self._thread = threading.Thread(target=self._worker, daemon=True)\n",
        "        self._thread.start()\n",
        "        return self\n",
        "\n",
        "    def stop(self):\n",
        "        self._running = False\n",
        "        self._queue.put(None)  # sentinel\n",
        "        if self._thread:\n",
        "            self._thread.join(timeout=10)\n",
        "\n",
        "    def submit(self, state_np):\n",
        "        \"\"\"Non-blocking: enviar estado, retorna Future.\"\"\"\n",
        "        f = concurrent.futures.Future()\n",
        "        self._queue.put((state_np, f))\n",
        "        return f\n",
        "\n",
        "    def predict(self, state_np):\n",
        "        \"\"\"Blocking: enviar estado, esperar resultado (policy, value).\"\"\"\n",
        "        return self.submit(state_np).result()\n",
        "\n",
        "    def predict_batch(self, states_list):\n",
        "        \"\"\"Enviar multiples estados, esperar todos los resultados.\"\"\"\n",
        "        if not states_list:\n",
        "            return []\n",
        "        futures = [self.submit(s) for s in states_list]\n",
        "        return [f.result() for f in futures]\n",
        "\n",
        "    def stats(self):\n",
        "        avg = self._total_calls / max(self._total_batches, 1)\n",
        "        return self._total_calls, self._total_batches, avg\n",
        "\n",
        "    def _worker(self):\n",
        "        while self._running:\n",
        "            batch = []\n",
        "            # Esperar primer request\n",
        "            try:\n",
        "                item = self._queue.get(timeout=1.0)\n",
        "            except queue_module.Empty:\n",
        "                continue\n",
        "            if item is None:\n",
        "                break\n",
        "            batch.append(item)\n",
        "\n",
        "            # Recolectar mas requests (hasta max_batch o timeout)\n",
        "            deadline = time.time() + self.timeout\n",
        "            while len(batch) < self.max_batch:\n",
        "                remaining = deadline - time.time()\n",
        "                if remaining <= 0:\n",
        "                    break\n",
        "                try:\n",
        "                    item = self._queue.get(timeout=max(0.0001, remaining))\n",
        "                except queue_module.Empty:\n",
        "                    break\n",
        "                if item is None:\n",
        "                    self._running = False\n",
        "                    break\n",
        "                batch.append(item)\n",
        "\n",
        "            if not batch:\n",
        "                continue\n",
        "\n",
        "            states = [b[0] for b in batch]\n",
        "            futures_list = [b[1] for b in batch]\n",
        "            self._total_calls += len(batch)\n",
        "            self._total_batches += 1\n",
        "\n",
        "            try:\n",
        "                arr = np.stack(states)\n",
        "                t = torch.as_tensor(arr, device=self.device)\n",
        "                with torch.no_grad():\n",
        "                    policies, values = self.net(t)\n",
        "                p_np = policies.cpu().numpy()\n",
        "                v_np = values.cpu().numpy()\n",
        "                for i, f in enumerate(futures_list):\n",
        "                    f.set_result((p_np[i], float(v_np[i, 0])))\n",
        "            except Exception as e:\n",
        "                for f in futures_list:\n",
        "                    if not f.done():\n",
        "                        f.set_exception(e)\n",
        "\n",
        "\n",
        "def make_predict_fn(net, device):\n",
        "    \"\"\"Crear funcion de prediccion directa (para uso single-thread, eval).\"\"\"\n",
        "    @torch.no_grad()\n",
        "    def predict(state_np):\n",
        "        t = torch.as_tensor(state_np, device=device).unsqueeze(0)\n",
        "        p, v = net(t)\n",
        "        return p[0].cpu().numpy(), v.item()\n",
        "    return predict\n",
        "\n",
        "print(\"InferenceBatcher implementado.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-play y Replay Buffer\n",
        "Se generan ejemplos `(state, pi, z)` por partida y se acumulan en buffer circular.\n",
        "\n",
        "**Takeaway:** datos frescos + buffer suficiente son críticos para estabilidad de entrenamiento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 6: Self-Play\n",
        "# ============================================================\n",
        "\n",
        "def self_play_game(predict_fn, cfg, predict_batch_fn=None, progress_cb=None):\n",
        "    \"\"\"\n",
        "    Jugar una partida completa usando MCTS.\n",
        "    predict_fn: callable(state_np) -> (policy_np, value_float)\n",
        "    predict_batch_fn: callable(list[state_np]) -> list[(policy, value)]\n",
        "    progress_cb: callable(move_count, score, length) opcional para telemetria.\n",
        "    Retorna: (examples, is_win, score, num_moves)\n",
        "    \"\"\"\n",
        "    env = SnakeEnv(cfg.board_size, cfg.max_steps)\n",
        "    mcts = MCTS(predict_fn, cfg, predict_batch_fn)\n",
        "    env.reset()\n",
        "\n",
        "    states = []\n",
        "    policies = []\n",
        "    move_count = 0\n",
        "\n",
        "    while not env.done:\n",
        "        # Temperatura: 1.0 hasta move temp_decay_move, luego 0.0\n",
        "        temp = cfg.temp_init if move_count < cfg.temp_decay_move else cfg.temp_final\n",
        "\n",
        "        # Busqueda MCTS\n",
        "        pi = mcts.search(env, temperature=temp)\n",
        "\n",
        "        # Guardar datos de entrenamiento\n",
        "        states.append(env.get_state().copy())\n",
        "        policies.append(pi.copy())\n",
        "\n",
        "        # Seleccionar accion\n",
        "        if temp == 0:\n",
        "            action = int(np.argmax(pi))\n",
        "        else:\n",
        "            action = int(np.random.choice(4, p=pi))\n",
        "\n",
        "        # Ejecutar\n",
        "        env.step(action)\n",
        "        move_count += 1\n",
        "        if progress_cb is not None:\n",
        "            tick_every = max(25, int(getattr(cfg, \"game_tick_moves\", 100)))\n",
        "            if move_count == 1 or move_count % tick_every == 0:\n",
        "                progress_cb(move_count, env.score, len(env.snake))\n",
        "\n",
        "    # Determinar resultado: +1 si gano (lleno tablero), -1 si no\n",
        "    z = 1.0 if env.is_win() else -1.0\n",
        "\n",
        "    # Crear ejemplos de entrenamiento\n",
        "    examples = [(s, p, z) for s, p in zip(states, policies)]\n",
        "    return examples, env.is_win(), env.score, move_count, len(env.snake)\n",
        "\n",
        "\n",
        "def run_n_games(n, request_queue, response_queue, progress_queue, worker_id, cfg):\n",
        "    \"\"\"\n",
        "    Ejecutar n partidas de self-play en un proceso worker.\n",
        "    Usa request_queue/response_queue para obtener (policy, value) del proceso principal.\n",
        "    Necesario para multiprocessing: cada worker usa su propio nucleo (sin GIL).\n",
        "    \"\"\"\n",
        "    def predict(state_np):\n",
        "        request_queue.put((worker_id, state_np))\n",
        "        return response_queue.get()\n",
        "\n",
        "    def predict_batch(states_list):\n",
        "        return [predict(s) for s in states_list]\n",
        "\n",
        "    all_examples = []\n",
        "    wins = 0\n",
        "    scores_list = []\n",
        "    moves_list = []\n",
        "    lengths_list = []\n",
        "\n",
        "    if progress_queue is not None:\n",
        "        progress_queue.put({\"type\": \"worker_start\", \"worker\": worker_id, \"games\": n})\n",
        "\n",
        "    for game_idx in range(n):\n",
        "        if progress_queue is not None:\n",
        "            progress_queue.put({\"type\": \"game_start\", \"worker\": worker_id, \"game_local\": game_idx + 1})\n",
        "\n",
        "        progress_cb = None\n",
        "        if progress_queue is not None and bool(getattr(cfg, \"verbose_game_updates\", False)):\n",
        "            def _progress_cb(move_count, score_now, len_now):\n",
        "                progress_queue.put({\n",
        "                    \"type\": \"game_tick\",\n",
        "                    \"worker\": worker_id,\n",
        "                    \"game_local\": game_idx + 1,\n",
        "                    \"moves\": move_count,\n",
        "                    \"score\": score_now,\n",
        "                    \"length\": len_now,\n",
        "                })\n",
        "            progress_cb = _progress_cb\n",
        "\n",
        "        examples, won, score, moves, length = self_play_game(predict, cfg, predict_batch, progress_cb=progress_cb)\n",
        "        all_examples.extend(examples)\n",
        "        if won:\n",
        "            wins += 1\n",
        "        scores_list.append(score)\n",
        "        moves_list.append(moves)\n",
        "        lengths_list.append(length)\n",
        "\n",
        "        if progress_queue is not None:\n",
        "            progress_queue.put({\n",
        "                \"type\": \"game_end\",\n",
        "                \"worker\": worker_id,\n",
        "                \"game_local\": game_idx + 1,\n",
        "                \"won\": won,\n",
        "                \"score\": score,\n",
        "                \"moves\": moves,\n",
        "                \"length\": length,\n",
        "                \"examples\": len(examples),\n",
        "            })\n",
        "\n",
        "    return all_examples, wins, scores_list, moves_list, lengths_list\n",
        "\n",
        "\n",
        "print(\"Self-play implementado.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 7: Replay Buffer y Training\n",
        "# ============================================================\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Buffer circular para almacenar ejemplos de self-play.\"\"\"\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "\n",
        "    def add(self, examples):\n",
        "        \"\"\"Agregar lista de (state, policy, value) al buffer.\"\"\"\n",
        "        self.buffer.extend(examples)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Samplear un batch aleatorio.\"\"\"\n",
        "        n = min(batch_size, len(self.buffer))\n",
        "        indices = np.random.choice(len(self.buffer), size=n, replace=False)\n",
        "        batch = [self.buffer[i] for i in indices]\n",
        "        states = np.array([b[0] for b in batch])\n",
        "        policies = np.array([b[1] for b in batch])\n",
        "        values = np.array([b[2] for b in batch], dtype=np.float32)\n",
        "        return states, policies, values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def save(self, path):\n",
        "        with open(path, 'wb') as f:\n",
        "            pickle.dump(list(self.buffer), f)\n",
        "\n",
        "    def load(self, path):\n",
        "        if os.path.exists(path):\n",
        "            with open(path, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "            self.buffer = deque(data, maxlen=self.buffer.maxlen)\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "\n",
        "def train_epoch(net, optimizer, buffer, cfg, device, scaler=None):\n",
        "    \"\"\"\n",
        "    Entrenar una epoca sobre el replay buffer.\n",
        "    Loss = (z - v)^2 - pi * log(p)\n",
        "    (weight_decay ya esta en el optimizer como L2)\n",
        "    \"\"\"\n",
        "    net.train()\n",
        "    total_loss = 0.0\n",
        "    total_p_loss = 0.0\n",
        "    total_v_loss = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    num_batches = max(1, len(buffer) // cfg.batch_size)\n",
        "    use_amp = bool(getattr(cfg, \"use_amp\", True) and device.type == \"cuda\")\n",
        "\n",
        "    for _ in range(num_batches):\n",
        "        states, policies, values = buffer.sample(cfg.batch_size)\n",
        "\n",
        "        states_t = torch.from_numpy(states)\n",
        "        policies_t = torch.from_numpy(policies)\n",
        "        values_t = torch.from_numpy(values).unsqueeze(1)\n",
        "\n",
        "        if device.type == \"cuda\":\n",
        "            states_t = states_t.pin_memory().to(device, non_blocking=True)\n",
        "            policies_t = policies_t.pin_memory().to(device, non_blocking=True)\n",
        "            values_t = values_t.pin_memory().to(device, non_blocking=True)\n",
        "        else:\n",
        "            states_t = states_t.to(device)\n",
        "            policies_t = policies_t.to(device)\n",
        "            values_t = values_t.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=use_amp):\n",
        "            pred_p, pred_v = net(states_t)\n",
        "\n",
        "            # Value loss: MSE\n",
        "            v_loss = F.mse_loss(pred_v, values_t)\n",
        "\n",
        "            # Policy loss: cross-entropy (pi * log(p))\n",
        "            p_loss = -torch.mean(torch.sum(policies_t * torch.log(pred_p + 1e-8), dim=1))\n",
        "\n",
        "            loss = v_loss + p_loss\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        if use_amp and scaler is not None:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_p_loss += p_loss.item()\n",
        "        total_v_loss += v_loss.item()\n",
        "        n_batches += 1\n",
        "\n",
        "    if n_batches == 0:\n",
        "        return 0, 0, 0\n",
        "    return total_loss / n_batches, total_p_loss / n_batches, total_v_loss / n_batches\n",
        "\n",
        "print(\"ReplayBuffer y train_epoch implementados.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluación y Champion Gate\n",
        "Evaluación MCTS sin ruido para métrica real y comparación `new vs best` por head-to-head pareado por seed.\n",
        "\n",
        "**Takeaway:** solo se acepta champion cuando supera umbral `>55%` en confrontación directa.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# Cell 8: Evaluacion (MCTS, policy y head-to-head champion gate)\n",
        "# ============================================================\n",
        "\n",
        "def evaluate_mcts(net, cfg, device, n_games=None):\n",
        "    # Evaluar modelo usando MCTS (greedy, temp=0).\n",
        "    if n_games is None:\n",
        "        n_games = cfg.eval_games\n",
        "    net.eval()\n",
        "\n",
        "    wins = 0\n",
        "    total_score = 0\n",
        "    scores = []\n",
        "\n",
        "    eval_cfg = dataclasses.replace(cfg, dir_eps=0.0)\n",
        "\n",
        "    for _ in tqdm(range(n_games), desc='Eval MCTS', leave=False):\n",
        "        env = SnakeEnv(eval_cfg.board_size, eval_cfg.max_steps)\n",
        "        mcts_eval = MCTS(make_predict_fn(net, device), eval_cfg)\n",
        "        env.reset()\n",
        "\n",
        "        while not env.done:\n",
        "            pi = mcts_eval.search(env, temperature=0)\n",
        "            action = int(np.argmax(pi))\n",
        "            env.step(action)\n",
        "\n",
        "        if env.is_win():\n",
        "            wins += 1\n",
        "        total_score += env.score\n",
        "        scores.append(env.score)\n",
        "\n",
        "    wr = wins / max(n_games, 1)\n",
        "    avg = total_score / max(n_games, 1)\n",
        "    return wr, avg, scores\n",
        "\n",
        "\n",
        "def evaluate_policy_only(net, cfg, device, n_games=100):\n",
        "    # Evaluacion rapida sin MCTS (solo monitoreo auxiliar).\n",
        "    net.eval()\n",
        "    wins = 0\n",
        "    total_score = 0\n",
        "\n",
        "    for _ in tqdm(range(n_games), desc='Eval Policy', leave=False):\n",
        "        env = SnakeEnv(cfg.board_size, cfg.max_steps)\n",
        "        env.reset()\n",
        "\n",
        "        while not env.done:\n",
        "            state_t = torch.from_numpy(env.get_state()).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                policy, _ = net(state_t)\n",
        "\n",
        "            valid = env.valid_actions()\n",
        "            mask = torch.zeros(4, device=device)\n",
        "            for a in valid:\n",
        "                mask[a] = 1.0\n",
        "            policy = policy.squeeze() * mask\n",
        "            total = policy.sum()\n",
        "            policy = (policy / total) if total > 0 else (mask / mask.sum())\n",
        "\n",
        "            action = int(torch.argmax(policy).item())\n",
        "            env.step(action)\n",
        "\n",
        "        if env.is_win():\n",
        "            wins += 1\n",
        "        total_score += env.score\n",
        "\n",
        "    wr = wins / max(n_games, 1)\n",
        "    avg = total_score / max(n_games, 1)\n",
        "    return wr, avg\n",
        "\n",
        "\n",
        "def _play_single_game_mcts(net, cfg, device, seed):\n",
        "    # Partida determinística por seed para evaluación pareada.\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    env = SnakeEnv(cfg.board_size, cfg.max_steps)\n",
        "    mcts_eval = MCTS(make_predict_fn(net, device), cfg)\n",
        "    env.reset()\n",
        "\n",
        "    while not env.done:\n",
        "        pi = mcts_eval.search(env, temperature=0)\n",
        "        action = int(np.argmax(pi))\n",
        "        env.step(action)\n",
        "\n",
        "    return env.is_win(), env.score\n",
        "\n",
        "\n",
        "def evaluate_head_to_head_mcts(new_net, best_net, cfg, device, n_games=None, seed_offset=10000):\n",
        "    \"\"\"\n",
        "    Champion gate fiel al paper: nuevo vs mejor modelo con MCTS, pareado por seed.\n",
        "    Aceptar si win_rate del nuevo > accept_threshold.\n",
        "    \"\"\"\n",
        "    if n_games is None:\n",
        "        n_games = cfg.eval_games\n",
        "\n",
        "    eval_cfg = dataclasses.replace(cfg, dir_eps=0.0)\n",
        "\n",
        "    new_wins = 0\n",
        "    best_wins = 0\n",
        "    draws = 0\n",
        "\n",
        "    for i in tqdm(range(n_games), desc='Eval H2H', leave=False):\n",
        "        seed = seed_offset + i\n",
        "        new_win, new_score = _play_single_game_mcts(new_net, eval_cfg, device, seed)\n",
        "        best_win, best_score = _play_single_game_mcts(best_net, eval_cfg, device, seed)\n",
        "\n",
        "        # Comparacion primaria por score final (mas granular)\n",
        "        if new_score > best_score:\n",
        "            new_wins += 1\n",
        "        elif best_score > new_score:\n",
        "            best_wins += 1\n",
        "        else:\n",
        "            # Desempate por victoria binaria\n",
        "            if new_win and not best_win:\n",
        "                new_wins += 1\n",
        "            elif best_win and not new_win:\n",
        "                best_wins += 1\n",
        "            else:\n",
        "                draws += 1\n",
        "\n",
        "    compared = max(1, new_wins + best_wins)\n",
        "    new_wr = new_wins / compared\n",
        "    return new_wr, new_wins, best_wins, draws\n",
        "\n",
        "\n",
        "print('Funciones de evaluacion implementadas (incluye head-to-head MCTS).')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loop principal de entrenamiento (faseado)\n",
        "Fase A: warm-up (12 iter) y Fase B: paper (25 iter), con early-stop por `wr>=0.94` en 3 checkpoints consecutivos.\n",
        "\n",
        "**Takeaway:** faseado reduce costo inicial sin perder objetivo de fidelidad final.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# Cell 9: Loop Principal (self-play -> train -> H2H -> champion)\n",
        "# ============================================================\n",
        "\n",
        "def save_checkpoint(net, best_net, optimizer, iteration, best_win_rate, cfg, buffer, consecutive_target_hits):\n",
        "    ckpt = {\n",
        "        'iteration': iteration,\n",
        "        'net': net.state_dict(),\n",
        "        'best_net': best_net.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'best_win_rate': best_win_rate,\n",
        "        'consecutive_target_hits': consecutive_target_hits,\n",
        "        'config': dataclasses.asdict(cfg),\n",
        "    }\n",
        "    path = os.path.join(cfg.save_dir, 'latest_checkpoint.pt')\n",
        "    torch.save(ckpt, path)\n",
        "\n",
        "    best_path = os.path.join(cfg.save_dir, 'best_model.pt')\n",
        "    torch.save(best_net.state_dict(), best_path)\n",
        "\n",
        "    buffer_path = os.path.join(cfg.save_dir, 'buffer.pkl')\n",
        "    buffer.save(buffer_path)\n",
        "\n",
        "\n",
        "def load_checkpoint(net, best_net, optimizer, cfg, buffer):\n",
        "    path = os.path.join(cfg.save_dir, 'latest_checkpoint.pt')\n",
        "    if not os.path.exists(path):\n",
        "        return 0, 0.0, 0\n",
        "\n",
        "    ckpt = torch.load(path, map_location=device)\n",
        "    net.load_state_dict(ckpt['net'])\n",
        "    best_net.load_state_dict(ckpt['best_net'])\n",
        "    optimizer.load_state_dict(ckpt['optimizer'])\n",
        "\n",
        "    buffer_path = os.path.join(cfg.save_dir, 'buffer.pkl')\n",
        "    buffer.load(buffer_path)\n",
        "\n",
        "    iteration = int(ckpt.get('iteration', 0))\n",
        "    best_wr = float(ckpt.get('best_win_rate', 0.0))\n",
        "    consecutive = int(ckpt.get('consecutive_target_hits', 0))\n",
        "\n",
        "    print(f'Checkpoint cargado: iter={iteration}, best_wr={best_wr:.3f}, target_hits={consecutive}')\n",
        "    print(f'Buffer restaurado: {len(buffer)} ejemplos')\n",
        "    return iteration, best_wr, consecutive\n",
        "\n",
        "\n",
        "def train_alphasnake(cfg):\n",
        "    print('=' * 70)\n",
        "    print(' AlphaSnake 10x10 Training (Vast.ai phased + paper-fidelity)')\n",
        "    print('=' * 70)\n",
        "\n",
        "    net = AlphaSnakeNet(4, cfg.net_blocks, cfg.net_channels, cfg.board_size).to(device)\n",
        "    best_net = AlphaSnakeNet(4, cfg.net_blocks, cfg.net_channels, cfg.board_size).to(device)\n",
        "    best_net.load_state_dict(net.state_dict())\n",
        "\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    buffer = ReplayBuffer(cfg.buffer_size)\n",
        "    scaler = torch.amp.GradScaler(device='cuda', enabled=bool(cfg.use_amp and device.type == 'cuda'))\n",
        "\n",
        "    start_iter, best_win_rate, consecutive_target_hits = load_checkpoint(net, best_net, optimizer, cfg, buffer)\n",
        "\n",
        "    training_start = time.time()\n",
        "    for iteration in range(start_iter, cfg.max_iterations):\n",
        "        iter_no = iteration + 1\n",
        "        iter_cfg = get_iteration_config(cfg, iter_no)\n",
        "\n",
        "        iter_start = time.time()\n",
        "        phase = 'warm-up' if iter_no <= cfg.warmup_iterations else 'paper'\n",
        "\n",
        "        print('\\n' + '=' * 70)\n",
        "        print(f'ITER {iter_no}/{cfg.max_iterations} | phase={phase}')\n",
        "        print(\n",
        "            f\"sims={iter_cfg.num_simulations} | games={iter_cfg.games_per_iter} | \"\n",
        "            f\"eval={iter_cfg.eval_games} | food_samples={iter_cfg.food_samples}\"\n",
        "        )\n",
        "        print('=' * 70)\n",
        "\n",
        "        # ------------------------------------------------\n",
        "        # 1) Self-play paralelo\n",
        "        # ------------------------------------------------\n",
        "        # ------------------------------------------------\n",
        "        # 1) Self-play paralelo\n",
        "        # ------------------------------------------------\n",
        "        cpu_total = os.cpu_count() or 8\n",
        "        n_workers = max(1, min(int(iter_cfg.selfplay_workers), cpu_total))\n",
        "        backend = getattr(iter_cfg, 'selfplay_backend', 'process')\n",
        "\n",
        "        all_examples = []\n",
        "        sp_wins = 0\n",
        "        sp_scores = []\n",
        "        sp_moves = []\n",
        "        sp_lengths = []\n",
        "\n",
        "        best_net.eval()\n",
        "        sp_start = time.time()\n",
        "\n",
        "        if backend == 'process':\n",
        "            mp_manager = multiprocessing.Manager()\n",
        "            mp_request_queue = mp_manager.Queue()\n",
        "            response_queues = [mp_manager.Queue() for _ in range(n_workers)]\n",
        "            progress_queue = mp_manager.Queue()\n",
        "\n",
        "            def inference_server():\n",
        "                batch_timeout = max(0.001, float(iter_cfg.inference_timeout_ms) / 1000.0)\n",
        "                max_batch = max(8, int(iter_cfg.inference_batch_size))\n",
        "                use_amp = bool(iter_cfg.use_amp and device.type == 'cuda')\n",
        "                while True:\n",
        "                    batch = []\n",
        "                    deadline = time.time() + batch_timeout\n",
        "                    while len(batch) < max_batch:\n",
        "                        try:\n",
        "                            item = mp_request_queue.get(timeout=max(0.001, deadline - time.time()))\n",
        "                            if item is None:\n",
        "                                return\n",
        "                            batch.append(item)\n",
        "                        except Empty:\n",
        "                            break\n",
        "                    if not batch:\n",
        "                        continue\n",
        "\n",
        "                    wids = [b[0] for b in batch]\n",
        "                    states = [b[1] for b in batch]\n",
        "                    arr = np.stack(states)\n",
        "                    t = torch.from_numpy(arr)\n",
        "                    if device.type == 'cuda':\n",
        "                        t = t.pin_memory().to(device, non_blocking=True)\n",
        "                    else:\n",
        "                        t = t.to(device)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        with torch.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):\n",
        "                            policies, values = best_net(t)\n",
        "\n",
        "                    p_np = policies.cpu().numpy()\n",
        "                    v_np = values.cpu().numpy()\n",
        "                    for i in range(len(batch)):\n",
        "                        response_queues[wids[i]].put((p_np[i], float(v_np[i, 0])))\n",
        "\n",
        "            server_thread = threading.Thread(target=inference_server, daemon=True)\n",
        "            server_thread.start()\n",
        "\n",
        "            games_per_worker = [\n",
        "                iter_cfg.games_per_iter // n_workers + (1 if i < iter_cfg.games_per_iter % n_workers else 0)\n",
        "                for i in range(n_workers)\n",
        "            ]\n",
        "\n",
        "            with concurrent.futures.ProcessPoolExecutor(max_workers=n_workers) as pool:\n",
        "                futures = [\n",
        "                    pool.submit(run_n_games, games_per_worker[i], mp_request_queue, response_queues[i], progress_queue, i, iter_cfg)\n",
        "                    for i in range(n_workers)\n",
        "                    if games_per_worker[i] > 0\n",
        "                ]\n",
        "                pending = set(futures)\n",
        "                games_reported = 0\n",
        "                heartbeat_s = max(10, int(getattr(iter_cfg, 'heartbeat_seconds', 30)))\n",
        "                last_heartbeat = time.time()\n",
        "\n",
        "                with tqdm(total=iter_cfg.games_per_iter, desc=f'Self-play iter {iter_no}', dynamic_ncols=True) as pbar:\n",
        "                    while pending:\n",
        "                        drained = 0\n",
        "                        while True:\n",
        "                            try:\n",
        "                                msg = progress_queue.get_nowait()\n",
        "                            except Empty:\n",
        "                                break\n",
        "\n",
        "                            drained += 1\n",
        "                            if isinstance(msg, dict) and msg.get('type') == 'game_end':\n",
        "                                games_reported += 1\n",
        "                                pbar.update(1)\n",
        "\n",
        "                        done_now, pending = concurrent.futures.wait(\n",
        "                            pending,\n",
        "                            timeout=1.0,\n",
        "                            return_when=concurrent.futures.FIRST_COMPLETED,\n",
        "                        )\n",
        "\n",
        "                        if not done_now and drained == 0 and (time.time() - last_heartbeat) >= heartbeat_s:\n",
        "                            print(\n",
        "                                f\"[Self-play heartbeat] juegos={games_reported}/{iter_cfg.games_per_iter} | workers_activos={len(pending)}\",\n",
        "                                flush=True,\n",
        "                            )\n",
        "                            last_heartbeat = time.time()\n",
        "\n",
        "                        for future in done_now:\n",
        "                            ex, w, sc, mv, ln = future.result()\n",
        "                            all_examples.extend(ex)\n",
        "                            sp_wins += w\n",
        "                            sp_scores.extend(sc)\n",
        "                            sp_moves.extend(mv)\n",
        "                            sp_lengths.extend(ln)\n",
        "\n",
        "                    # Drenar eventos restantes\n",
        "                    while True:\n",
        "                        try:\n",
        "                            msg = progress_queue.get_nowait()\n",
        "                        except Empty:\n",
        "                            break\n",
        "                        if isinstance(msg, dict) and msg.get('type') == 'game_end':\n",
        "                            games_reported += 1\n",
        "\n",
        "                    if pbar.n < iter_cfg.games_per_iter:\n",
        "                        pbar.update(iter_cfg.games_per_iter - pbar.n)\n",
        "\n",
        "            for _ in range(n_workers):\n",
        "                mp_request_queue.put(None)\n",
        "            server_thread.join(timeout=10)\n",
        "\n",
        "        else:\n",
        "            # Fallback: thread backend (más estable, pero limitado por GIL)\n",
        "            progress_queue = queue_module.Queue()\n",
        "            batcher = InferenceBatcher(\n",
        "                best_net,\n",
        "                device,\n",
        "                max_batch=max(8, int(iter_cfg.inference_batch_size)),\n",
        "                timeout_ms=max(1, int(iter_cfg.inference_timeout_ms)),\n",
        "            ).start()\n",
        "\n",
        "            def run_n_games_thread(n_games, worker_id):\n",
        "                examples_all = []\n",
        "                wins_local = 0\n",
        "                scores_local = []\n",
        "                moves_local = []\n",
        "                lengths_local = []\n",
        "\n",
        "                for _ in range(n_games):\n",
        "                    examples, won, score, moves, length = self_play_game(\n",
        "                        batcher.predict,\n",
        "                        iter_cfg,\n",
        "                        predict_batch_fn=batcher.predict_batch,\n",
        "                        progress_cb=None,\n",
        "                    )\n",
        "                    examples_all.extend(examples)\n",
        "                    if won:\n",
        "                        wins_local += 1\n",
        "                    scores_local.append(score)\n",
        "                    moves_local.append(moves)\n",
        "                    lengths_local.append(length)\n",
        "                    progress_queue.put(1)\n",
        "\n",
        "                return examples_all, wins_local, scores_local, moves_local, lengths_local\n",
        "\n",
        "            games_per_worker = [\n",
        "                iter_cfg.games_per_iter // n_workers + (1 if i < iter_cfg.games_per_iter % n_workers else 0)\n",
        "                for i in range(n_workers)\n",
        "            ]\n",
        "\n",
        "            with concurrent.futures.ThreadPoolExecutor(max_workers=n_workers) as pool:\n",
        "                futures = [\n",
        "                    pool.submit(run_n_games_thread, games_per_worker[i], i)\n",
        "                    for i in range(n_workers)\n",
        "                    if games_per_worker[i] > 0\n",
        "                ]\n",
        "                pending = set(futures)\n",
        "                games_reported = 0\n",
        "                heartbeat_s = max(10, int(getattr(iter_cfg, 'heartbeat_seconds', 30)))\n",
        "                last_heartbeat = time.time()\n",
        "\n",
        "                with tqdm(total=iter_cfg.games_per_iter, desc=f'Self-play iter {iter_no}', dynamic_ncols=True) as pbar:\n",
        "                    while pending:\n",
        "                        drained = 0\n",
        "                        while True:\n",
        "                            try:\n",
        "                                progress_queue.get_nowait()\n",
        "                                drained += 1\n",
        "                            except queue_module.Empty:\n",
        "                                break\n",
        "\n",
        "                        if drained:\n",
        "                            games_reported += drained\n",
        "                            pbar.update(drained)\n",
        "\n",
        "                        done_now, pending = concurrent.futures.wait(\n",
        "                            pending,\n",
        "                            timeout=1.0,\n",
        "                            return_when=concurrent.futures.FIRST_COMPLETED,\n",
        "                        )\n",
        "\n",
        "                        if not done_now and drained == 0 and (time.time() - last_heartbeat) >= heartbeat_s:\n",
        "                            print(\n",
        "                                f\"[Self-play heartbeat] juegos={games_reported}/{iter_cfg.games_per_iter} | workers_activos={len(pending)}\",\n",
        "                                flush=True,\n",
        "                            )\n",
        "                            last_heartbeat = time.time()\n",
        "\n",
        "                        for future in done_now:\n",
        "                            ex, w, sc, mv, ln = future.result()\n",
        "                            all_examples.extend(ex)\n",
        "                            sp_wins += w\n",
        "                            sp_scores.extend(sc)\n",
        "                            sp_moves.extend(mv)\n",
        "                            sp_lengths.extend(ln)\n",
        "\n",
        "                    if pbar.n < iter_cfg.games_per_iter:\n",
        "                        pbar.update(iter_cfg.games_per_iter - pbar.n)\n",
        "\n",
        "            total_calls, total_batches, avg_batch = batcher.stats()\n",
        "            batcher.stop()\n",
        "            print(f\"[Inference batcher] calls={total_calls} | batches={total_batches} | avg_batch={avg_batch:.1f}\")\n",
        "\n",
        "        sp_time = time.time() - sp_start\n",
        "        sp_wr = sp_wins / max(iter_cfg.games_per_iter, 1)\n",
        "        sp_scores_arr = np.array(sp_scores if sp_scores else [0])\n",
        "        sp_moves_arr = np.array(sp_moves if sp_moves else [0])\n",
        "        sp_lengths_arr = np.array(sp_lengths if sp_lengths else [0])\n",
        "\n",
        "        buffer.add(all_examples)\n",
        "\n",
        "        print(f'[Self-play] {sp_time:.0f}s | wins={sp_wins}/{iter_cfg.games_per_iter} ({100*sp_wr:.1f}%) | examples={len(all_examples)}')\n",
        "        print(f\"           score avg={sp_scores_arr.mean():.1f} | moves med={np.median(sp_moves_arr):.0f} | len med={np.median(sp_lengths_arr):.0f}\")\n",
        "        print(f'           buffer={len(buffer)}')\n",
        "\n",
        "        # ------------------------------------------------\n",
        "        # 2) Training\n",
        "        # ------------------------------------------------\n",
        "        train_start = time.time()\n",
        "        last_loss, last_p, last_v = 0.0, 0.0, 0.0\n",
        "\n",
        "        for epoch in tqdm(range(iter_cfg.epochs_per_iter), desc=f'Train iter {iter_no}', leave=False):\n",
        "            last_loss, last_p, last_v = train_epoch(net, optimizer, buffer, iter_cfg, device, scaler=scaler)\n",
        "            if (epoch + 1) in {1, iter_cfg.epochs_per_iter}:\n",
        "                tqdm.write(f'  epoch {epoch+1}/{iter_cfg.epochs_per_iter}: loss={last_loss:.4f} (p={last_p:.4f}, v={last_v:.4f})')\n",
        "\n",
        "        train_time = time.time() - train_start\n",
        "\n",
        "        # ------------------------------------------------\n",
        "        # 3) Champion gate (head-to-head MCTS > 55%)\n",
        "        # ------------------------------------------------\n",
        "        eval_start = time.time()\n",
        "        h2h_wr, new_wins, best_wins, draws = evaluate_head_to_head_mcts(\n",
        "            net, best_net, iter_cfg, device, n_games=iter_cfg.eval_games\n",
        "        )\n",
        "\n",
        "        if h2h_wr > iter_cfg.accept_threshold:\n",
        "            print(f'[Champion] NUEVO modelo aceptado: h2h_wr={h2h_wr:.3f} ({new_wins}-{best_wins}, draws={draws})')\n",
        "            best_net.load_state_dict(net.state_dict())\n",
        "            best_win_rate = max(best_win_rate, h2h_wr)\n",
        "        else:\n",
        "            print(f'[Champion] Se mantiene best: h2h_wr={h2h_wr:.3f} ({new_wins}-{best_wins}, draws={draws})')\n",
        "            net.load_state_dict(best_net.state_dict())\n",
        "\n",
        "        # Métrica de éxito final (MCTS sobre champion)\n",
        "        wr_eval, avg_eval, _ = evaluate_mcts(best_net, iter_cfg, device, n_games=iter_cfg.eval_games)\n",
        "        if wr_eval >= iter_cfg.target_win_rate:\n",
        "            consecutive_target_hits += 1\n",
        "        else:\n",
        "            consecutive_target_hits = 0\n",
        "\n",
        "        eval_time = time.time() - eval_start\n",
        "\n",
        "        # ------------------------------------------------\n",
        "        # 4) Summary + checkpoint + early-stop\n",
        "        # ------------------------------------------------\n",
        "        iter_time = time.time() - iter_start\n",
        "        elapsed = (time.time() - training_start) / 60.0\n",
        "\n",
        "        print(f'[Summary] train={train_time:.0f}s | eval={eval_time:.0f}s | iter={iter_time:.0f}s | elapsed={elapsed:.1f} min')\n",
        "        print(f'          champion_eval_wr={wr_eval:.3f}, champion_eval_avg={avg_eval:.1f}, target_hits={consecutive_target_hits}/{iter_cfg.target_win_rate_patience}')\n",
        "\n",
        "        if iter_no % iter_cfg.checkpoint_interval == 0:\n",
        "            save_checkpoint(net, best_net, optimizer, iter_no, best_win_rate, cfg, buffer, consecutive_target_hits)\n",
        "            print(f'[Checkpoint] guardado en {cfg.save_dir} (iter={iter_no})')\n",
        "\n",
        "        if consecutive_target_hits >= iter_cfg.target_win_rate_patience:\n",
        "            print(f\"[Early stop] Se alcanzo wr>={iter_cfg.target_win_rate:.2f} por {iter_cfg.target_win_rate_patience} checkpoints consecutivos.\")\n",
        "            break\n",
        "\n",
        "    print('\\nEntrenamiento completado.')\n",
        "    return best_net\n",
        "\n",
        "\n",
        "print('Loop de entrenamiento definido. Ejecutar la siguiente celda para entrenar.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# Cell X: Smoke checks de contrato entorno/modelo\n",
        "# ============================================================\n",
        "\n",
        "# 1) shape\n",
        "env = SnakeEnv(board_size=10, max_steps=1000)\n",
        "obs = env.reset()\n",
        "assert obs.shape == (4, 10, 10), obs.shape\n",
        "\n",
        "# 2) reversa prohibida\n",
        "start_dir = env.direction\n",
        "rev = env.OPPOSITES[start_dir]\n",
        "_, _ = env.step(rev)\n",
        "assert env.direction == start_dir, 'La reversa directa debe ignorarse'\n",
        "\n",
        "# 3) rewards validos\n",
        "env = SnakeEnv(board_size=10, max_steps=1000)\n",
        "env.reset()\n",
        "for _ in range(30):\n",
        "    r, _ = env.step(random.choice(env.valid_actions()))\n",
        "    assert r in (-1.0, 0.0, 1.0), r\n",
        "\n",
        "# 4) win condition controlada\n",
        "env = SnakeEnv(board_size=2, max_steps=20)\n",
        "env.snake = deque([(1, 1), (0, 1), (0, 0)])\n",
        "env.direction = 3\n",
        "env.food = (1, 0)\n",
        "env.done = False\n",
        "env.steps = 0\n",
        "env.score = 0\n",
        "r, d = env.step(0)  # UP\n",
        "assert d is True and r == 1.0 and env.is_win(), (r, d, env.is_win())\n",
        "\n",
        "print('Smoke checks OK.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# Cell 10: EJECUTAR ENTRENAMIENTO\n",
        "# ============================================================\n",
        "# Ejecuta entrenamiento completo (resume automático si existe checkpoint).\n",
        "best_model = train_alphasnake(config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluación final y exportación\n",
        "Validación final MCTS y export ONNX con chequeo de equivalencia numérica.\n",
        "\n",
        "**Takeaway:** el modelo queda listo para deploy en navegador (`onnxruntime-web`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# Cell 11: Evaluacion Final con MCTS (champion)\n",
        "# ============================================================\n",
        "best_model_path = os.path.join(config.save_dir, 'best_model.pt')\n",
        "if os.path.exists(best_model_path):\n",
        "    best_model = AlphaSnakeNet(4, config.net_blocks, config.net_channels, config.board_size).to(device)\n",
        "    best_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "    print('Mejor modelo cargado desde save_dir.')\n",
        "\n",
        "eval_cfg = get_iteration_config(config, config.max_iterations)\n",
        "eval_cfg = dataclasses.replace(eval_cfg, dir_eps=0.0)\n",
        "\n",
        "print('\\nEvaluacion final MCTS (200 juegos recomendados en fase paper)...')\n",
        "wr, avg_score, scores = evaluate_mcts(best_model, eval_cfg, device, n_games=eval_cfg.eval_games)\n",
        "print(f'Win rate: {wr:.3f} ({int(wr*eval_cfg.eval_games)}/{eval_cfg.eval_games})')\n",
        "print(f'Avg score: {avg_score:.1f} / {config.board_size**2 - 3}')\n",
        "print(f'Min: {min(scores)}, Median: {np.median(scores):.0f}, Max: {max(scores)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# Cell 12: Exportar ONNX + verificacion PyTorch/ONNX\n",
        "# ============================================================\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "\n",
        "best_model.eval()\n",
        "best_model.cpu()\n",
        "\n",
        "dummy_input = torch.randn(1, 4, config.board_size, config.board_size)\n",
        "onnx_path = os.path.join(config.save_dir, 'alphasnake.onnx')\n",
        "\n",
        "torch.onnx.export(\n",
        "    best_model,\n",
        "    dummy_input,\n",
        "    onnx_path,\n",
        "    export_params=True,\n",
        "    opset_version=13,\n",
        "    do_constant_folding=True,\n",
        "    input_names=['state'],\n",
        "    output_names=['policy', 'value'],\n",
        "    dynamic_axes={\n",
        "        'state': {0: 'batch_size'},\n",
        "        'policy': {0: 'batch_size'},\n",
        "        'value': {0: 'batch_size'},\n",
        "    }\n",
        ")\n",
        "\n",
        "model_onnx = onnx.load(onnx_path)\n",
        "onnx.checker.check_model(model_onnx)\n",
        "print(f'Modelo ONNX exportado y verificado: {onnx_path}')\n",
        "\n",
        "ort_session = ort.InferenceSession(onnx_path)\n",
        "test_input = np.random.randn(1, 4, config.board_size, config.board_size).astype(np.float32)\n",
        "\n",
        "with torch.no_grad():\n",
        "    pt_policy, pt_value = best_model(torch.from_numpy(test_input))\n",
        "pt_policy = pt_policy.numpy()\n",
        "pt_value = pt_value.numpy()\n",
        "\n",
        "ort_policy, ort_value = ort_session.run(None, {'state': test_input})\n",
        "print(f'Policy diff max: {np.abs(pt_policy - ort_policy).max():.8f}')\n",
        "print(f'Value diff max:  {np.abs(pt_value - ort_value).max():.8f}')\n",
        "print(f\"Tamaño ONNX: {os.path.getsize(onnx_path) / (1024*1024):.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Integración en el juego\n",
        "Copia `alphasnake.onnx` al directorio del juego y ejecuta `games/snake/ai.html`.\n",
        "\n",
        "**Checklist rápido:**\n",
        "- [ ] `ai.html` arranca en MCTS\n",
        "- [ ] stats de win usan `state.won`\n",
        "- [ ] score AI representa comidas (máx. 97)\n",
        "\n",
        "**Takeaway:** la integración debe adaptar el juego al modelo entrenado, no al revés.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================\n",
        "# Cell 13: Artefactos e integración con el juego\n",
        "# ============================================================\n",
        "print(f'best_model.pt : {os.path.join(config.save_dir, \"best_model.pt\")}')\n",
        "print(f'alphasnake.onnx: {os.path.join(config.save_dir, \"alphasnake.onnx\")}')\n",
        "print('Copiar ONNX a: games/snake/ai/alphasnake.onnx')\n",
        "print('Abrir demo: games/snake/ai.html (modo MCTS por defecto)')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
