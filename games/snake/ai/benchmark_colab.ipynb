{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark CPU & GPU — Colab\n",
    "\n",
    "Script para verificar que CPU y GPU llegan a su máximo rendimiento.\n",
    "Ejecuta celda por celda y observa los resultados."
   ],
   "id": "title"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "import multiprocessing\n",
    "import threading\n",
    "import subprocess\n",
    "import math\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"INFORMACION DEL SISTEMA\")\n",
    "print(\"=\"*60)\n",
    "print(f\"CPU cores (logicos): {multiprocessing.cpu_count()}\")\n",
    "print(f\"CPU cores (fisicos): {os.cpu_count()}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM total: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
    "    print(f\"Compute capability: {torch.cuda.get_device_properties(0).major}.{torch.cuda.get_device_properties(0).minor}\")\n",
    "print()\n",
    "\n",
    "# RAM info\n",
    "try:\n",
    "    with open('/proc/meminfo') as f:\n",
    "        for line in f:\n",
    "            if 'MemTotal' in line:\n",
    "                gb = int(line.split()[1]) / 1e6\n",
    "                print(f\"RAM total: {gb:.1f} GB\")\n",
    "                break\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# CPU model\n",
    "try:\n",
    "    result = subprocess.run(['lscpu'], capture_output=True, text=True)\n",
    "    for line in result.stdout.split('\\n'):\n",
    "        if 'Model name' in line:\n",
    "            print(f\"CPU: {line.split(':')[1].strip()}\")\n",
    "            break\n",
    "except:\n",
    "    pass"
   ],
   "id": "info"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Benchmark GPU — Operaciones matriciales"
   ],
   "id": "gpu-title"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print(\"NO HAY GPU — salta esta celda\")\n",
    "else:\n",
    "    device = torch.device('cuda')\n",
    "    print(\"=\"*60)\n",
    "    print(\"BENCHMARK GPU — Multiplicacion de matrices\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Warmup\n",
    "    a = torch.randn(1000, 1000, device=device)\n",
    "    b = torch.randn(1000, 1000, device=device)\n",
    "    for _ in range(10):\n",
    "        _ = torch.mm(a, b)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    sizes = [1024, 2048, 4096, 8192]\n",
    "    for size in sizes:\n",
    "        a = torch.randn(size, size, device=device)\n",
    "        b = torch.randn(size, size, device=device)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        iters = 20 if size <= 4096 else 5\n",
    "        t0 = time.perf_counter()\n",
    "        for _ in range(iters):\n",
    "            c = torch.mm(a, b)\n",
    "        torch.cuda.synchronize()\n",
    "        elapsed = (time.perf_counter() - t0) / iters\n",
    "\n",
    "        flops = 2 * size**3\n",
    "        tflops = flops / elapsed / 1e12\n",
    "        print(f\"  [{size}x{size}] {elapsed*1000:.2f} ms/op | {tflops:.2f} TFLOPS\")\n",
    "\n",
    "    # FP16 (tensor cores)\n",
    "    print(\"\\n  --- FP16 (Tensor Cores) ---\")\n",
    "    for size in sizes:\n",
    "        a = torch.randn(size, size, device=device, dtype=torch.float16)\n",
    "        b = torch.randn(size, size, device=device, dtype=torch.float16)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        iters = 20 if size <= 4096 else 5\n",
    "        t0 = time.perf_counter()\n",
    "        for _ in range(iters):\n",
    "            c = torch.mm(a, b)\n",
    "        torch.cuda.synchronize()\n",
    "        elapsed = (time.perf_counter() - t0) / iters\n",
    "\n",
    "        flops = 2 * size**3\n",
    "        tflops = flops / elapsed / 1e12\n",
    "        print(f\"  [{size}x{size}] {elapsed*1000:.2f} ms/op | {tflops:.2f} TFLOPS\")\n",
    "\n",
    "    print(\"\\n  GPU matmul benchmark completado.\")"
   ],
   "id": "gpu-matmul"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Benchmark GPU — Inferencia de red neuronal (simula AlphaSnake)"
   ],
   "id": "gpu-nn-title"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print(\"NO HAY GPU — salta esta celda\")\n",
    "else:\n",
    "    import torch.nn as nn\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"BENCHMARK GPU — Inferencia NN (simula AlphaSnake)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    class SimpleResNet(nn.Module):\n",
    "        def __init__(self, ch, blocks, board):\n",
    "            super().__init__()\n",
    "            self.stem = nn.Sequential(nn.Conv2d(4, ch, 3, padding=1), nn.BatchNorm2d(ch), nn.ReLU())\n",
    "            layers = []\n",
    "            for _ in range(blocks):\n",
    "                layers.append(nn.Sequential(\n",
    "                    nn.Conv2d(ch, ch, 3, padding=1), nn.BatchNorm2d(ch), nn.ReLU(),\n",
    "                    nn.Conv2d(ch, ch, 3, padding=1), nn.BatchNorm2d(ch)\n",
    "                ))\n",
    "            self.blocks = nn.ModuleList(layers)\n",
    "            self.fc = nn.Linear(ch * board * board, 4)\n",
    "        def forward(self, x):\n",
    "            x = self.stem(x)\n",
    "            for block in self.blocks:\n",
    "                x = torch.relu(x + block(x))\n",
    "            return self.fc(x.flatten(1))\n",
    "\n",
    "    configs = [\n",
    "        (\"10x10 (64ch, 6 bloques)\", 64, 6, 10),\n",
    "        (\"20x20 (128ch, 10 bloques)\", 128, 10, 20),\n",
    "    ]\n",
    "\n",
    "    for name, ch, blocks, board in configs:\n",
    "        net = SimpleResNet(ch, blocks, board).to(device).eval()\n",
    "        params = sum(p.numel() for p in net.parameters())\n",
    "        print(f\"\\n  {name} — {params:,} params\")\n",
    "\n",
    "        # Batch = 1 (como en MCTS)\n",
    "        for batch in [1, 8, 16, 32, 64]:\n",
    "            x = torch.randn(batch, 4, board, board, device=device)\n",
    "            # Warmup\n",
    "            with torch.no_grad():\n",
    "                for _ in range(20):\n",
    "                    _ = net(x)\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            iters = 200\n",
    "            t0 = time.perf_counter()\n",
    "            with torch.no_grad():\n",
    "                for _ in range(iters):\n",
    "                    _ = net(x)\n",
    "            torch.cuda.synchronize()\n",
    "            elapsed = (time.perf_counter() - t0) / iters\n",
    "            inferences_per_sec = batch / elapsed\n",
    "            print(f\"    batch={batch:3d}: {elapsed*1000:.3f} ms | {inferences_per_sec:.0f} inf/s\")\n",
    "\n",
    "        del net\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\\n  NN inference benchmark completado.\")"
   ],
   "id": "gpu-nn"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Benchmark GPU — Uso maximo (stress test)"
   ],
   "id": "gpu-stress-title"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print(\"NO HAY GPU — salta esta celda\")\n",
    "else:\n",
    "    print(\"=\"*60)\n",
    "    print(\"STRESS TEST GPU — 15 segundos de carga maxima\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Ejecutando... monitorea con nvidia-smi en otra celda.\")\n",
    "\n",
    "    size = 4096\n",
    "    a = torch.randn(size, size, device=device)\n",
    "    b = torch.randn(size, size, device=device)\n",
    "\n",
    "    ops = 0\n",
    "    t_start = time.perf_counter()\n",
    "    duration = 15  # segundos\n",
    "\n",
    "    # Monitor thread\n",
    "    gpu_utils = []\n",
    "    stop_monitor = threading.Event()\n",
    "\n",
    "    def monitor_gpu():\n",
    "        while not stop_monitor.is_set():\n",
    "            try:\n",
    "                r = subprocess.run(\n",
    "                    ['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu,power.draw',\n",
    "                     '--format=csv,noheader,nounits'],\n",
    "                    capture_output=True, text=True, timeout=5\n",
    "                )\n",
    "                parts = r.stdout.strip().split(',')\n",
    "                if len(parts) >= 5:\n",
    "                    gpu_utils.append({\n",
    "                        'util': float(parts[0]),\n",
    "                        'mem_used': float(parts[1]),\n",
    "                        'mem_total': float(parts[2]),\n",
    "                        'temp': float(parts[3]),\n",
    "                        'power': float(parts[4]),\n",
    "                    })\n",
    "            except:\n",
    "                pass\n",
    "            time.sleep(1)\n",
    "\n",
    "    t = threading.Thread(target=monitor_gpu, daemon=True)\n",
    "    t.start()\n",
    "\n",
    "    while time.perf_counter() - t_start < duration:\n",
    "        c = torch.mm(a, b)\n",
    "        ops += 1\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    stop_monitor.set()\n",
    "    t.join(timeout=3)\n",
    "\n",
    "    elapsed = time.perf_counter() - t_start\n",
    "    avg_tflops = (2 * size**3 * ops) / elapsed / 1e12\n",
    "\n",
    "    print(f\"\\n  Duracion: {elapsed:.1f}s\")\n",
    "    print(f\"  Operaciones: {ops}\")\n",
    "    print(f\"  Rendimiento promedio: {avg_tflops:.2f} TFLOPS (FP32)\")\n",
    "\n",
    "    if gpu_utils:\n",
    "        avg_util = np.mean([g['util'] for g in gpu_utils])\n",
    "        max_util = max(g['util'] for g in gpu_utils)\n",
    "        avg_temp = np.mean([g['temp'] for g in gpu_utils])\n",
    "        avg_power = np.mean([g['power'] for g in gpu_utils])\n",
    "        max_mem = max(g['mem_used'] for g in gpu_utils)\n",
    "        print(f\"\\n  GPU Utilization: avg={avg_util:.0f}%, max={max_util:.0f}%\")\n",
    "        print(f\"  VRAM: {max_mem:.0f} / {gpu_utils[0]['mem_total']:.0f} MiB\")\n",
    "        print(f\"  Temperatura: {avg_temp:.0f}°C\")\n",
    "        print(f\"  Potencia: {avg_power:.0f}W\")\n",
    "\n",
    "    del a, b\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\n  Stress test GPU completado.\")"
   ],
   "id": "gpu-stress"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Benchmark CPU — Single-thread y Multi-thread"
   ],
   "id": "cpu-title"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"BENCHMARK CPU\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def cpu_work(n=2_000_000):\n",
    "    \"\"\"Trabajo CPU-bound: calculos matematicos intensivos.\"\"\"\n",
    "    total = 0.0\n",
    "    for i in range(1, n):\n",
    "        total += math.sin(i) * math.cos(i) * math.sqrt(i)\n",
    "    return total\n",
    "\n",
    "# --- Single thread ---\n",
    "print(\"\\n  --- Single Thread ---\")\n",
    "t0 = time.perf_counter()\n",
    "cpu_work(2_000_000)\n",
    "single_time = time.perf_counter() - t0\n",
    "print(f\"  1 thread: {single_time:.2f}s\")\n",
    "\n",
    "# --- Multi thread (ThreadPool — muestra GIL bottleneck) ---\n",
    "print(\"\\n  --- Multi Thread (ThreadPoolExecutor) ---\")\n",
    "print(\"  NOTA: Python tiene GIL, asi que threads NO paralelizan CPU puro.\")\n",
    "print(\"  Esto muestra el overhead real del MCTS paralelo.\")\n",
    "n_threads_list = [1, 4, 8, 16, 32]\n",
    "work_per_thread = 500_000\n",
    "\n",
    "for n_threads in n_threads_list:\n",
    "    t0 = time.perf_counter()\n",
    "    with ThreadPoolExecutor(max_workers=n_threads) as pool:\n",
    "        futures = [pool.submit(cpu_work, work_per_thread) for _ in range(n_threads)]\n",
    "        results = [f.result() for f in futures]\n",
    "    elapsed = time.perf_counter() - t0\n",
    "    speedup = (single_time * n_threads / 4) / elapsed  # relativo\n",
    "    print(f\"  {n_threads:2d} threads: {elapsed:.2f}s (total work = {n_threads}x)\")\n",
    "\n",
    "# --- Multi process (ProcessPool — verdadero paralelismo) ---\n",
    "print(\"\\n  --- Multi Process (ProcessPoolExecutor) ---\")\n",
    "print(\"  Esto SI paraleliza en multiples cores.\")\n",
    "\n",
    "for n_procs in [1, 2, 4, 8]:\n",
    "    t0 = time.perf_counter()\n",
    "    with ProcessPoolExecutor(max_workers=n_procs) as pool:\n",
    "        futures = [pool.submit(cpu_work, 1_000_000) for _ in range(n_procs)]\n",
    "        results = [f.result() for f in futures]\n",
    "    elapsed = time.perf_counter() - t0\n",
    "    speedup = (single_time / 2 * n_procs) / elapsed\n",
    "    print(f\"  {n_procs} procesos: {elapsed:.2f}s | speedup: {speedup:.2f}x\")\n",
    "\n",
    "print(\"\\n  CPU benchmark completado.\")"
   ],
   "id": "cpu-bench"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Benchmark CPU — Simula carga tipo MCTS"
   ],
   "id": "mcts-sim-title"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"BENCHMARK — Simula carga MCTS (Python loops + GPU inference)\")\n",
    "print(\"=\"*60)\n",
    "print(\"Esto replica el patron real: loops Python + inferencia GPU.\")\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Red simple para simular inferencia\n",
    "class TinyNet(nn.Module):\n",
    "    def __init__(self, board=20, ch=128):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(4, ch, 3, padding=1), nn.BatchNorm2d(ch), nn.ReLU(),\n",
    "            nn.Conv2d(ch, ch, 3, padding=1), nn.BatchNorm2d(ch), nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Linear(ch * board * board, 5)\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.conv(x).flatten(1))\n",
    "\n",
    "net = TinyNet(20, 128).to(device).eval()\n",
    "\n",
    "def simulate_mcts_game(net, board_size, num_sims, max_moves):\n",
    "    \"\"\"Simula un juego MCTS: loops Python + inference GPU.\"\"\"\n",
    "    moves = 0\n",
    "    total_inferences = 0\n",
    "    for move in range(max_moves):\n",
    "        # Simula num_sims evaluaciones MCTS\n",
    "        for sim in range(num_sims):\n",
    "            # Trabajo Python (simula tree traversal)\n",
    "            dummy = sum(range(50))  # overhead Python tipico\n",
    "            # Inferencia GPU\n",
    "            x = torch.randn(1, 4, board_size, board_size, device=device)\n",
    "            with torch.no_grad():\n",
    "                _ = net(x)\n",
    "            total_inferences += 1\n",
    "        moves += 1\n",
    "    return moves, total_inferences\n",
    "\n",
    "# Test: un juego corto (10 movimientos, 100 sims)\n",
    "print(\"\\n  Simulando 1 juego (10 moves x 100 sims = 1000 inferences)...\")\n",
    "t0 = time.perf_counter()\n",
    "moves, infs = simulate_mcts_game(net, 20, 100, 10)\n",
    "torch.cuda.synchronize()\n",
    "elapsed = time.perf_counter() - t0\n",
    "print(f\"  Tiempo: {elapsed:.2f}s\")\n",
    "print(f\"  Inferences: {infs}\")\n",
    "print(f\"  Inferences/s: {infs/elapsed:.0f}\")\n",
    "print(f\"  Tiempo por movimiento: {elapsed/moves*1000:.1f} ms\")\n",
    "print(f\"  Tiempo por simulacion: {elapsed/infs*1000:.2f} ms\")\n",
    "\n",
    "# Estimar tiempo por juego completo\n",
    "avg_moves_20x20 = 500  # estimacion conservadora\n",
    "est_time_per_game = elapsed / moves * avg_moves_20x20\n",
    "print(f\"\\n  --- Estimacion para juego completo 20x20 ---\")\n",
    "print(f\"  ~{avg_moves_20x20} moves x 100 sims = {avg_moves_20x20*100:,} inferences\")\n",
    "print(f\"  Tiempo estimado: {est_time_per_game:.0f}s ({est_time_per_game/60:.1f} min)\")\n",
    "print(f\"  100 juegos (1 iter fast): {est_time_per_game*100/3600:.1f} horas\")\n",
    "\n",
    "# Ahora con threading (como el InferenceBatcher)\n",
    "print(\"\\n  --- Con 16 threads paralelos (como InferenceBatcher) ---\")\n",
    "\n",
    "def simulate_short_game(net, board_size):\n",
    "    return simulate_mcts_game(net, board_size, 100, 10)\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "with ThreadPoolExecutor(max_workers=16) as pool:\n",
    "    futures = [pool.submit(simulate_short_game, net, 20) for _ in range(16)]\n",
    "    results = [f.result() for f in futures]\n",
    "torch.cuda.synchronize()\n",
    "elapsed_parallel = time.perf_counter() - t0\n",
    "total_infs = sum(r[1] for r in results)\n",
    "speedup = (elapsed * 16) / elapsed_parallel\n",
    "\n",
    "print(f\"  16 juegos paralelos: {elapsed_parallel:.2f}s\")\n",
    "print(f\"  Total inferences: {total_infs:,}\")\n",
    "print(f\"  Inferences/s: {total_infs/elapsed_parallel:.0f}\")\n",
    "print(f\"  Speedup vs secuencial: {speedup:.2f}x\")\n",
    "\n",
    "est_parallel = est_time_per_game * 100 / speedup\n",
    "print(f\"  100 juegos con paralelismo: {est_parallel/3600:.1f} horas\")\n",
    "\n",
    "# GPU utilization during this\n",
    "try:\n",
    "    r = subprocess.run(\n",
    "        ['nvidia-smi', '--query-gpu=utilization.gpu,memory.used',\n",
    "         '--format=csv,noheader,nounits'],\n",
    "        capture_output=True, text=True, timeout=5\n",
    "    )\n",
    "    print(f\"\\n  GPU actual: {r.stdout.strip()}% util, MiB used\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "del net\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n  MCTS simulation benchmark completado.\")"
   ],
   "id": "mcts-sim"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Resumen y recomendaciones"
   ],
   "id": "summary-title"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"RESUMEN\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "INTERPRETACION DE RESULTADOS:\n",
    "\n",
    "1. GPU Matmul:\n",
    "   - T4:  ~8 TFLOPS FP32, ~65 TFLOPS FP16\n",
    "   - A100: ~20 TFLOPS FP32, ~312 TFLOPS FP16\n",
    "   - H100: ~50 TFLOPS FP32, ~990 TFLOPS FP16\n",
    "   Si ves numeros cercanos a estos, la GPU esta rindiendo bien.\n",
    "\n",
    "2. GPU Stress Test:\n",
    "   - Deberia mostrar ~95-100% utilization.\n",
    "   - Si es menor, hay throttling termico o de potencia.\n",
    "\n",
    "3. NN Inference:\n",
    "   - batch=1 es LENTO en cualquier GPU (latencia domina)\n",
    "   - batch=16+ es donde la GPU brilla\n",
    "   - Esto explica por que MCTS (batch=1) no aprovecha la GPU.\n",
    "\n",
    "4. CPU Threads vs Processes:\n",
    "   - ThreadPool: NO acelera trabajo CPU (GIL de Python)\n",
    "   - ProcessPool: SI acelera (verdadero paralelismo)\n",
    "   - MCTS usa threads (para compartir GPU) -> limitado por GIL\n",
    "\n",
    "5. Simulacion MCTS:\n",
    "   - Muestra el cuello de botella real: Python loops + batch=1\n",
    "   - El speedup con threads viene de la GPU batching, no de CPU\n",
    "\n",
    "CONCLUSION:\n",
    "   La GPU SI funciona bien. El problema es que MCTS en Python\n",
    "   no puede saturar la GPU porque el cuello de botella es el\n",
    "   loop Python (GIL + overhead de objetos). La solucion real\n",
    "   seria MCTS en C++ con bindings Python.\n",
    "\"\"\")"
   ],
   "id": "summary"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
