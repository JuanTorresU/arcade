{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c3005de0",
      "metadata": {},
      "source": [
        "# AlphaSnake 10x10 - Paper-Faithful para Vast.ai\n",
        "\n",
        "Objetivo:\n",
        "- Entrenar un agente Snake con enfoque AlphaZero (MCTS + Policy/Value ResNet) replicando la receta 10x10 del paper.\n",
        "- Exportar un modelo ONNX compatible con `/Users/JuanCamiloTorresUrrego/Documents/tiktok/arcade/games/snake/ai.html`.\n",
        "\n",
        "Criterios de exito:\n",
        "1. Perfil `paper_strict` (default) con hiperparametros paper-faithful.\n",
        "2. Perfil `smoke` para validar pipeline end-to-end en pocos minutos.\n",
        "3. ONNX con contrato fijo: input `state`, outputs `policy` y `value`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0604a895",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 1: Setup reproducible + rutas de artefactos\n",
        "# ============================================================\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "cpu_threads = max(1, min(4, int(os.environ.get('ALPHASNAKE_CPU_THREADS', '4'))))\n",
        "torch.set_num_threads(cpu_threads)\n",
        "torch.set_num_interop_threads(1)\n",
        "\n",
        "DEFAULT_VAST_SAVE_DIR = '/workspace/alphasnake_paper_10x10'\n",
        "if os.path.exists('/workspace'):\n",
        "    SAVE_DIR = os.environ.get('ALPHASNAKE_SAVE_DIR', DEFAULT_VAST_SAVE_DIR)\n",
        "else:\n",
        "    SAVE_DIR = os.environ.get('ALPHASNAKE_SAVE_DIR', os.path.join(os.getcwd(), 'alphasnake_paper_10x10'))\n",
        "\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "print(f'PyTorch: {torch.__version__}')\n",
        "print(f'CUDA disponible: {torch.cuda.is_available()}')\n",
        "print(f'CPU threads torch: {torch.get_num_threads()}')\n",
        "print(f'Directorio de checkpoints/modelo: {SAVE_DIR}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06fded1c",
      "metadata": {},
      "source": [
        "## Perfil de ejecucion\n",
        "\n",
        "Este notebook define dos perfiles:\n",
        "- `paper_strict` (default): reproduccion fiel de parametros paper 10x10.\n",
        "- `smoke`: validacion rapida del pipeline (self-play -> train -> eval -> export).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f556d70",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perfil activo (puedes cambiarlo via variable de entorno ALPHASNAKE_PROFILE)\n",
        "PROFILE = os.environ.get('ALPHASNAKE_PROFILE', 'paper_strict').strip().lower()\n",
        "ALLOWED_PROFILES = {'paper_strict', 'smoke'}\n",
        "if PROFILE not in ALLOWED_PROFILES:\n",
        "    raise ValueError(f'Perfil invalido: {PROFILE}. Opciones: {sorted(ALLOWED_PROFILES)}')\n",
        "\n",
        "print(f'Perfil seleccionado: {PROFILE}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c46913b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 2: Imports + Config central (TrainConfig)\n",
        "# ============================================================\n",
        "import dataclasses\n",
        "from dataclasses import dataclass\n",
        "from collections import deque\n",
        "from queue import Empty\n",
        "import concurrent.futures\n",
        "import math\n",
        "import multiprocessing\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import threading\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    board_size: int = 10\n",
        "    max_steps: int = 1000\n",
        "\n",
        "    net_channels: int = 64\n",
        "    net_blocks: int = 6\n",
        "\n",
        "    num_simulations: int = 400\n",
        "    c_puct: float = 1.0\n",
        "    dir_alpha: float = 0.03\n",
        "    dir_eps: float = 0.25\n",
        "    temp_init: float = 1.0\n",
        "    temp_final: float = 0.0\n",
        "    temp_decay_move: int = 30\n",
        "    food_samples: int = 8\n",
        "\n",
        "    lr: float = 1e-3\n",
        "    weight_decay: float = 1e-4\n",
        "    batch_size: int = 128\n",
        "    buffer_size: int = 200_000\n",
        "    epochs_per_iter: int = 10\n",
        "\n",
        "    games_per_iter: int = 1000\n",
        "    max_iterations: int = 200\n",
        "\n",
        "    eval_games: int = 200\n",
        "    accept_threshold: float = 0.55\n",
        "\n",
        "    save_dir: str = SAVE_DIR\n",
        "    checkpoint_interval: int = 1\n",
        "\n",
        "    selfplay_workers: int = 10\n",
        "    inference_batch_size: int = 128\n",
        "    worker_oversub_factor: float = 3.0\n",
        "    inference_timeout_ms: int = 20\n",
        "    predict_timeout_s: int = 180\n",
        "    use_amp: bool = True\n",
        "\n",
        "    verbose_game_updates: bool = False\n",
        "    game_log_interval: int = 10\n",
        "    game_tick_moves: int = 10\n",
        "    heartbeat_seconds: int = 20\n",
        "\n",
        "\n",
        "def build_config(profile: str, save_dir: str) -> TrainConfig:\n",
        "    if profile == 'smoke':\n",
        "        return TrainConfig(\n",
        "            save_dir=save_dir,\n",
        "            num_simulations=80,\n",
        "            games_per_iter=24,\n",
        "            max_iterations=1,\n",
        "            eval_games=20,\n",
        "            epochs_per_iter=2,\n",
        "            batch_size=64,\n",
        "            buffer_size=20_000,\n",
        "            food_samples=4,\n",
        "            selfplay_workers=4,\n",
        "            inference_batch_size=64,\n",
        "            inference_timeout_ms=10,\n",
        "            use_amp=True,\n",
        "        )\n",
        "    return TrainConfig(save_dir=save_dir)\n",
        "\n",
        "\n",
        "config = build_config(PROFILE, SAVE_DIR)\n",
        "Config = TrainConfig\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "print(f'Device: {device}')\n",
        "print(f'Config activa: {dataclasses.asdict(config)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Takeaway - Config\n",
        "- `paper_strict` mantiene la receta paper 10x10 como default.\n",
        "- `smoke` permite validar el pipeline end-to-end sin esperar horas.\n",
        "- El directorio objetivo de Vast.ai queda en `/workspace/alphasnake_paper_10x10`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c38d38f4",
      "metadata": {},
      "source": [
        "## Entorno Snake (paper-faithful)\n",
        "\n",
        "- Grid: `10x10`\n",
        "- Acciones: `0=UP, 1=DOWN, 2=LEFT, 3=RIGHT`\n",
        "- Sin reversa directa\n",
        "- Observacion: `(4, 10, 10)` en channels-first\n",
        "- Recompensa exacta: `+1` comida, `-1` muerte, `0` otro\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7ce26a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 3: Entorno Snake\n",
        "# ============================================================\n",
        "\n",
        "class SnakeEnv:\n",
        "    \"\"\"\n",
        "    Snake environment estilo AlphaSnake.\n",
        "    Grid board_size x board_size, 4 acciones, observacion (4, H, W).\n",
        "    \"\"\"\n",
        "    # 0=UP, 1=DOWN, 2=LEFT, 3=RIGHT\n",
        "    ACTIONS = {0: (0, -1), 1: (0, 1), 2: (-1, 0), 3: (1, 0)}\n",
        "    OPPOSITES = {0: 1, 1: 0, 2: 3, 3: 2}\n",
        "    ACTION_NAMES = {0: 'UP', 1: 'DOWN', 2: 'LEFT', 3: 'RIGHT'}\n",
        "\n",
        "    def __init__(self, board_size=10, max_steps=1000):\n",
        "        self.board_size = board_size\n",
        "        self.max_steps = max_steps\n",
        "        self.max_score = board_size * board_size - 3  # 97 para 10x10\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        mid = self.board_size // 2\n",
        "        # Snake empieza en el centro, apuntando a la derecha\n",
        "        self.snake = deque([(mid, mid), (mid - 1, mid), (mid - 2, mid)])\n",
        "        self.direction = 3  # RIGHT\n",
        "        self.food = None\n",
        "        self.done = False\n",
        "        self.steps = 0\n",
        "        self.score = 0\n",
        "        self._place_food()\n",
        "        return self.get_state()\n",
        "\n",
        "    def _get_free_cells(self):\n",
        "        snake_set = set(self.snake)\n",
        "        return [(x, y) for y in range(self.board_size)\n",
        "                for x in range(self.board_size)\n",
        "                if (x, y) not in snake_set]\n",
        "\n",
        "    def _place_food(self):\n",
        "        free = self._get_free_cells()\n",
        "        if free:\n",
        "            self.food = random.choice(free)\n",
        "        else:\n",
        "            # Tablero lleno = victoria\n",
        "            self.food = None\n",
        "            self.done = True\n",
        "\n",
        "    def get_state(self):\n",
        "        \"\"\"Retorna observacion (4, board_size, board_size) float32.\"\"\"\n",
        "        s = np.zeros((4, self.board_size, self.board_size), dtype=np.float32)\n",
        "        # Canal 0: cuerpo\n",
        "        for x, y in self.snake:\n",
        "            s[0, y, x] = 1.0\n",
        "        # Canal 1: cabeza\n",
        "        hx, hy = self.snake[0]\n",
        "        s[1, hy, hx] = 1.0\n",
        "        # Canal 2: comida\n",
        "        if self.food:\n",
        "            fx, fy = self.food\n",
        "            s[2, fy, fx] = 1.0\n",
        "        # Canal 3: direccion (valor constante normalizado)\n",
        "        # UP=0.25, DOWN=0.5, LEFT=0.75, RIGHT=1.0\n",
        "        s[3, :, :] = (self.direction + 1) / 4.0\n",
        "        return s\n",
        "\n",
        "    def valid_actions(self):\n",
        "        \"\"\"Acciones validas (excluye reversa directa).\"\"\"\n",
        "        rev = self.OPPOSITES[self.direction]\n",
        "        return [a for a in range(4) if a != rev]\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Ejecuta accion. Retorna (reward, done).\n",
        "        reward: +1.0 comida, -1.0 muerte, 0.0 otro.\n",
        "        \"\"\"\n",
        "        if self.done:\n",
        "            return 0.0, True\n",
        "\n",
        "        # Prohibir reversa directa: si intenta reversa, mantener direccion\n",
        "        if action == self.OPPOSITES[self.direction]:\n",
        "            action = self.direction\n",
        "\n",
        "        self.direction = action\n",
        "        dx, dy = self.ACTIONS[action]\n",
        "        hx, hy = self.snake[0]\n",
        "        nx, ny = hx + dx, hy + dy\n",
        "\n",
        "        # Colision con pared\n",
        "        if nx < 0 or nx >= self.board_size or ny < 0 or ny >= self.board_size:\n",
        "            self.done = True\n",
        "            return -1.0, True\n",
        "\n",
        "        # Verificar si comera comida (antes de mover)\n",
        "        ate_food = self.food is not None and (nx, ny) == self.food\n",
        "\n",
        "        # Colision con cuerpo\n",
        "        # Si NO come comida, la cola se movera, asi que es valido ir a la pos actual de la cola\n",
        "        body_set = set(self.snake)\n",
        "        if not ate_food:\n",
        "            body_set.discard(self.snake[-1])\n",
        "        if (nx, ny) in body_set:\n",
        "            self.done = True\n",
        "            return -1.0, True\n",
        "\n",
        "        # Mover\n",
        "        self.snake.appendleft((nx, ny))\n",
        "        if ate_food:\n",
        "            self.score += 1\n",
        "            self._place_food()\n",
        "            if self.done:  # _place_food puso done=True si el tablero esta lleno\n",
        "                return 1.0, True\n",
        "        else:\n",
        "            self.snake.pop()\n",
        "\n",
        "        self.steps += 1\n",
        "        if self.steps >= self.max_steps:\n",
        "            self.done = True\n",
        "            return 0.0, True\n",
        "\n",
        "        return (1.0 if ate_food else 0.0), False\n",
        "\n",
        "    def is_win(self):\n",
        "        \"\"\"True si la serpiente lleno todo el tablero.\"\"\"\n",
        "        return len(self.snake) >= self.board_size ** 2\n",
        "\n",
        "    def clone(self):\n",
        "        \"\"\"Copia eficiente del entorno.\"\"\"\n",
        "        env = SnakeEnv.__new__(SnakeEnv)\n",
        "        env.board_size = self.board_size\n",
        "        env.max_steps = self.max_steps\n",
        "        env.max_score = self.max_score\n",
        "        env.snake = deque(self.snake)\n",
        "        env.direction = self.direction\n",
        "        env.food = self.food\n",
        "        env.done = self.done\n",
        "        env.steps = self.steps\n",
        "        env.score = self.score\n",
        "        return env\n",
        "\n",
        "# --- Test rapido ---\n",
        "env = SnakeEnv(10, 1000)\n",
        "s = env.reset()\n",
        "print(f\"State shape: {s.shape}\")\n",
        "print(f\"Snake length: {len(env.snake)}, food: {env.food}\")\n",
        "print(f\"Valid actions: {[env.ACTION_NAMES[a] for a in env.valid_actions()]}\")\n",
        "\n",
        "# Jugar unos pasos random\n",
        "for _ in range(20):\n",
        "    a = random.choice(env.valid_actions())\n",
        "    r, done = env.step(a)\n",
        "    if done:\n",
        "        print(f\"Game over! Score: {env.score}, Win: {env.is_win()}\")\n",
        "        break\n",
        "else:\n",
        "    print(f\"After 20 steps: score={env.score}, length={len(env.snake)}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 3b: Sanity checks del entorno (contrato minimo)\n",
        "# ============================================================\n",
        "\n",
        "def run_env_sanity_checks():\n",
        "    env = SnakeEnv(10, 1000)\n",
        "    s = env.reset()\n",
        "    assert s.shape == (4, 10, 10), f'Shape invalido: {s.shape}'\n",
        "\n",
        "    env = SnakeEnv(10, 1000)\n",
        "    env.reset()\n",
        "    env.direction = 3\n",
        "    _, _ = env.step(2)\n",
        "    assert env.direction == 3, 'Se permitio reversa directa'\n",
        "\n",
        "    env = SnakeEnv(10, 1000)\n",
        "    env.reset()\n",
        "    hx, hy = env.snake[0]\n",
        "    env.direction = 3\n",
        "    env.food = (hx + 1, hy)\n",
        "    r, done = env.step(3)\n",
        "    assert r == 1.0 and not done, f'Reward comer invalido: r={r}, done={done}'\n",
        "\n",
        "    env = SnakeEnv(10, 1000)\n",
        "    env.reset()\n",
        "    env.food = (0, 0)\n",
        "    r, done = env.step(3)\n",
        "    assert r == 0.0 and not done, f'Reward paso invalido: r={r}, done={done}'\n",
        "\n",
        "    env = SnakeEnv(10, 1000)\n",
        "    env.reset()\n",
        "    env.snake = deque([(0, 0), (0, 1), (0, 2)])\n",
        "    env.direction = 2\n",
        "    r, done = env.step(2)\n",
        "    assert r == -1.0 and done, f'Reward muerte invalido: r={r}, done={done}'\n",
        "\n",
        "run_env_sanity_checks()\n",
        "print('Sanity checks del entorno: OK')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Takeaway - Entorno\n",
        "El contrato del entorno quedo alineado con paper: acciones discretas, no reversa,\n",
        "observacion `(4,10,10)` y rewards exactos `+1/0/-1`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89e329bd",
      "metadata": {},
      "source": [
        "## Red Neuronal: AlphaSnakeNet\n",
        "\n",
        "Arquitectura AlphaZero reducida:\n",
        "- Stem conv `3x3` con 64 canales\n",
        "- `6` bloques residuales\n",
        "- Policy head -> `softmax` de 4 acciones\n",
        "- Value head -> `tanh` en `[-1, 1]`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "331a6943",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 4: Red Neuronal \u2014 AlphaSnakeNet\n",
        "# ============================================================\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    \"\"\"Bloque residual: Conv\u2192BN\u2192ReLU\u2192Conv\u2192BN\u2192skip\u2192ReLU\"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(channels)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out = F.relu(out + residual)\n",
        "        return out\n",
        "\n",
        "\n",
        "class AlphaSnakeNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Policy + Value network estilo AlphaZero.\n",
        "    Input:  (batch, 4, board_size, board_size)\n",
        "    Output: policy (batch, 4), value (batch, 1)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=4, num_blocks=6, channels=64, board_size=10):\n",
        "        super().__init__()\n",
        "        self.board_size = board_size\n",
        "\n",
        "        # Stem\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, channels, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Residual tower\n",
        "        self.res_tower = nn.Sequential(*[ResBlock(channels) for _ in range(num_blocks)])\n",
        "\n",
        "        # Policy head\n",
        "        self.policy_conv = nn.Conv2d(channels, 2, 1, bias=False)\n",
        "        self.policy_bn = nn.BatchNorm2d(2)\n",
        "        self.policy_fc = nn.Linear(2 * board_size * board_size, 4)\n",
        "\n",
        "        # Value head\n",
        "        self.value_conv = nn.Conv2d(channels, 1, 1, bias=False)\n",
        "        self.value_bn = nn.BatchNorm2d(1)\n",
        "        self.value_fc1 = nn.Linear(board_size * board_size, 64)\n",
        "        self.value_fc2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shared trunk\n",
        "        x = self.stem(x)\n",
        "        x = self.res_tower(x)\n",
        "\n",
        "        # Policy\n",
        "        p = F.relu(self.policy_bn(self.policy_conv(x)))\n",
        "        p = p.view(p.size(0), -1)\n",
        "        p = F.softmax(self.policy_fc(p), dim=1)\n",
        "\n",
        "        # Value\n",
        "        v = F.relu(self.value_bn(self.value_conv(x)))\n",
        "        v = v.view(v.size(0), -1)\n",
        "        v = F.relu(self.value_fc1(v))\n",
        "        v = torch.tanh(self.value_fc2(v))\n",
        "\n",
        "        return p, v\n",
        "\n",
        "# --- Test ---\n",
        "net = AlphaSnakeNet(\n",
        "    in_channels=4,\n",
        "    num_blocks=config.net_blocks,\n",
        "    channels=config.net_channels,\n",
        "    board_size=config.board_size\n",
        ").to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in net.parameters())\n",
        "print(f\"AlphaSnakeNet: {total_params:,} parametros\")\n",
        "\n",
        "dummy = torch.randn(1, 4, 10, 10).to(device)\n",
        "pol, val = net(dummy)\n",
        "print(f\"Policy shape: {pol.shape}, sum={pol.sum().item():.4f}\")\n",
        "print(f\"Value shape: {val.shape}, val={val.item():.4f}\")\n",
        "del net, dummy  # liberar memoria"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cf1c9f0",
      "metadata": {},
      "source": [
        "## MCTS - Monte Carlo Tree Search\n",
        "\n",
        "Implementacion AlphaZero con:\n",
        "- UCB PUCT\n",
        "- Dirichlet noise en raiz (`alpha=0.03`, `eps=0.25`)\n",
        "- Mascara de acciones invalidas\n",
        "- Manejo de estocasticidad de comida (`food_samples`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "197ece1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 5: MCTS \u2014 Monte Carlo Tree Search\n",
        "# ============================================================\n",
        "\n",
        "class MCTSNode:\n",
        "    \"\"\"Nodo del arbol MCTS.\"\"\"\n",
        "    __slots__ = ['prior', 'visit_count', 'value_sum', 'children',\n",
        "                 'is_expanded', 'env', 'food_eaten']\n",
        "\n",
        "    def __init__(self, prior=0.0):\n",
        "        self.prior = prior\n",
        "        self.visit_count = 0\n",
        "        self.value_sum = 0.0\n",
        "        self.children = {}        # action (int) -> MCTSNode\n",
        "        self.is_expanded = False\n",
        "        self.env = None           # SnakeEnv snapshot\n",
        "        self.food_eaten = False   # True si la transicion a este nodo comio comida\n",
        "\n",
        "    def value(self):\n",
        "        if self.visit_count == 0:\n",
        "            return 0.0\n",
        "        return self.value_sum / self.visit_count\n",
        "\n",
        "\n",
        "class MCTS:\n",
        "    \"\"\"\n",
        "    Monte Carlo Tree Search estilo AlphaZero con soporte\n",
        "    para estocasticidad de comida (food sampling).\n",
        "    Acepta predict_fn y predict_batch_fn para permitir\n",
        "    batching de inferencia desde multiples hilos.\n",
        "    \"\"\"\n",
        "    def __init__(self, predict_fn, cfg, predict_batch_fn=None):\n",
        "        self.predict_fn = predict_fn\n",
        "        self.predict_batch_fn = predict_batch_fn or (lambda states: [predict_fn(s) for s in states])\n",
        "        self.c_puct = cfg.c_puct\n",
        "        self.num_simulations = cfg.num_simulations\n",
        "        self.dir_alpha = cfg.dir_alpha\n",
        "        self.dir_eps = cfg.dir_eps\n",
        "        self.food_samples = cfg.food_samples\n",
        "\n",
        "    def _predict(self, state_np):\n",
        "        \"\"\"Inferir policy y value.\"\"\"\n",
        "        return self.predict_fn(state_np)\n",
        "\n",
        "    def _ucb_score(self, parent, child):\n",
        "        \"\"\"UCB score clasico AlphaZero.\"\"\"\n",
        "        q = child.value()\n",
        "        u = self.c_puct * child.prior * math.sqrt(parent.visit_count) / (1 + child.visit_count)\n",
        "        return q + u\n",
        "\n",
        "    def _select_child(self, node):\n",
        "        \"\"\"Seleccionar hijo con mayor UCB score.\"\"\"\n",
        "        best_score = -float('inf')\n",
        "        best_action = -1\n",
        "        best_child = None\n",
        "        for action, child in node.children.items():\n",
        "            score = self._ucb_score(node, child)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_action = action\n",
        "                best_child = child\n",
        "        return best_action, best_child\n",
        "\n",
        "    def _expand(self, node):\n",
        "        \"\"\"\n",
        "        Expandir nodo hoja: evaluar con la red, crear hijos.\n",
        "        Maneja estocasticidad de comida promediando valores.\n",
        "        Retorna el valor del nodo.\n",
        "        \"\"\"\n",
        "        env = node.env\n",
        "        if env.done:\n",
        "            return 1.0 if env.is_win() else -1.0\n",
        "\n",
        "        policy, value = self._predict(env.get_state())\n",
        "\n",
        "        # --- Food stochasticity (batched via predict_batch_fn) ---\n",
        "        if node.food_eaten and self.food_samples > 1:\n",
        "            snake_set = set(env.snake)\n",
        "            free = [(x, y) for y in range(env.board_size)\n",
        "                    for x in range(env.board_size)\n",
        "                    if (x, y) not in snake_set and (x, y) != env.food]\n",
        "            k = min(self.food_samples - 1, len(free))\n",
        "            if k > 0:\n",
        "                sampled_positions = random.sample(free, k)\n",
        "                food_states = []\n",
        "                for food_pos in sampled_positions:\n",
        "                    env_copy = env.clone()\n",
        "                    env_copy.food = food_pos\n",
        "                    food_states.append(env_copy.get_state())\n",
        "                results = self.predict_batch_fn(food_states)\n",
        "                food_values = [r[1] for r in results]\n",
        "                value = (value + sum(food_values)) / (1 + len(food_values))\n",
        "\n",
        "        # Mascara de acciones validas y renormalizacion\n",
        "        valid = env.valid_actions()\n",
        "        mask = np.zeros(4, dtype=np.float32)\n",
        "        for a in valid:\n",
        "            mask[a] = 1.0\n",
        "        policy = policy * mask\n",
        "        total = policy.sum()\n",
        "        if total > 0:\n",
        "            policy /= total\n",
        "        else:\n",
        "            policy = mask / mask.sum()\n",
        "\n",
        "        # Crear hijos\n",
        "        for a in valid:\n",
        "            node.children[a] = MCTSNode(prior=policy[a])\n",
        "\n",
        "        node.is_expanded = True\n",
        "        return value\n",
        "\n",
        "    def _add_dirichlet_noise(self, node):\n",
        "        \"\"\"Agregar Dirichlet noise al root para exploracion.\"\"\"\n",
        "        actions = list(node.children.keys())\n",
        "        if not actions:\n",
        "            return\n",
        "        noise = np.random.dirichlet([self.dir_alpha] * len(actions))\n",
        "        for i, a in enumerate(actions):\n",
        "            node.children[a].prior = (\n",
        "                (1 - self.dir_eps) * node.children[a].prior +\n",
        "                self.dir_eps * noise[i]\n",
        "            )\n",
        "\n",
        "    def search(self, root_env, temperature=1.0):\n",
        "        \"\"\"\n",
        "        Ejecutar MCTS completo desde un estado raiz.\n",
        "        Retorna distribucion de probabilidad sobre acciones (4,).\n",
        "        \"\"\"\n",
        "        root = MCTSNode()\n",
        "        root.env = root_env.clone()\n",
        "\n",
        "        # Expandir raiz\n",
        "        self._expand(root)\n",
        "        self._add_dirichlet_noise(root)\n",
        "\n",
        "        # Simulaciones\n",
        "        for _ in range(self.num_simulations):\n",
        "            node = root\n",
        "            path = [node]\n",
        "\n",
        "            # --- SELECT ---\n",
        "            while node.is_expanded and node.children:\n",
        "                action, child = self._select_child(node)\n",
        "\n",
        "                # Computacion lazy del estado del hijo\n",
        "                if child.env is None:\n",
        "                    env_copy = node.env.clone()\n",
        "                    old_score = env_copy.score\n",
        "                    env_copy.step(action)\n",
        "                    child.env = env_copy\n",
        "                    child.food_eaten = (env_copy.score > old_score and not env_copy.done)\n",
        "\n",
        "                node = child\n",
        "                path.append(node)\n",
        "\n",
        "            # --- EXPAND & EVALUATE ---\n",
        "            if not node.is_expanded:\n",
        "                value = self._expand(node)\n",
        "            else:\n",
        "                # Nodo terminal (expanded pero sin hijos)\n",
        "                value = 1.0 if node.env.is_win() else -1.0\n",
        "\n",
        "            # --- BACKUP ---\n",
        "            for n in reversed(path):\n",
        "                n.visit_count += 1\n",
        "                n.value_sum += value\n",
        "\n",
        "        # --- Construir distribucion de acciones ---\n",
        "        visits = np.zeros(4, dtype=np.float32)\n",
        "        for a, child in root.children.items():\n",
        "            visits[a] = child.visit_count\n",
        "\n",
        "        if temperature == 0:\n",
        "            # Greedy\n",
        "            best = np.argmax(visits)\n",
        "            probs = np.zeros(4, dtype=np.float32)\n",
        "            probs[best] = 1.0\n",
        "        else:\n",
        "            # Con temperatura\n",
        "            visits_temp = np.power(visits, 1.0 / temperature)\n",
        "            total = visits_temp.sum()\n",
        "            if total > 0:\n",
        "                probs = visits_temp / total\n",
        "            else:\n",
        "                probs = np.ones(4, dtype=np.float32) / 4.0\n",
        "\n",
        "        return probs\n",
        "\n",
        "print(\"MCTS implementado correctamente.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Takeaway - MCTS\n",
        "La busqueda incluye ruido Dirichlet en raiz para exploracion durante self-play y\n",
        "muestreo de comida (`food_samples`) para manejar estocasticidad del entorno.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ade956a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 5b: InferenceBatcher \u2014 Batching de GPU para self-play paralelo\n",
        "# ============================================================\n",
        "import queue as queue_module\n",
        "import concurrent.futures\n",
        "\n",
        "class InferenceBatcher:\n",
        "    \"\"\"\n",
        "    Recolecta requests de inferencia de multiples hilos y los\n",
        "    procesa en batches en la GPU. Esto maximiza la utilizacion\n",
        "    de la GPU durante self-play paralelo.\n",
        "    \"\"\"\n",
        "    def __init__(self, net, device, max_batch=64, timeout_ms=3):\n",
        "        self.net = net\n",
        "        self.device = device\n",
        "        self.max_batch = max_batch\n",
        "        self.timeout = timeout_ms / 1000.0\n",
        "        self._queue = queue_module.Queue()\n",
        "        self._running = False\n",
        "        self._thread = None\n",
        "        self._total_calls = 0\n",
        "        self._total_batches = 0\n",
        "\n",
        "    def start(self):\n",
        "        self._running = True\n",
        "        self._total_calls = 0\n",
        "        self._total_batches = 0\n",
        "        self._thread = threading.Thread(target=self._worker, daemon=True)\n",
        "        self._thread.start()\n",
        "        return self\n",
        "\n",
        "    def stop(self):\n",
        "        self._running = False\n",
        "        self._queue.put(None)  # sentinel\n",
        "        if self._thread:\n",
        "            self._thread.join(timeout=10)\n",
        "\n",
        "    def submit(self, state_np):\n",
        "        \"\"\"Non-blocking: enviar estado, retorna Future.\"\"\"\n",
        "        f = concurrent.futures.Future()\n",
        "        self._queue.put((state_np, f))\n",
        "        return f\n",
        "\n",
        "    def predict(self, state_np):\n",
        "        \"\"\"Blocking: enviar estado, esperar resultado (policy, value).\"\"\"\n",
        "        return self.submit(state_np).result()\n",
        "\n",
        "    def predict_batch(self, states_list):\n",
        "        \"\"\"Enviar multiples estados, esperar todos los resultados.\"\"\"\n",
        "        if not states_list:\n",
        "            return []\n",
        "        futures = [self.submit(s) for s in states_list]\n",
        "        return [f.result() for f in futures]\n",
        "\n",
        "    def stats(self):\n",
        "        avg = self._total_calls / max(self._total_batches, 1)\n",
        "        return self._total_calls, self._total_batches, avg\n",
        "\n",
        "    def _worker(self):\n",
        "        while self._running:\n",
        "            batch = []\n",
        "            # Esperar primer request\n",
        "            try:\n",
        "                item = self._queue.get(timeout=1.0)\n",
        "            except queue_module.Empty:\n",
        "                continue\n",
        "            if item is None:\n",
        "                break\n",
        "            batch.append(item)\n",
        "\n",
        "            # Recolectar mas requests (hasta max_batch o timeout)\n",
        "            deadline = time.time() + self.timeout\n",
        "            while len(batch) < self.max_batch:\n",
        "                remaining = deadline - time.time()\n",
        "                if remaining <= 0:\n",
        "                    break\n",
        "                try:\n",
        "                    item = self._queue.get(timeout=max(0.0001, remaining))\n",
        "                except queue_module.Empty:\n",
        "                    break\n",
        "                if item is None:\n",
        "                    self._running = False\n",
        "                    break\n",
        "                batch.append(item)\n",
        "\n",
        "            if not batch:\n",
        "                continue\n",
        "\n",
        "            states = [b[0] for b in batch]\n",
        "            futures_list = [b[1] for b in batch]\n",
        "            self._total_calls += len(batch)\n",
        "            self._total_batches += 1\n",
        "\n",
        "            try:\n",
        "                arr = np.stack(states)\n",
        "                t = torch.as_tensor(arr, device=self.device)\n",
        "                with torch.no_grad():\n",
        "                    policies, values = self.net(t)\n",
        "                p_np = policies.cpu().numpy()\n",
        "                v_np = values.cpu().numpy()\n",
        "                for i, f in enumerate(futures_list):\n",
        "                    f.set_result((p_np[i], float(v_np[i, 0])))\n",
        "            except Exception as e:\n",
        "                for f in futures_list:\n",
        "                    if not f.done():\n",
        "                        f.set_exception(e)\n",
        "\n",
        "\n",
        "def make_predict_fn(net, device):\n",
        "    \"\"\"Crear funcion de prediccion directa (para uso single-thread, eval).\"\"\"\n",
        "    @torch.no_grad()\n",
        "    def predict(state_np):\n",
        "        t = torch.as_tensor(state_np, device=device).unsqueeze(0)\n",
        "        p, v = net(t)\n",
        "        return p[0].cpu().numpy(), v.item()\n",
        "    return predict\n",
        "\n",
        "print(\"InferenceBatcher implementado.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe45e252",
      "metadata": {},
      "source": [
        "## Self-play -> Replay Buffer -> Train -> Eval\n",
        "\n",
        "Pipeline por iteracion:\n",
        "1. Self-play con MCTS para generar `(state, pi, z)`\n",
        "2. Entrenamiento por batches en replay buffer\n",
        "3. Evaluacion MCTS real\n",
        "4. Champion update + checkpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc64eb4a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 6: Self-Play\n",
        "# ============================================================\n",
        "\n",
        "def self_play_game(predict_fn, cfg, predict_batch_fn=None, progress_cb=None):\n",
        "    \"\"\"\n",
        "    Jugar una partida completa usando MCTS.\n",
        "    predict_fn: callable(state_np) -> (policy_np, value_float)\n",
        "    predict_batch_fn: callable(list[state_np]) -> list[(policy, value)]\n",
        "    progress_cb: callable(move_count, score, length) opcional para telemetria.\n",
        "    Retorna: (examples, is_win, score, num_moves)\n",
        "    \"\"\"\n",
        "    env = SnakeEnv(cfg.board_size, cfg.max_steps)\n",
        "    mcts = MCTS(predict_fn, cfg, predict_batch_fn)\n",
        "    env.reset()\n",
        "\n",
        "    states = []\n",
        "    policies = []\n",
        "    move_count = 0\n",
        "\n",
        "    while not env.done:\n",
        "        # Temperatura: 1.0 hasta move temp_decay_move, luego 0.0\n",
        "        temp = cfg.temp_init if move_count < cfg.temp_decay_move else cfg.temp_final\n",
        "\n",
        "        # Busqueda MCTS\n",
        "        pi = mcts.search(env, temperature=temp)\n",
        "\n",
        "        # Guardar datos de entrenamiento\n",
        "        states.append(env.get_state().copy())\n",
        "        policies.append(pi.copy())\n",
        "\n",
        "        # Seleccionar accion\n",
        "        if temp == 0:\n",
        "            action = int(np.argmax(pi))\n",
        "        else:\n",
        "            action = int(np.random.choice(4, p=pi))\n",
        "\n",
        "        # Ejecutar\n",
        "        env.step(action)\n",
        "        move_count += 1\n",
        "        if progress_cb is not None:\n",
        "            tick_every = max(1, int(getattr(cfg, \"game_tick_moves\", 10)))\n",
        "            if move_count == 1 or move_count % tick_every == 0:\n",
        "                progress_cb(move_count, env.score, len(env.snake))\n",
        "\n",
        "    # Determinar resultado: +1 si gano (lleno tablero), -1 si no\n",
        "    z = 1.0 if env.is_win() else -1.0\n",
        "\n",
        "    # Crear ejemplos de entrenamiento\n",
        "    examples = [(s, p, z) for s, p in zip(states, policies)]\n",
        "    return examples, env.is_win(), env.score, move_count, len(env.snake)\n",
        "\n",
        "\n",
        "def run_n_games_with_predict(n, predict_fn, predict_batch_fn, progress_queue, worker_id, cfg):\n",
        "    \"\"\"\n",
        "    Ejecutar n partidas de self-play (thread-safe), usando funciones de inferencia ya construidas.\n",
        "    \"\"\"\n",
        "    all_examples = []\n",
        "    wins = 0\n",
        "    scores_list = []\n",
        "    moves_list = []\n",
        "    lengths_list = []\n",
        "\n",
        "    if progress_queue is not None:\n",
        "        progress_queue.put({\"type\": \"worker_start\", \"worker\": worker_id, \"games\": n})\n",
        "\n",
        "    for game_idx in range(n):\n",
        "        if progress_queue is not None:\n",
        "            progress_queue.put({\"type\": \"game_start\", \"worker\": worker_id, \"game_local\": game_idx + 1})\n",
        "\n",
        "        def _progress_cb(move_count, score_now, len_now):\n",
        "            if progress_queue is not None:\n",
        "                progress_queue.put({\n",
        "                    \"type\": \"game_tick\",\n",
        "                    \"worker\": worker_id,\n",
        "                    \"game_local\": game_idx + 1,\n",
        "                    \"moves\": move_count,\n",
        "                    \"score\": score_now,\n",
        "                    \"length\": len_now,\n",
        "                })\n",
        "\n",
        "        try:\n",
        "            examples, won, score, moves, length = self_play_game(\n",
        "                predict_fn,\n",
        "                cfg,\n",
        "                predict_batch_fn,\n",
        "                progress_cb=_progress_cb,\n",
        "            )\n",
        "        except Exception as exc:\n",
        "            if progress_queue is not None:\n",
        "                progress_queue.put({\n",
        "                    \"type\": \"worker_error\",\n",
        "                    \"worker\": worker_id,\n",
        "                    \"game_local\": game_idx + 1,\n",
        "                    \"message\": f\"{type(exc).__name__}: {exc}\",\n",
        "                })\n",
        "            raise\n",
        "\n",
        "        all_examples.extend(examples)\n",
        "        if won:\n",
        "            wins += 1\n",
        "        scores_list.append(score)\n",
        "        moves_list.append(moves)\n",
        "        lengths_list.append(length)\n",
        "\n",
        "        if progress_queue is not None:\n",
        "            progress_queue.put({\n",
        "                \"type\": \"game_end\",\n",
        "                \"worker\": worker_id,\n",
        "                \"game_local\": game_idx + 1,\n",
        "                \"won\": won,\n",
        "                \"score\": score,\n",
        "                \"moves\": moves,\n",
        "                \"length\": length,\n",
        "                \"examples\": len(examples),\n",
        "            })\n",
        "\n",
        "    return all_examples, wins, scores_list, moves_list, lengths_list\n",
        "\n",
        "\n",
        "def run_n_games(n, request_queue, response_queue, progress_queue, worker_id, cfg):\n",
        "    \"\"\"\n",
        "    Compat con version de procesos: arma predict_fn sobre colas y delega al runner generico.\n",
        "    \"\"\"\n",
        "    def predict(state_np):\n",
        "        request_queue.put((worker_id, False, state_np))\n",
        "        timeout_s = float(getattr(cfg, \"predict_timeout_s\", 180))\n",
        "        try:\n",
        "            resp = response_queue.get(timeout=timeout_s)\n",
        "        except Empty:\n",
        "            if progress_queue is not None:\n",
        "                progress_queue.put({\n",
        "                    \"type\": \"worker_error\",\n",
        "                    \"worker\": worker_id,\n",
        "                    \"game_local\": -1,\n",
        "                    \"message\": f\"Timeout esperando inferencia ({timeout_s:.0f}s)\",\n",
        "                })\n",
        "            raise TimeoutError(f\"Worker {worker_id}: timeout esperando inferencia ({timeout_s:.0f}s)\")\n",
        "\n",
        "        if isinstance(resp, dict) and resp.get(\"type\") == \"inference_error\":\n",
        "            msg = resp.get(\"message\", \"error desconocido de inference_server\")\n",
        "            if progress_queue is not None:\n",
        "                progress_queue.put({\n",
        "                    \"type\": \"worker_error\",\n",
        "                    \"worker\": worker_id,\n",
        "                    \"game_local\": -1,\n",
        "                    \"message\": msg,\n",
        "                })\n",
        "            raise RuntimeError(f\"Worker {worker_id}: inference_server error -> {msg}\")\n",
        "\n",
        "        return resp\n",
        "\n",
        "    def predict_batch(states_list):\n",
        "        if not states_list:\n",
        "            return []\n",
        "\n",
        "        request_queue.put((worker_id, True, states_list))\n",
        "        timeout_s = float(getattr(cfg, \"predict_timeout_s\", 180))\n",
        "        try:\n",
        "            resp = response_queue.get(timeout=timeout_s)\n",
        "        except Empty:\n",
        "            if progress_queue is not None:\n",
        "                progress_queue.put({\n",
        "                    \"type\": \"worker_error\",\n",
        "                    \"worker\": worker_id,\n",
        "                    \"game_local\": -1,\n",
        "                    \"message\": f\"Timeout esperando inferencia batch ({timeout_s:.0f}s)\",\n",
        "                })\n",
        "            raise TimeoutError(f\"Worker {worker_id}: timeout esperando inferencia batch ({timeout_s:.0f}s)\")\n",
        "\n",
        "        if isinstance(resp, dict) and resp.get(\"type\") == \"inference_error\":\n",
        "            msg = resp.get(\"message\", \"error desconocido de inference_server\")\n",
        "            if progress_queue is not None:\n",
        "                progress_queue.put({\n",
        "                    \"type\": \"worker_error\",\n",
        "                    \"worker\": worker_id,\n",
        "                    \"game_local\": -1,\n",
        "                    \"message\": msg,\n",
        "                })\n",
        "            raise RuntimeError(f\"Worker {worker_id}: inference_server error -> {msg}\")\n",
        "\n",
        "        if not isinstance(resp, list):\n",
        "            raise RuntimeError(f\"Worker {worker_id}: respuesta batch invalida: {type(resp)}\")\n",
        "\n",
        "        return resp\n",
        "\n",
        "    return run_n_games_with_predict(n, predict, predict_batch, progress_queue, worker_id, cfg)\n",
        "\n",
        "\n",
        "def run_n_games_process_entry(n, request_queue, response_queue, progress_queue, result_queue, worker_id, cfg):\n",
        "    \"\"\"\n",
        "    Entry-point para worker de proceso.\n",
        "    Ejecuta run_n_games y envia resultado/errores por result_queue.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Evitar oversubscription dentro de cada proceso worker\n",
        "        try:\n",
        "            torch.set_num_threads(1)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        result = run_n_games(n, request_queue, response_queue, progress_queue, worker_id, cfg)\n",
        "        result_queue.put({\n",
        "            \"type\": \"worker_done\",\n",
        "            \"worker\": worker_id,\n",
        "            \"result\": result,\n",
        "        })\n",
        "    except Exception as exc:\n",
        "        msg = f\"{type(exc).__name__}: {exc}\"\n",
        "        if progress_queue is not None:\n",
        "            progress_queue.put({\n",
        "                \"type\": \"worker_error\",\n",
        "                \"worker\": worker_id,\n",
        "                \"game_local\": -1,\n",
        "                \"message\": msg,\n",
        "            })\n",
        "        try:\n",
        "            result_queue.put({\n",
        "                \"type\": \"worker_fail\",\n",
        "                \"worker\": worker_id,\n",
        "                \"message\": msg,\n",
        "            })\n",
        "        except Exception:\n",
        "            pass\n",
        "        raise\n",
        "\n",
        "\n",
        "print(\"Self-play implementado.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8484df3d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 7: Replay Buffer y Training\n",
        "# ============================================================\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Buffer circular para almacenar ejemplos de self-play.\"\"\"\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "\n",
        "    def add(self, examples):\n",
        "        \"\"\"Agregar lista de (state, policy, value) al buffer.\"\"\"\n",
        "        self.buffer.extend(examples)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Samplear un batch aleatorio.\"\"\"\n",
        "        n = min(batch_size, len(self.buffer))\n",
        "        indices = np.random.choice(len(self.buffer), size=n, replace=False)\n",
        "        batch = [self.buffer[i] for i in indices]\n",
        "        states = np.array([b[0] for b in batch])\n",
        "        policies = np.array([b[1] for b in batch])\n",
        "        values = np.array([b[2] for b in batch], dtype=np.float32)\n",
        "        return states, policies, values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def save(self, path):\n",
        "        with open(path, 'wb') as f:\n",
        "            pickle.dump(list(self.buffer), f)\n",
        "\n",
        "    def load(self, path):\n",
        "        if os.path.exists(path):\n",
        "            with open(path, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "            self.buffer = deque(data, maxlen=self.buffer.maxlen)\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "\n",
        "def train_epoch(net, optimizer, buffer, cfg, device, scaler=None):\n",
        "    \"\"\"\n",
        "    Entrenar una epoca sobre el replay buffer.\n",
        "    Loss = (z - v)^2 - pi * log(p)\n",
        "    (weight_decay ya esta en el optimizer como L2)\n",
        "    \"\"\"\n",
        "    net.train()\n",
        "    total_loss = 0.0\n",
        "    total_p_loss = 0.0\n",
        "    total_v_loss = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    num_batches = max(1, len(buffer) // cfg.batch_size)\n",
        "    use_amp = bool(getattr(cfg, \"use_amp\", True) and device.type == \"cuda\")\n",
        "\n",
        "    for _ in range(num_batches):\n",
        "        states, policies, values = buffer.sample(cfg.batch_size)\n",
        "\n",
        "        states_t = torch.from_numpy(states)\n",
        "        policies_t = torch.from_numpy(policies)\n",
        "        values_t = torch.from_numpy(values).unsqueeze(1)\n",
        "\n",
        "        if device.type == \"cuda\":\n",
        "            states_t = states_t.pin_memory().to(device, non_blocking=True)\n",
        "            policies_t = policies_t.pin_memory().to(device, non_blocking=True)\n",
        "            values_t = values_t.pin_memory().to(device, non_blocking=True)\n",
        "        else:\n",
        "            states_t = states_t.to(device)\n",
        "            policies_t = policies_t.to(device)\n",
        "            values_t = values_t.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=use_amp):\n",
        "            pred_p, pred_v = net(states_t)\n",
        "\n",
        "            # Value loss: MSE\n",
        "            v_loss = F.mse_loss(pred_v, values_t)\n",
        "\n",
        "            # Policy loss: cross-entropy (pi * log(p))\n",
        "            p_loss = -torch.mean(torch.sum(policies_t * torch.log(pred_p + 1e-8), dim=1))\n",
        "\n",
        "            loss = v_loss + p_loss\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        if use_amp and scaler is not None:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_p_loss += p_loss.item()\n",
        "        total_v_loss += v_loss.item()\n",
        "        n_batches += 1\n",
        "\n",
        "    if n_batches == 0:\n",
        "        return 0, 0, 0\n",
        "    return total_loss / n_batches, total_p_loss / n_batches, total_v_loss / n_batches\n",
        "\n",
        "print(\"ReplayBuffer y train_epoch implementados.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6881665",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 8: Evaluacion\n",
        "# ============================================================\n",
        "\n",
        "def evaluate_mcts(net, cfg, device, n_games=None):\n",
        "    \"\"\"Evaluar modelo usando MCTS (greedy, temp=0) y sin Dirichlet noise.\"\"\"\n",
        "    if n_games is None:\n",
        "        n_games = cfg.eval_games\n",
        "    net.eval()\n",
        "\n",
        "    wins = 0\n",
        "    total_score = 0\n",
        "    scores = []\n",
        "\n",
        "    eval_cfg = dataclasses.replace(cfg, dir_eps=0.0, temp_init=0.0, temp_final=0.0)\n",
        "\n",
        "    for _ in tqdm(range(n_games), desc='Eval MCTS', leave=False):\n",
        "        env = SnakeEnv(eval_cfg.board_size, eval_cfg.max_steps)\n",
        "        mcts_eval = MCTS(make_predict_fn(net, device), eval_cfg)\n",
        "        env.reset()\n",
        "\n",
        "        while not env.done:\n",
        "            pi = mcts_eval.search(env, temperature=0)\n",
        "            action = int(np.argmax(pi))\n",
        "            env.step(action)\n",
        "\n",
        "        if env.is_win():\n",
        "            wins += 1\n",
        "        total_score += env.score\n",
        "        scores.append(env.score)\n",
        "\n",
        "    wr = wins / max(n_games, 1)\n",
        "    avg = total_score / max(n_games, 1)\n",
        "    return wr, avg, scores\n",
        "\n",
        "\n",
        "print('Funcion evaluate_mcts implementada (paper-faithful).')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9dd4d65",
      "metadata": {},
      "source": [
        "## Loop Principal de Entrenamiento\n",
        "\n",
        "Secuencia: self-play -> train -> eval MCTS -> champion update -> checkpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae4e1d90",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 9: Loop Principal de Entrenamiento\n",
        "# ============================================================\n",
        "\n",
        "def save_checkpoint(net, best_net, optimizer, iteration, best_win_rate, cfg, buffer):\n",
        "    \"\"\"Guardar checkpoint completo en save_dir.\"\"\"\n",
        "    ckpt = {\n",
        "        'iteration': iteration,\n",
        "        'net': net.state_dict(),\n",
        "        'best_net': best_net.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'best_win_rate': best_win_rate,\n",
        "        'config': dataclasses.asdict(cfg),\n",
        "    }\n",
        "    path = os.path.join(cfg.save_dir, 'latest_checkpoint.pt')\n",
        "    torch.save(ckpt, path)\n",
        "\n",
        "    # Guardar mejor modelo por separado\n",
        "    best_path = os.path.join(cfg.save_dir, 'best_model.pt')\n",
        "    torch.save(best_net.state_dict(), best_path)\n",
        "\n",
        "    # Guardar buffer\n",
        "    buffer_path = os.path.join(cfg.save_dir, 'buffer.pkl')\n",
        "    buffer.save(buffer_path)\n",
        "\n",
        "    print(f\"  Checkpoint guardado (iter {iteration})\")\n",
        "\n",
        "\n",
        "def load_checkpoint(net, best_net, optimizer, cfg, buffer):\n",
        "    \"\"\"Cargar checkpoint desde save_dir si existe.\"\"\"\n",
        "    path = os.path.join(cfg.save_dir, 'latest_checkpoint.pt')\n",
        "    if not os.path.exists(path):\n",
        "        return 0, 0.0\n",
        "\n",
        "    ckpt = torch.load(path, map_location=device)\n",
        "    net.load_state_dict(ckpt['net'])\n",
        "    best_net.load_state_dict(ckpt['best_net'])\n",
        "    optimizer.load_state_dict(ckpt['optimizer'])\n",
        "\n",
        "    buffer_path = os.path.join(cfg.save_dir, 'buffer.pkl')\n",
        "    buffer.load(buffer_path)\n",
        "\n",
        "    iteration = ckpt.get('iteration', 0)\n",
        "    best_wr = ckpt.get('best_win_rate', 0.0)\n",
        "    print(f\"Checkpoint cargado: iteracion {iteration}, best_win_rate={best_wr:.3f}\")\n",
        "    print(f\"Buffer restaurado: {len(buffer)} ejemplos\")\n",
        "    return iteration, best_wr\n",
        "\n",
        "\n",
        "def train_alphasnake(cfg):\n",
        "    \"\"\"Loop principal de entrenamiento AlphaSnake.\"\"\"\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\" AlphaSnake Training\")\n",
        "    print(f\" Board: {cfg.board_size}x{cfg.board_size}\")\n",
        "    print(f\" Simulations: {cfg.num_simulations}\")\n",
        "    print(f\" Games/iter: {cfg.games_per_iter}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    # Crear redes\n",
        "    net = AlphaSnakeNet(4, cfg.net_blocks, cfg.net_channels, cfg.board_size).to(device)\n",
        "    best_net = AlphaSnakeNet(4, cfg.net_blocks, cfg.net_channels, cfg.board_size).to(device)\n",
        "    best_net.load_state_dict(net.state_dict())\n",
        "\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    buffer = ReplayBuffer(cfg.buffer_size)\n",
        "    scaler = torch.amp.GradScaler(device=\"cuda\", enabled=bool(getattr(cfg, \"use_amp\", True) and device.type == \"cuda\"))\n",
        "\n",
        "    # Intentar cargar checkpoint\n",
        "    start_iter, best_win_rate = load_checkpoint(net, best_net, optimizer, cfg, buffer)\n",
        "\n",
        "    training_start = time.time()\n",
        "    for iteration in range(start_iter, cfg.max_iterations):\n",
        "        iter_start = time.time()\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"  ITERACION {iteration + 1} / {cfg.max_iterations}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # ------------------------------------------------\n",
        "        # 1. SELF-PLAY PARALELO (procesos + inference batching)\n",
        "        # ------------------------------------------------\n",
        "        start_methods = multiprocessing.get_all_start_methods()\n",
        "        mp_method = \"fork\" if \"fork\" in start_methods else \"spawn\"\n",
        "        mp_ctx = multiprocessing.get_context(mp_method)\n",
        "\n",
        "        cpu_total = os.cpu_count() or 8\n",
        "        requested_workers = int(getattr(cfg, \"selfplay_workers\", 10))\n",
        "        oversub_factor = float(getattr(cfg, \"worker_oversub_factor\", 3.0))\n",
        "        max_allowed_workers = max(1, int(max(1, cpu_total) * max(1.0, oversub_factor)))\n",
        "        n_workers = max(1, min(requested_workers, max_allowed_workers))\n",
        "        best_net.eval()\n",
        "        all_examples = []\n",
        "        sp_wins = 0\n",
        "        sp_scores = []\n",
        "        sp_moves = []\n",
        "        sp_lengths = []\n",
        "\n",
        "        mp_request_queue = mp_ctx.Queue(maxsize=8192)\n",
        "        response_queues = [mp_ctx.Queue(maxsize=2048) for _ in range(n_workers)]\n",
        "        progress_queue = mp_ctx.Queue(maxsize=8192)\n",
        "        result_queue = mp_ctx.Queue(maxsize=max(16, n_workers * 4))\n",
        "\n",
        "        infer_stats = {\"requests\": 0, \"states\": 0, \"batches\": 0}\n",
        "\n",
        "        def inference_server():\n",
        "            batch_timeout = max(0.001, float(getattr(cfg, \"inference_timeout_ms\", 20)) / 1000.0)\n",
        "            max_batch = max(8, int(getattr(cfg, \"inference_batch_size\", 64)))\n",
        "            use_amp = bool(getattr(cfg, \"use_amp\", True) and device.type == \"cuda\")\n",
        "            try:\n",
        "                while True:\n",
        "                    requests = []\n",
        "                    deadline = time.time() + batch_timeout\n",
        "                    while len(requests) < max_batch:\n",
        "                        try:\n",
        "                            item = mp_request_queue.get(timeout=max(0.001, deadline - time.time()))\n",
        "                            if item is None:\n",
        "                                return\n",
        "                            requests.append(item)\n",
        "                        except Empty:\n",
        "                            break\n",
        "                    if not requests:\n",
        "                        continue\n",
        "\n",
        "                    parsed = []\n",
        "                    flat_states = []\n",
        "\n",
        "                    for item in requests:\n",
        "                        if not isinstance(item, tuple):\n",
        "                            continue\n",
        "\n",
        "                        if len(item) == 2:\n",
        "                            wid, payload = item\n",
        "                            is_batch = False\n",
        "                        elif len(item) >= 3:\n",
        "                            wid, is_batch, payload = item[0], bool(item[1]), item[2]\n",
        "                        else:\n",
        "                            continue\n",
        "\n",
        "                        if is_batch:\n",
        "                            states_list = payload if isinstance(payload, list) else list(payload)\n",
        "                            start_idx = len(flat_states)\n",
        "                            flat_states.extend(states_list)\n",
        "                            parsed.append((int(wid), True, start_idx, len(states_list)))\n",
        "                        else:\n",
        "                            start_idx = len(flat_states)\n",
        "                            flat_states.append(payload)\n",
        "                            parsed.append((int(wid), False, start_idx, 1))\n",
        "\n",
        "                    if not parsed or not flat_states:\n",
        "                        continue\n",
        "\n",
        "                    infer_stats[\"requests\"] += len(parsed)\n",
        "                    infer_stats[\"states\"] += len(flat_states)\n",
        "                    infer_stats[\"batches\"] += 1\n",
        "\n",
        "                    arr = np.stack(flat_states)\n",
        "                    t = torch.from_numpy(arr)\n",
        "                    if device.type == \"cuda\":\n",
        "                        t = t.pin_memory().to(device, non_blocking=True)\n",
        "                    else:\n",
        "                        t = t.to(device)\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=use_amp):\n",
        "                            policies, values = best_net(t)\n",
        "\n",
        "                    p_np = policies.cpu().numpy()\n",
        "                    v_np = values.cpu().numpy()\n",
        "\n",
        "                    for wid, is_batch, start_idx, count in parsed:\n",
        "                        if is_batch:\n",
        "                            out = [(p_np[start_idx + j], float(v_np[start_idx + j, 0])) for j in range(count)]\n",
        "                            response_queues[wid].put(out)\n",
        "                        else:\n",
        "                            response_queues[wid].put((p_np[start_idx], float(v_np[start_idx, 0])))\n",
        "            except Exception as exc:\n",
        "                err = f\"{type(exc).__name__}: {exc}\"\n",
        "                print(f\"      [Inference ERROR] {err}\", flush=True)\n",
        "                try:\n",
        "                    progress_queue.put({\"type\": \"inference_error\", \"message\": err})\n",
        "                except Exception:\n",
        "                    pass\n",
        "                for q in response_queues:\n",
        "                    try:\n",
        "                        q.put({\"type\": \"inference_error\", \"message\": err})\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "        server_thread = threading.Thread(target=inference_server, daemon=True)\n",
        "        server_thread.start()\n",
        "\n",
        "        iter_stamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        print(f\"  [Iter {iteration + 1}] Inicio confirmado: {iter_stamp}\")\n",
        "        print(f\"  [Self-play] Arranco con {n_workers} workers y {cfg.games_per_iter} juegos objetivo (backend=processes:{mp_method}, cpu_total={cpu_total}, requested={requested_workers}, max_allowed={max_allowed_workers})\")\n",
        "\n",
        "        sp_start = time.time()\n",
        "        games_per_worker = [cfg.games_per_iter // n_workers + (1 if i < cfg.games_per_iter % n_workers else 0) for i in range(n_workers)]\n",
        "\n",
        "        workers = []\n",
        "        for i in range(n_workers):\n",
        "            wp = mp_ctx.Process(\n",
        "                target=run_n_games_process_entry,\n",
        "                args=(games_per_worker[i], mp_request_queue, response_queues[i], progress_queue, result_queue, i, cfg),\n",
        "                daemon=True,\n",
        "            )\n",
        "            wp.start()\n",
        "            workers.append(wp)\n",
        "\n",
        "        pending_workers = set(range(n_workers))\n",
        "        completed = 0\n",
        "        games_reported = 0\n",
        "        verbose_games = bool(getattr(cfg, \"verbose_game_updates\", True))\n",
        "        log_every = max(1, int(getattr(cfg, \"game_log_interval\", 1)))\n",
        "        heartbeat_s = max(10, int(getattr(cfg, \"heartbeat_seconds\", 20)))\n",
        "        last_heartbeat = time.time()\n",
        "\n",
        "        try:\n",
        "            with tqdm(total=cfg.games_per_iter, desc=f\"Self-play iter {iteration+1}\", mininterval=0.5, dynamic_ncols=True) as pbar:\n",
        "                while pending_workers:\n",
        "                    drained = 0\n",
        "\n",
        "                    while True:\n",
        "                        try:\n",
        "                            msg = progress_queue.get_nowait()\n",
        "                        except Empty:\n",
        "                            break\n",
        "\n",
        "                        drained += 1\n",
        "                        msg_type = msg.get(\"type\", \"unknown\") if isinstance(msg, dict) else \"unknown\"\n",
        "\n",
        "                        if msg_type == \"worker_start\":\n",
        "                            print(f\"      [Worker {msg['worker']}] iniciado | juegos_asignados={msg['games']}\", flush=True)\n",
        "                            continue\n",
        "\n",
        "                        if msg_type == \"game_start\":\n",
        "                            print(f\"      [Worker {msg['worker']}] inicio juego local={msg['game_local']}\", flush=True)\n",
        "                            continue\n",
        "\n",
        "                        if msg_type == \"game_tick\":\n",
        "                            if verbose_games:\n",
        "                                print(\n",
        "                                    f\"      [Tick] worker={msg['worker']} juego={msg['game_local']} \"\n",
        "                                    f\"moves={msg['moves']} score={msg['score']} len={msg['length']}\",\n",
        "                                    flush=True,\n",
        "                                )\n",
        "                            continue\n",
        "\n",
        "                        if msg_type == \"worker_error\":\n",
        "                            print(\n",
        "                                f\"      [Worker ERROR] worker={msg.get('worker')} juego={msg.get('game_local')} msg={msg.get('message')}\",\n",
        "                                flush=True,\n",
        "                            )\n",
        "                            continue\n",
        "\n",
        "                        if msg_type == \"inference_error\":\n",
        "                            print(f\"      [Inference ERROR] {msg.get('message')}\", flush=True)\n",
        "                            continue\n",
        "\n",
        "                        if msg_type == \"game_end\":\n",
        "                            games_reported += 1\n",
        "                            pbar.update(1)\n",
        "                            if verbose_games and (games_reported == 1 or games_reported % log_every == 0):\n",
        "                                result_tag = \"WIN\" if msg[\"won\"] else \"LOSE\"\n",
        "                                print(\n",
        "                                    f\"      [Juego {games_reported}/{cfg.games_per_iter}] \"\n",
        "                                    f\"worker={msg['worker']} local={msg['game_local']} {result_tag} \"\n",
        "                                    f\"score={msg['score']} moves={msg['moves']} len={msg['length']} ex={msg['examples']}\",\n",
        "                                    flush=True,\n",
        "                                )\n",
        "                            continue\n",
        "\n",
        "                    while True:\n",
        "                        try:\n",
        "                            rmsg = result_queue.get_nowait()\n",
        "                        except Empty:\n",
        "                            break\n",
        "\n",
        "                        drained += 1\n",
        "                        rtype = rmsg.get(\"type\") if isinstance(rmsg, dict) else None\n",
        "\n",
        "                        if rtype == \"worker_done\":\n",
        "                            wid = int(rmsg[\"worker\"])\n",
        "                            ex, w, sc, mv, ln = rmsg[\"result\"]\n",
        "                            all_examples.extend(ex)\n",
        "                            sp_wins += w\n",
        "                            sp_scores.extend(sc)\n",
        "                            sp_moves.extend(mv)\n",
        "                            sp_lengths.extend(ln)\n",
        "                            if wid in pending_workers:\n",
        "                                pending_workers.remove(wid)\n",
        "                                completed += 1\n",
        "                            games_done = len(sp_scores)\n",
        "                            wr_so_far = 100 * sp_wins / games_done if games_done else 0\n",
        "                            avg_sc = np.mean(sp_scores) if sp_scores else 0\n",
        "                            print(f\"      [Worker completado] {completed}/{n_workers} | juegos agregados={len(sc)} | acumulado={games_done}/{cfg.games_per_iter} | wins={sp_wins} ({wr_so_far:.1f}%) | avg_score={avg_sc:.1f} | ejemplos={len(all_examples)}\", flush=True)\n",
        "                            continue\n",
        "\n",
        "                        if rtype == \"worker_fail\":\n",
        "                            raise RuntimeError(f\"Worker {rmsg.get('worker')} failed: {rmsg.get('message')}\")\n",
        "\n",
        "                    if pending_workers:\n",
        "                        now = time.time()\n",
        "                        if drained == 0 and now - last_heartbeat >= heartbeat_s:\n",
        "                            reqs = infer_stats[\"requests\"]\n",
        "                            states_n = infer_stats[\"states\"]\n",
        "                            batches = infer_stats[\"batches\"]\n",
        "                            avg_states = states_n / max(batches, 1)\n",
        "                            print(\n",
        "                                f\"      [Heartbeat] juegos={games_reported}/{cfg.games_per_iter} | workers={completed}/{n_workers} | infer_reqs={reqs} | infer_states={states_n} | infer_batches={batches} | avg_states_batch={avg_states:.1f}\",\n",
        "                                flush=True,\n",
        "                            )\n",
        "                            last_heartbeat = now\n",
        "                        time.sleep(0.2)\n",
        "        finally:\n",
        "            for wp in workers:\n",
        "                if wp.is_alive():\n",
        "                    wp.join(timeout=0.2)\n",
        "            # cerrar inference server\n",
        "            mp_request_queue.put(None)\n",
        "            server_thread.join(timeout=10)\n",
        "\n",
        "            # si quedaron workers vivos, forzar cierre\n",
        "            for wp in workers:\n",
        "                if wp.is_alive():\n",
        "                    wp.terminate()\n",
        "                    wp.join(timeout=2)\n",
        "\n",
        "        reqs = infer_stats[\"requests\"]\n",
        "        states_n = infer_stats[\"states\"]\n",
        "        batches = infer_stats[\"batches\"]\n",
        "        avg_states = states_n / max(batches, 1)\n",
        "        print(f\"    Inference batching -> reqs={reqs}, states={states_n}, batches={batches}, avg_states_batch={avg_states:.1f}\")\n",
        "\n",
        "        sp_time = time.time() - sp_start\n",
        "        sp_wr = sp_wins / cfg.games_per_iter\n",
        "        sp_scores_arr = np.array(sp_scores)\n",
        "        sp_moves_arr = np.array(sp_moves)\n",
        "        sp_lengths_arr = np.array(sp_lengths)\n",
        "\n",
        "        ejemplos_per_min = len(all_examples) / (sp_time / 60.0) if sp_time > 0 else 0\n",
        "        print(f\"  --- Self-play ({sp_time:.0f}s, {n_workers} procesos) ---\")\n",
        "        print(f\"    Juegos: {cfg.games_per_iter} | Wins: {sp_wins} ({100*sp_wr:.1f}%) | Ejemplos: {len(all_examples)} ({ejemplos_per_min:.0f}/min)\")\n",
        "        print(f\"    Score  -> min: {sp_scores_arr.min():.0f}  med: {np.median(sp_scores_arr):.0f}  max: {sp_scores_arr.max():.0f}  avg: {sp_scores_arr.mean():.1f}\")\n",
        "        print(f\"    Length -> min: {sp_lengths_arr.min():.0f}  med: {np.median(sp_lengths_arr):.0f}  max: {sp_lengths_arr.max():.0f}  (max: {cfg.board_size**2})\")\n",
        "        print(f\"    Moves  -> min: {sp_moves_arr.min():.0f}  med: {np.median(sp_moves_arr):.0f}  max: {sp_moves_arr.max():.0f}\")\n",
        "\n",
        "        buffer.add(all_examples)\n",
        "        print(f\"    Buffer total: {len(buffer)}\")\n",
        "\n",
        "        # ------------------------------------------------\n",
        "        # 2. TRAINING\n",
        "        # ------------------------------------------------\n",
        "        train_start = time.time()\n",
        "        last_loss, last_p, last_v = 0.0, 0.0, 0.0\n",
        "        for epoch in tqdm(range(cfg.epochs_per_iter), desc=f\"Training iter {iteration+1}\", leave=False):\n",
        "            last_loss, last_p, last_v = train_epoch(net, optimizer, buffer, cfg, device, scaler=scaler)\n",
        "            if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "                tqdm.write(f\"    Epoch {epoch+1}/{cfg.epochs_per_iter}: loss={last_loss:.4f} (policy={last_p:.4f}, value={last_v:.4f})\")\n",
        "        train_time = time.time() - train_start\n",
        "        print(f\"  --- Training ({train_time:.0f}s, {cfg.epochs_per_iter} epochs) ---\")\n",
        "        print(f\"    Loss: total={last_loss:.4f} | policy={last_p:.4f} | value={last_v:.4f}\")\n",
        "\n",
        "        # ------------------------------------------------\n",
        "        # 3. EVALUACION MCTS (paper-faithful)\n",
        "        # ------------------------------------------------\n",
        "        eval_start = time.time()\n",
        "        wr_new_mcts, avg_new_mcts, _ = evaluate_mcts(net, cfg, device, n_games=cfg.eval_games)\n",
        "        eval_time = time.time() - eval_start\n",
        "\n",
        "        print(f\"  --- Eval MCTS ({cfg.eval_games} juegos, {eval_time:.0f}s) ---\")\n",
        "        print(f\"    Nuevo: win_rate={100*wr_new_mcts:.1f}% | avg_score={avg_new_mcts:.1f}\")\n",
        "        print(f\"    Umbral aceptacion: {100*cfg.accept_threshold:.1f}% | Best actual: {100*best_win_rate:.1f}%\")\n",
        "\n",
        "        # ------------------------------------------------\n",
        "        # 4. CHAMPION UPDATE (paper-faithful)\n",
        "        # ------------------------------------------------\n",
        "        required_wr = max(cfg.accept_threshold, best_win_rate)\n",
        "        if wr_new_mcts >= required_wr:\n",
        "            print('  >>> NUEVO CHAMPION aceptado <<<')\n",
        "            best_net.load_state_dict(net.state_dict())\n",
        "            best_win_rate = wr_new_mcts\n",
        "        else:\n",
        "            print('  Champion previo retenido.')\n",
        "            net.load_state_dict(best_net.state_dict())\n",
        "\n",
        "        # ------------------------------------------------\n",
        "        # 5. RESUMEN ITERACION\n",
        "        # ------------------------------------------------\n",
        "        iter_time = time.time() - iter_start\n",
        "        total_elapsed = time.time() - training_start\n",
        "        iters_done = iteration - start_iter + 1\n",
        "        iters_left = cfg.max_iterations - iteration - 1\n",
        "        eta_sec = (iter_time * iters_left) if iters_left > 0 else 0\n",
        "        eta_min = eta_sec / 60\n",
        "\n",
        "        print(f\"  --- Resumen iter {iteration+1}/{cfg.max_iterations} ---\")\n",
        "        print(f\"    Tiempos: self-play={sp_time:.0f}s  train={train_time:.0f}s  eval={eval_time:.0f}s  total={iter_time:.0f}s\")\n",
        "        print(f\"    Acumulado: {total_elapsed/60:.1f} min | Restantes: {iters_left} iters | ETA ~{eta_min:.0f} min\")\n",
        "        print(f\"    Buffer: {len(buffer)} posiciones | Mejor win_rate (MCTS): {best_win_rate:.3f}\")\n",
        "        if (iteration + 1) % cfg.checkpoint_interval == 0:\n",
        "            save_checkpoint(net, best_net, optimizer, iteration + 1, best_win_rate, cfg, buffer)\n",
        "            print(f\"    Checkpoint guardado (iter {iteration+1})\")\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"  ENTRENAMIENTO COMPLETADO\")\n",
        "    print(f\"  Mejor win rate (policy): {best_win_rate:.3f}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    return best_net\n",
        "\n",
        "print(\"Loop de entrenamiento definido. Ejecutar la siguiente celda para entrenar.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Takeaway - Champion\n",
        "La seleccion de champion usa evaluacion MCTS real y umbral `accept_threshold=0.55`.\n",
        "El modelo solo se promueve si supera el mejor win-rate acumulado y ese umbral.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aca6bd9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell X: Benchmark rapido de throughput (opcional)\n",
        "# ============================================================\n",
        "# Ejecuta esta celda solo si quieres estimar el tiempo por iteracion.\n",
        "\n",
        "net_bench = AlphaSnakeNet(4, config.net_blocks, config.net_channels, config.board_size).to(device)\n",
        "opt_bench = torch.optim.Adam(net_bench.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
        "buf_bench = ReplayBuffer(max(config.batch_size * 8, 2048))\n",
        "\n",
        "synth_states = np.random.randn(config.batch_size, 4, config.board_size, config.board_size).astype(np.float32)\n",
        "synth_policies = np.random.dirichlet(alpha=np.ones(4), size=config.batch_size).astype(np.float32)\n",
        "synth_values = np.random.uniform(-1, 1, size=(config.batch_size,)).astype(np.float32)\n",
        "for _ in range(max(8, 2048 // config.batch_size)):\n",
        "    buf_bench.add(list(zip(synth_states, synth_policies, synth_values)))\n",
        "\n",
        "bench_start = time.time()\n",
        "_ = train_epoch(net_bench, opt_bench, buf_bench, config, device)\n",
        "bench_elapsed = time.time() - bench_start\n",
        "print(f'Benchmark train_epoch: {bench_elapsed:.3f}s (batch_size={config.batch_size})')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a46b1d40",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 10: Ejecutar entrenamiento\n",
        "# ============================================================\n",
        "# Esta celda lanza el loop completo.\n",
        "# Puedes interrumpir y reanudar: usa checkpoints en config.save_dir.\n",
        "\n",
        "best_model = train_alphasnake(config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4424c52",
      "metadata": {},
      "source": [
        "## Evaluacion Final + Export ONNX\n",
        "\n",
        "Se valida el champion con MCTS real y luego se exporta a ONNX con\n",
        "el contrato requerido por `ai-bot.js` y `ai-mcts-worker.js`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb3eb272",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 11: Evaluacion final con MCTS\n",
        "# ============================================================\n",
        "\n",
        "best_model_path = os.path.join(config.save_dir, 'best_model.pt')\n",
        "if os.path.exists(best_model_path):\n",
        "    best_model = AlphaSnakeNet(4, config.net_blocks, config.net_channels, config.board_size).to(device)\n",
        "    best_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "    print('Mejor modelo cargado desde save_dir.')\n",
        "\n",
        "print()\n",
        "print('Evaluacion final con MCTS (puede tardar)...')\n",
        "final_games = max(100, config.eval_games)\n",
        "wr, avg_score, scores = evaluate_mcts(best_model, config, device, n_games=final_games)\n",
        "\n",
        "print()\n",
        "print(f'Resultados finales ({final_games} juegos MCTS):')\n",
        "print(f'  Win rate: {wr:.3f} ({int(wr*final_games)}/{final_games})')\n",
        "print(f'  Avg score: {avg_score:.1f} / {config.board_size**2 - 3}')\n",
        "print(f'  Min score: {min(scores)}, Max score: {max(scores)}')\n",
        "print(f'  Median score: {np.median(scores):.0f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4403abbc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 12: Exportar y validar ONNX\n",
        "# ============================================================\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "\n",
        "best_model.eval()\n",
        "best_model.cpu()\n",
        "\n",
        "dummy_input = torch.randn(1, 4, config.board_size, config.board_size)\n",
        "onnx_path = os.path.join(config.save_dir, 'alphasnake.onnx')\n",
        "\n",
        "torch.onnx.export(\n",
        "    best_model,\n",
        "    dummy_input,\n",
        "    onnx_path,\n",
        "    export_params=True,\n",
        "    opset_version=13,\n",
        "    do_constant_folding=True,\n",
        "    input_names=['state'],\n",
        "    output_names=['policy', 'value'],\n",
        "    dynamic_axes={\n",
        "        'state': {0: 'batch_size'},\n",
        "        'policy': {0: 'batch_size'},\n",
        "        'value': {0: 'batch_size'},\n",
        "    }\n",
        ")\n",
        "\n",
        "model_onnx = onnx.load(onnx_path)\n",
        "onnx.checker.check_model(model_onnx)\n",
        "print(f'Modelo ONNX exportado y verificado: {onnx_path}')\n",
        "\n",
        "ort_session = ort.InferenceSession(onnx_path)\n",
        "test_input = np.random.randn(1, 4, config.board_size, config.board_size).astype(np.float32)\n",
        "\n",
        "with torch.no_grad():\n",
        "    pt_policy, pt_value = best_model(torch.from_numpy(test_input))\n",
        "pt_policy = pt_policy.numpy()\n",
        "pt_value = pt_value.numpy()\n",
        "\n",
        "ort_policy, ort_value = ort_session.run(None, {'state': test_input})\n",
        "\n",
        "print()\n",
        "print('Verificacion PyTorch vs ONNX:')\n",
        "print(f'  Policy diff max: {np.abs(pt_policy - ort_policy).max():.8f}')\n",
        "print(f'  Value diff max:  {np.abs(pt_value - ort_value).max():.8f}')\n",
        "print(f'  Tamano modelo: {os.path.getsize(onnx_path) / (1024 * 1024):.2f} MB')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Takeaway - ONNX\n",
        "Se valida el contrato exacto para el juego:\n",
        "- Input `state`: `[batch, 4, 10, 10]`\n",
        "- Outputs: `policy` `[batch, 4]`, `value` `[batch, 1]`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bfe4a5e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 13: Integracion con el juego existente\n",
        "# ============================================================\n",
        "\n",
        "REPO_MODEL_PATH = '/Users/JuanCamiloTorresUrrego/Documents/tiktok/arcade/games/snake/ai/alphasnake.onnx'\n",
        "\n",
        "print('Modelo ONNX generado en:')\n",
        "print(f'  {onnx_path}')\n",
        "print()\n",
        "print('Pasos de integracion:')\n",
        "print(f'1) Copiar el modelo: cp \"{onnx_path}\" \"{REPO_MODEL_PATH}\"')\n",
        "print('2) Levantar juego: npm run start:game')\n",
        "print('3) Abrir: http://localhost:3000/snake/ai.html')\n",
        "print('4) En la UI, usar modo MCTS y slider en 400 simulaciones para condiciones paper.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Takeaways finales\n",
        "\n",
        "- Notebook listo para Vast.ai con perfiles `paper_strict` y `smoke`.\n",
        "- Pipeline completo reproducible: self-play -> train -> eval MCTS -> champion -> export ONNX.\n",
        "- Integracion con el juego lista sin adaptar el modelo al engine.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
