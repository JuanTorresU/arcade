{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c3005de0",
      "metadata": {},
      "source": [
        "# AlphaSnake \u2014 Entrenamiento AlphaZero para Snake\n",
        "\n",
        "Pipeline completo de entrenamiento: **MCTS + Policy/Value ResNet** para Snake 10x10.\n",
        "\n",
        "- **Red**: ResNet 6 bloques residuales (64 ch), policy head (softmax 4) + value head (tanh)\n",
        "- **MCTS**: 400 sims, UCB c_puct=1.0, Dirichlet noise, food stochasticity sampling\n",
        "- **Pipeline**: Self-play \u2192 Replay Buffer \u2192 Train \u2192 Evaluate \u2192 Champion update\n",
        "- **Export**: ONNX para inferencia en navegador\n",
        "\n",
        "**Requisitos**: GPU (Colab: Runtime \u2192 Change runtime type \u2192 T4 GPU | Vast.ai: PyTorch Jupyter con GPU).  \n",
        "Con el notebook en el **directorio ra\u00edz**, los checkpoints se guardan en `./alphasnake`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0604a895",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 1: Setup \u2014 Verificar GPU y directorio de checkpoints\n",
        "# ============================================================\n",
        "# Notebook en directorio ra\u00edz: checkpoints van a SAVE_DIR (subcarpeta 'alphasnake').\n",
        "# Colab: Drive. Vast.ai / local: <ra\u00edz>/alphasnake (ej. /workspace/alphasnake).\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Directorio base = donde se ejecuta el notebook (ra\u00edz del proyecto)\n",
        "ROOT_DIR = os.getcwd()\n",
        "if os.path.exists(\"/workspace\"):\n",
        "    # Vast.ai: persistente; notebook en ra\u00edz -> /workspace/alphasnake\n",
        "    SAVE_DIR = os.path.join(ROOT_DIR, \"alphasnake\")\n",
        "elif os.path.exists(\"/content\"):\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount(\"/content/drive\")\n",
        "        SAVE_DIR = \"/content/drive/MyDrive/alphasnake\"\n",
        "    except ImportError:\n",
        "        SAVE_DIR = \"/content/alphasnake\"\n",
        "else:\n",
        "    SAVE_DIR = os.path.join(ROOT_DIR, \"alphasnake\")\n",
        "\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"PyTorch {torch.__version__} | CUDA: {torch.cuda.is_available()} | Checkpoints: {SAVE_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06fded1c",
      "metadata": {},
      "source": [
        "### Progreso del entrenamiento\n",
        "El entrenamiento muestra por iteraci\u00f3n: self-play (wins, scores, ejemplos), training (loss), evaluaci\u00f3n y resumen con tiempos y ETA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f556d70",
      "metadata": {},
      "outputs": [],
      "source": [
        "# La salida del entrenamiento muestra progreso detallado por iteraci\u00f3n (self-play, training, eval, ETA).\n",
        "# No se imprime uso de CPU/GPU para mantener el foco en m\u00e9tricas del modelo.\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c46913b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 2: Imports y Configuracion\n",
        "# ============================================================\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import pickle\n",
        "import subprocess\n",
        "import threading\n",
        "import multiprocessing\n",
        "from queue import Empty\n",
        "import dataclasses\n",
        "from dataclasses import dataclass\n",
        "from collections import deque\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# Configuracion \u2014 Ajustar segun recursos disponibles\n",
        "# \"rtx4060\" = RTX 4060 Ti + ~5 nucleos (200 sims, 300 juegos, batch 64) \u2014 recomendado para Vast.ai\n",
        "# \"paper\"   = settings del paper (~2h+ por iter, 400 sims, 1000 juegos)\n",
        "# \"fast\"    = iter ~15-30 min (100 sims, 200 juegos)\n",
        "# \"colab_free\" = muy reducido para pruebas rapidas\n",
        "# -----------------------------------------------------------\n",
        "PRESET = \"paper\"  # \"paper\" | \"rtx4060\" (4060 Ti + 5 nucleos) | \"fast\" | \"colab_free\"\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # Entorno\n",
        "    board_size: int = 10\n",
        "    max_steps: int = 1000\n",
        "\n",
        "    # Red neuronal\n",
        "    net_channels: int = 64\n",
        "    net_blocks: int = 6\n",
        "\n",
        "    # MCTS\n",
        "    num_simulations: int = 400\n",
        "    c_puct: float = 1.0\n",
        "    dir_alpha: float = 0.03\n",
        "    dir_eps: float = 0.25\n",
        "    temp_init: float = 1.0\n",
        "    temp_final: float = 0.0\n",
        "    temp_decay_move: int = 30\n",
        "    food_samples: int = 8\n",
        "\n",
        "    # Training\n",
        "    lr: float = 1e-3\n",
        "    weight_decay: float = 1e-4\n",
        "    batch_size: int = 128\n",
        "    buffer_size: int = 200_000\n",
        "    epochs_per_iter: int = 10\n",
        "\n",
        "    # Self-play\n",
        "    games_per_iter: int = 1000\n",
        "    max_iterations: int = 200\n",
        "\n",
        "    # Evaluacion\n",
        "    eval_games: int = 200\n",
        "    accept_threshold: float = 0.55\n",
        "\n",
        "    # Checkpointing\n",
        "    save_dir: str = SAVE_DIR\n",
        "    checkpoint_interval: int = 1\n",
        "\n",
        "    # Rendimiento\n",
        "    selfplay_workers: int = 5\n",
        "    inference_batch_size: int = 64\n",
        "    inference_timeout_ms: int = 20\n",
        "    use_amp: bool = True\n",
        "\n",
        "    # Logging de progreso\n",
        "    verbose_game_updates: bool = True\n",
        "    game_log_interval: int = 1\n",
        "    game_tick_moves: int = 100\n",
        "    heartbeat_seconds: int = 30\n",
        "\n",
        "if PRESET == \"rtx4060\":\n",
        "    # RTX 4060 Ti (8GB) con CPU multi-core: priorizar throughput estable sin OOM.\n",
        "    config = Config(\n",
        "        num_simulations=220,\n",
        "        games_per_iter=360,\n",
        "        eval_games=80,\n",
        "        max_iterations=200,\n",
        "        batch_size=128,\n",
        "        buffer_size=140_000,\n",
        "        food_samples=6,\n",
        "        selfplay_workers=10,\n",
        "        inference_batch_size=128,\n",
        "        inference_timeout_ms=8,\n",
        "        use_amp=True,\n",
        "    )\n",
        "    print(\"Usando preset RTX4060 optimizado (220 sims, 360 juegos/iter, batch 128, 10 workers)\")\n",
        "elif PRESET == \"colab_free\":\n",
        "    config = Config(\n",
        "        num_simulations=100,\n",
        "        games_per_iter=50,\n",
        "        eval_games=30,\n",
        "        max_iterations=100,\n",
        "        buffer_size=50_000,\n",
        "        food_samples=4,\n",
        "    )\n",
        "    print(\"Usando preset COLAB_FREE (reducido para pruebas rapidas)\")\n",
        "elif PRESET == \"fast\":\n",
        "    config = Config(\n",
        "        num_simulations=100,\n",
        "        games_per_iter=200,\n",
        "        eval_games=50,\n",
        "        max_iterations=200,\n",
        "        buffer_size=100_000,\n",
        "        food_samples=4,\n",
        "    )\n",
        "    print(\"Usando preset FAST (~100 sims, 200 juegos/iter; iteracion ~20-45 min)\")\n",
        "else:\n",
        "    config = Config()\n",
        "    print(\"Usando preset PAPER (400 sims, 1000 juegos/iter; requiere mucho tiempo)\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Config: {dataclasses.asdict(config)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c38d38f4",
      "metadata": {},
      "source": [
        "## Entorno Snake\n",
        "\n",
        "Grid 10x10, observacion `(4, 10, 10)`:\n",
        "- Canal 0: cuerpo de la serpiente\n",
        "- Canal 1: cabeza\n",
        "- Canal 2: comida\n",
        "- Canal 3: direccion actual (valor constante normalizado en todo el plano)\n",
        "\n",
        "Recompensas: `+1` comida, `-1` muerte, `0` otro. Max 1000 pasos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7ce26a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 3: Entorno Snake\n",
        "# ============================================================\n",
        "\n",
        "class SnakeEnv:\n",
        "    \"\"\"\n",
        "    Snake environment estilo AlphaSnake.\n",
        "    Grid board_size x board_size, 4 acciones, observacion (4, H, W).\n",
        "    \"\"\"\n",
        "    # 0=UP, 1=DOWN, 2=LEFT, 3=RIGHT\n",
        "    ACTIONS = {0: (0, -1), 1: (0, 1), 2: (-1, 0), 3: (1, 0)}\n",
        "    OPPOSITES = {0: 1, 1: 0, 2: 3, 3: 2}\n",
        "    ACTION_NAMES = {0: 'UP', 1: 'DOWN', 2: 'LEFT', 3: 'RIGHT'}\n",
        "\n",
        "    def __init__(self, board_size=10, max_steps=1000):\n",
        "        self.board_size = board_size\n",
        "        self.max_steps = max_steps\n",
        "        self.max_score = board_size * board_size - 3  # 97 para 10x10\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        mid = self.board_size // 2\n",
        "        # Snake empieza en el centro, apuntando a la derecha\n",
        "        self.snake = deque([(mid, mid), (mid - 1, mid), (mid - 2, mid)])\n",
        "        self.direction = 3  # RIGHT\n",
        "        self.food = None\n",
        "        self.done = False\n",
        "        self.steps = 0\n",
        "        self.score = 0\n",
        "        self._place_food()\n",
        "        return self.get_state()\n",
        "\n",
        "    def _get_free_cells(self):\n",
        "        snake_set = set(self.snake)\n",
        "        return [(x, y) for y in range(self.board_size)\n",
        "                for x in range(self.board_size)\n",
        "                if (x, y) not in snake_set]\n",
        "\n",
        "    def _place_food(self):\n",
        "        free = self._get_free_cells()\n",
        "        if free:\n",
        "            self.food = random.choice(free)\n",
        "        else:\n",
        "            # Tablero lleno = victoria\n",
        "            self.food = None\n",
        "            self.done = True\n",
        "\n",
        "    def get_state(self):\n",
        "        \"\"\"Retorna observacion (4, board_size, board_size) float32.\"\"\"\n",
        "        s = np.zeros((4, self.board_size, self.board_size), dtype=np.float32)\n",
        "        # Canal 0: cuerpo\n",
        "        for x, y in self.snake:\n",
        "            s[0, y, x] = 1.0\n",
        "        # Canal 1: cabeza\n",
        "        hx, hy = self.snake[0]\n",
        "        s[1, hy, hx] = 1.0\n",
        "        # Canal 2: comida\n",
        "        if self.food:\n",
        "            fx, fy = self.food\n",
        "            s[2, fy, fx] = 1.0\n",
        "        # Canal 3: direccion (valor constante normalizado)\n",
        "        # UP=0.25, DOWN=0.5, LEFT=0.75, RIGHT=1.0\n",
        "        s[3, :, :] = (self.direction + 1) / 4.0\n",
        "        return s\n",
        "\n",
        "    def valid_actions(self):\n",
        "        \"\"\"Acciones validas (excluye reversa directa).\"\"\"\n",
        "        rev = self.OPPOSITES[self.direction]\n",
        "        return [a for a in range(4) if a != rev]\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Ejecuta accion. Retorna (reward, done).\n",
        "        reward: +1.0 comida, -1.0 muerte, 0.0 otro.\n",
        "        \"\"\"\n",
        "        if self.done:\n",
        "            return 0.0, True\n",
        "\n",
        "        # Prohibir reversa directa: si intenta reversa, mantener direccion\n",
        "        if action == self.OPPOSITES[self.direction]:\n",
        "            action = self.direction\n",
        "\n",
        "        self.direction = action\n",
        "        dx, dy = self.ACTIONS[action]\n",
        "        hx, hy = self.snake[0]\n",
        "        nx, ny = hx + dx, hy + dy\n",
        "\n",
        "        # Colision con pared\n",
        "        if nx < 0 or nx >= self.board_size or ny < 0 or ny >= self.board_size:\n",
        "            self.done = True\n",
        "            return -1.0, True\n",
        "\n",
        "        # Verificar si comera comida (antes de mover)\n",
        "        ate_food = self.food is not None and (nx, ny) == self.food\n",
        "\n",
        "        # Colision con cuerpo\n",
        "        # Si NO come comida, la cola se movera, asi que es valido ir a la pos actual de la cola\n",
        "        body_set = set(self.snake)\n",
        "        if not ate_food:\n",
        "            body_set.discard(self.snake[-1])\n",
        "        if (nx, ny) in body_set:\n",
        "            self.done = True\n",
        "            return -1.0, True\n",
        "\n",
        "        # Mover\n",
        "        self.snake.appendleft((nx, ny))\n",
        "        if ate_food:\n",
        "            self.score += 1\n",
        "            self._place_food()\n",
        "            if self.done:  # _place_food puso done=True si el tablero esta lleno\n",
        "                return 1.0, True\n",
        "        else:\n",
        "            self.snake.pop()\n",
        "\n",
        "        self.steps += 1\n",
        "        if self.steps >= self.max_steps:\n",
        "            self.done = True\n",
        "            return 0.0, True\n",
        "\n",
        "        return (1.0 if ate_food else 0.0), False\n",
        "\n",
        "    def is_win(self):\n",
        "        \"\"\"True si la serpiente lleno todo el tablero.\"\"\"\n",
        "        return len(self.snake) >= self.board_size ** 2\n",
        "\n",
        "    def clone(self):\n",
        "        \"\"\"Copia eficiente del entorno.\"\"\"\n",
        "        env = SnakeEnv.__new__(SnakeEnv)\n",
        "        env.board_size = self.board_size\n",
        "        env.max_steps = self.max_steps\n",
        "        env.max_score = self.max_score\n",
        "        env.snake = deque(self.snake)\n",
        "        env.direction = self.direction\n",
        "        env.food = self.food\n",
        "        env.done = self.done\n",
        "        env.steps = self.steps\n",
        "        env.score = self.score\n",
        "        return env\n",
        "\n",
        "# --- Test rapido ---\n",
        "env = SnakeEnv(10, 1000)\n",
        "s = env.reset()\n",
        "print(f\"State shape: {s.shape}\")\n",
        "print(f\"Snake length: {len(env.snake)}, food: {env.food}\")\n",
        "print(f\"Valid actions: {[env.ACTION_NAMES[a] for a in env.valid_actions()]}\")\n",
        "\n",
        "# Jugar unos pasos random\n",
        "for _ in range(20):\n",
        "    a = random.choice(env.valid_actions())\n",
        "    r, done = env.step(a)\n",
        "    if done:\n",
        "        print(f\"Game over! Score: {env.score}, Win: {env.is_win()}\")\n",
        "        break\n",
        "else:\n",
        "    print(f\"After 20 steps: score={env.score}, length={len(env.snake)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89e329bd",
      "metadata": {},
      "source": [
        "## Red Neuronal \u2014 AlphaSnakeNet\n",
        "\n",
        "ResNet estilo AlphaZero reducido:\n",
        "- **Stem**: Conv2d(4\u219264, 3x3) + BN + ReLU\n",
        "- **6 bloques residuales**: Conv(64, 3x3) \u2192 BN \u2192 ReLU \u2192 Conv(64, 3x3) \u2192 BN \u2192 skip \u2192 ReLU\n",
        "- **Policy head**: Conv(2, 1x1) \u2192 BN \u2192 ReLU \u2192 Flatten \u2192 Linear(200\u21924) \u2192 Softmax\n",
        "- **Value head**: Conv(1, 1x1) \u2192 BN \u2192 ReLU \u2192 Flatten \u2192 Linear(100\u219264) \u2192 ReLU \u2192 Linear(64\u21921) \u2192 Tanh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "331a6943",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 4: Red Neuronal \u2014 AlphaSnakeNet\n",
        "# ============================================================\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    \"\"\"Bloque residual: Conv\u2192BN\u2192ReLU\u2192Conv\u2192BN\u2192skip\u2192ReLU\"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(channels)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out = F.relu(out + residual)\n",
        "        return out\n",
        "\n",
        "\n",
        "class AlphaSnakeNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Policy + Value network estilo AlphaZero.\n",
        "    Input:  (batch, 4, board_size, board_size)\n",
        "    Output: policy (batch, 4), value (batch, 1)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=4, num_blocks=6, channels=64, board_size=10):\n",
        "        super().__init__()\n",
        "        self.board_size = board_size\n",
        "\n",
        "        # Stem\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, channels, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Residual tower\n",
        "        self.res_tower = nn.Sequential(*[ResBlock(channels) for _ in range(num_blocks)])\n",
        "\n",
        "        # Policy head\n",
        "        self.policy_conv = nn.Conv2d(channels, 2, 1, bias=False)\n",
        "        self.policy_bn = nn.BatchNorm2d(2)\n",
        "        self.policy_fc = nn.Linear(2 * board_size * board_size, 4)\n",
        "\n",
        "        # Value head\n",
        "        self.value_conv = nn.Conv2d(channels, 1, 1, bias=False)\n",
        "        self.value_bn = nn.BatchNorm2d(1)\n",
        "        self.value_fc1 = nn.Linear(board_size * board_size, 64)\n",
        "        self.value_fc2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shared trunk\n",
        "        x = self.stem(x)\n",
        "        x = self.res_tower(x)\n",
        "\n",
        "        # Policy\n",
        "        p = F.relu(self.policy_bn(self.policy_conv(x)))\n",
        "        p = p.view(p.size(0), -1)\n",
        "        p = F.softmax(self.policy_fc(p), dim=1)\n",
        "\n",
        "        # Value\n",
        "        v = F.relu(self.value_bn(self.value_conv(x)))\n",
        "        v = v.view(v.size(0), -1)\n",
        "        v = F.relu(self.value_fc1(v))\n",
        "        v = torch.tanh(self.value_fc2(v))\n",
        "\n",
        "        return p, v\n",
        "\n",
        "# --- Test ---\n",
        "net = AlphaSnakeNet(\n",
        "    in_channels=4,\n",
        "    num_blocks=config.net_blocks,\n",
        "    channels=config.net_channels,\n",
        "    board_size=config.board_size\n",
        ").to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in net.parameters())\n",
        "print(f\"AlphaSnakeNet: {total_params:,} parametros\")\n",
        "\n",
        "dummy = torch.randn(1, 4, 10, 10).to(device)\n",
        "pol, val = net(dummy)\n",
        "print(f\"Policy shape: {pol.shape}, sum={pol.sum().item():.4f}\")\n",
        "print(f\"Value shape: {val.shape}, val={val.item():.4f}\")\n",
        "del net, dummy  # liberar memoria"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cf1c9f0",
      "metadata": {},
      "source": [
        "## MCTS \u2014 Monte Carlo Tree Search\n",
        "\n",
        "Implementacion AlphaZero con manejo de estocasticidad (food sampling).\n",
        "\n",
        "- UCB: `Q(s,a) + c_puct * P(s,a) * sqrt(N(s)) / (1 + N(s,a))`\n",
        "- Dirichlet noise en root: `P = (1 - eps) * P + eps * Dir(alpha)`\n",
        "- Food stochasticity: al expandir nodo donde se comio comida, promediar valor sobre k posiciones de comida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "197ece1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 5: MCTS \u2014 Monte Carlo Tree Search\n",
        "# ============================================================\n",
        "\n",
        "class MCTSNode:\n",
        "    \"\"\"Nodo del arbol MCTS.\"\"\"\n",
        "    __slots__ = ['prior', 'visit_count', 'value_sum', 'children',\n",
        "                 'is_expanded', 'env', 'food_eaten']\n",
        "\n",
        "    def __init__(self, prior=0.0):\n",
        "        self.prior = prior\n",
        "        self.visit_count = 0\n",
        "        self.value_sum = 0.0\n",
        "        self.children = {}        # action (int) -> MCTSNode\n",
        "        self.is_expanded = False\n",
        "        self.env = None           # SnakeEnv snapshot\n",
        "        self.food_eaten = False   # True si la transicion a este nodo comio comida\n",
        "\n",
        "    def value(self):\n",
        "        if self.visit_count == 0:\n",
        "            return 0.0\n",
        "        return self.value_sum / self.visit_count\n",
        "\n",
        "\n",
        "class MCTS:\n",
        "    \"\"\"\n",
        "    Monte Carlo Tree Search estilo AlphaZero con soporte\n",
        "    para estocasticidad de comida (food sampling).\n",
        "    Acepta predict_fn y predict_batch_fn para permitir\n",
        "    batching de inferencia desde multiples hilos.\n",
        "    \"\"\"\n",
        "    def __init__(self, predict_fn, cfg, predict_batch_fn=None):\n",
        "        self.predict_fn = predict_fn\n",
        "        self.predict_batch_fn = predict_batch_fn or (lambda states: [predict_fn(s) for s in states])\n",
        "        self.c_puct = cfg.c_puct\n",
        "        self.num_simulations = cfg.num_simulations\n",
        "        self.dir_alpha = cfg.dir_alpha\n",
        "        self.dir_eps = cfg.dir_eps\n",
        "        self.food_samples = cfg.food_samples\n",
        "\n",
        "    def _predict(self, state_np):\n",
        "        \"\"\"Inferir policy y value.\"\"\"\n",
        "        return self.predict_fn(state_np)\n",
        "\n",
        "    def _ucb_score(self, parent, child):\n",
        "        \"\"\"UCB score clasico AlphaZero.\"\"\"\n",
        "        q = child.value()\n",
        "        u = self.c_puct * child.prior * math.sqrt(parent.visit_count) / (1 + child.visit_count)\n",
        "        return q + u\n",
        "\n",
        "    def _select_child(self, node):\n",
        "        \"\"\"Seleccionar hijo con mayor UCB score.\"\"\"\n",
        "        best_score = -float('inf')\n",
        "        best_action = -1\n",
        "        best_child = None\n",
        "        for action, child in node.children.items():\n",
        "            score = self._ucb_score(node, child)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_action = action\n",
        "                best_child = child\n",
        "        return best_action, best_child\n",
        "\n",
        "    def _expand(self, node):\n",
        "        \"\"\"\n",
        "        Expandir nodo hoja: evaluar con la red, crear hijos.\n",
        "        Maneja estocasticidad de comida promediando valores.\n",
        "        Retorna el valor del nodo.\n",
        "        \"\"\"\n",
        "        env = node.env\n",
        "        if env.done:\n",
        "            return 1.0 if env.is_win() else -1.0\n",
        "\n",
        "        policy, value = self._predict(env.get_state())\n",
        "\n",
        "        # --- Food stochasticity (batched via predict_batch_fn) ---\n",
        "        if node.food_eaten and self.food_samples > 1:\n",
        "            snake_set = set(env.snake)\n",
        "            free = [(x, y) for y in range(env.board_size)\n",
        "                    for x in range(env.board_size)\n",
        "                    if (x, y) not in snake_set and (x, y) != env.food]\n",
        "            k = min(self.food_samples - 1, len(free))\n",
        "            if k > 0:\n",
        "                sampled_positions = random.sample(free, k)\n",
        "                food_states = []\n",
        "                for food_pos in sampled_positions:\n",
        "                    env_copy = env.clone()\n",
        "                    env_copy.food = food_pos\n",
        "                    food_states.append(env_copy.get_state())\n",
        "                results = self.predict_batch_fn(food_states)\n",
        "                food_values = [r[1] for r in results]\n",
        "                value = (value + sum(food_values)) / (1 + len(food_values))\n",
        "\n",
        "        # Mascara de acciones validas y renormalizacion\n",
        "        valid = env.valid_actions()\n",
        "        mask = np.zeros(4, dtype=np.float32)\n",
        "        for a in valid:\n",
        "            mask[a] = 1.0\n",
        "        policy = policy * mask\n",
        "        total = policy.sum()\n",
        "        if total > 0:\n",
        "            policy /= total\n",
        "        else:\n",
        "            policy = mask / mask.sum()\n",
        "\n",
        "        # Crear hijos\n",
        "        for a in valid:\n",
        "            node.children[a] = MCTSNode(prior=policy[a])\n",
        "\n",
        "        node.is_expanded = True\n",
        "        return value\n",
        "\n",
        "    def _add_dirichlet_noise(self, node):\n",
        "        \"\"\"Agregar Dirichlet noise al root para exploracion.\"\"\"\n",
        "        actions = list(node.children.keys())\n",
        "        if not actions:\n",
        "            return\n",
        "        noise = np.random.dirichlet([self.dir_alpha] * len(actions))\n",
        "        for i, a in enumerate(actions):\n",
        "            node.children[a].prior = (\n",
        "                (1 - self.dir_eps) * node.children[a].prior +\n",
        "                self.dir_eps * noise[i]\n",
        "            )\n",
        "\n",
        "    def search(self, root_env, temperature=1.0):\n",
        "        \"\"\"\n",
        "        Ejecutar MCTS completo desde un estado raiz.\n",
        "        Retorna distribucion de probabilidad sobre acciones (4,).\n",
        "        \"\"\"\n",
        "        root = MCTSNode()\n",
        "        root.env = root_env.clone()\n",
        "\n",
        "        # Expandir raiz\n",
        "        self._expand(root)\n",
        "        self._add_dirichlet_noise(root)\n",
        "\n",
        "        # Simulaciones\n",
        "        for _ in range(self.num_simulations):\n",
        "            node = root\n",
        "            path = [node]\n",
        "\n",
        "            # --- SELECT ---\n",
        "            while node.is_expanded and node.children:\n",
        "                action, child = self._select_child(node)\n",
        "\n",
        "                # Computacion lazy del estado del hijo\n",
        "                if child.env is None:\n",
        "                    env_copy = node.env.clone()\n",
        "                    old_score = env_copy.score\n",
        "                    env_copy.step(action)\n",
        "                    child.env = env_copy\n",
        "                    child.food_eaten = (env_copy.score > old_score and not env_copy.done)\n",
        "\n",
        "                node = child\n",
        "                path.append(node)\n",
        "\n",
        "            # --- EXPAND & EVALUATE ---\n",
        "            if not node.is_expanded:\n",
        "                value = self._expand(node)\n",
        "            else:\n",
        "                # Nodo terminal (expanded pero sin hijos)\n",
        "                value = 1.0 if node.env.is_win() else -1.0\n",
        "\n",
        "            # --- BACKUP ---\n",
        "            for n in reversed(path):\n",
        "                n.visit_count += 1\n",
        "                n.value_sum += value\n",
        "\n",
        "        # --- Construir distribucion de acciones ---\n",
        "        visits = np.zeros(4, dtype=np.float32)\n",
        "        for a, child in root.children.items():\n",
        "            visits[a] = child.visit_count\n",
        "\n",
        "        if temperature == 0:\n",
        "            # Greedy\n",
        "            best = np.argmax(visits)\n",
        "            probs = np.zeros(4, dtype=np.float32)\n",
        "            probs[best] = 1.0\n",
        "        else:\n",
        "            # Con temperatura\n",
        "            visits_temp = np.power(visits, 1.0 / temperature)\n",
        "            total = visits_temp.sum()\n",
        "            if total > 0:\n",
        "                probs = visits_temp / total\n",
        "            else:\n",
        "                probs = np.ones(4, dtype=np.float32) / 4.0\n",
        "\n",
        "        return probs\n",
        "\n",
        "print(\"MCTS implementado correctamente.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ade956a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 5b: InferenceBatcher \u2014 Batching de GPU para self-play paralelo\n",
        "# ============================================================\n",
        "import queue as queue_module\n",
        "import concurrent.futures\n",
        "\n",
        "class InferenceBatcher:\n",
        "    \"\"\"\n",
        "    Recolecta requests de inferencia de multiples hilos y los\n",
        "    procesa en batches en la GPU. Esto maximiza la utilizacion\n",
        "    de la GPU durante self-play paralelo.\n",
        "    \"\"\"\n",
        "    def __init__(self, net, device, max_batch=64, timeout_ms=3):\n",
        "        self.net = net\n",
        "        self.device = device\n",
        "        self.max_batch = max_batch\n",
        "        self.timeout = timeout_ms / 1000.0\n",
        "        self._queue = queue_module.Queue()\n",
        "        self._running = False\n",
        "        self._thread = None\n",
        "        self._total_calls = 0\n",
        "        self._total_batches = 0\n",
        "\n",
        "    def start(self):\n",
        "        self._running = True\n",
        "        self._total_calls = 0\n",
        "        self._total_batches = 0\n",
        "        self._thread = threading.Thread(target=self._worker, daemon=True)\n",
        "        self._thread.start()\n",
        "        return self\n",
        "\n",
        "    def stop(self):\n",
        "        self._running = False\n",
        "        self._queue.put(None)  # sentinel\n",
        "        if self._thread:\n",
        "            self._thread.join(timeout=10)\n",
        "\n",
        "    def submit(self, state_np):\n",
        "        \"\"\"Non-blocking: enviar estado, retorna Future.\"\"\"\n",
        "        f = concurrent.futures.Future()\n",
        "        self._queue.put((state_np, f))\n",
        "        return f\n",
        "\n",
        "    def predict(self, state_np):\n",
        "        \"\"\"Blocking: enviar estado, esperar resultado (policy, value).\"\"\"\n",
        "        return self.submit(state_np).result()\n",
        "\n",
        "    def predict_batch(self, states_list):\n",
        "        \"\"\"Enviar multiples estados, esperar todos los resultados.\"\"\"\n",
        "        if not states_list:\n",
        "            return []\n",
        "        futures = [self.submit(s) for s in states_list]\n",
        "        return [f.result() for f in futures]\n",
        "\n",
        "    def stats(self):\n",
        "        avg = self._total_calls / max(self._total_batches, 1)\n",
        "        return self._total_calls, self._total_batches, avg\n",
        "\n",
        "    def _worker(self):\n",
        "        while self._running:\n",
        "            batch = []\n",
        "            # Esperar primer request\n",
        "            try:\n",
        "                item = self._queue.get(timeout=1.0)\n",
        "            except queue_module.Empty:\n",
        "                continue\n",
        "            if item is None:\n",
        "                break\n",
        "            batch.append(item)\n",
        "\n",
        "            # Recolectar mas requests (hasta max_batch o timeout)\n",
        "            deadline = time.time() + self.timeout\n",
        "            while len(batch) < self.max_batch:\n",
        "                remaining = deadline - time.time()\n",
        "                if remaining <= 0:\n",
        "                    break\n",
        "                try:\n",
        "                    item = self._queue.get(timeout=max(0.0001, remaining))\n",
        "                except queue_module.Empty:\n",
        "                    break\n",
        "                if item is None:\n",
        "                    self._running = False\n",
        "                    break\n",
        "                batch.append(item)\n",
        "\n",
        "            if not batch:\n",
        "                continue\n",
        "\n",
        "            states = [b[0] for b in batch]\n",
        "            futures_list = [b[1] for b in batch]\n",
        "            self._total_calls += len(batch)\n",
        "            self._total_batches += 1\n",
        "\n",
        "            try:\n",
        "                arr = np.stack(states)\n",
        "                t = torch.as_tensor(arr, device=self.device)\n",
        "                with torch.no_grad():\n",
        "                    policies, values = self.net(t)\n",
        "                p_np = policies.cpu().numpy()\n",
        "                v_np = values.cpu().numpy()\n",
        "                for i, f in enumerate(futures_list):\n",
        "                    f.set_result((p_np[i], float(v_np[i, 0])))\n",
        "            except Exception as e:\n",
        "                for f in futures_list:\n",
        "                    if not f.done():\n",
        "                        f.set_exception(e)\n",
        "\n",
        "\n",
        "def make_predict_fn(net, device):\n",
        "    \"\"\"Crear funcion de prediccion directa (para uso single-thread, eval).\"\"\"\n",
        "    @torch.no_grad()\n",
        "    def predict(state_np):\n",
        "        t = torch.as_tensor(state_np, device=device).unsqueeze(0)\n",
        "        p, v = net(t)\n",
        "        return p[0].cpu().numpy(), v.item()\n",
        "    return predict\n",
        "\n",
        "print(\"InferenceBatcher implementado.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe45e252",
      "metadata": {},
      "source": [
        "## Self-Play, Replay Buffer, Training y Evaluacion\n",
        "\n",
        "- **Self-play**: jugar partidas completas usando MCTS, guardar `(state, pi, z)` por cada paso\n",
        "- **Replay Buffer**: buffer circular de 200k posiciones\n",
        "- **Training**: Adam, loss = MSE(z, v) + CrossEntropy(pi, p) + L2 regularizacion\n",
        "- **Evaluacion**: comparar modelos por win rate independiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc64eb4a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 6: Self-Play\n",
        "# ============================================================\n",
        "\n",
        "def self_play_game(predict_fn, cfg, predict_batch_fn=None, progress_cb=None):\n",
        "    \"\"\"\n",
        "    Jugar una partida completa usando MCTS.\n",
        "    predict_fn: callable(state_np) -> (policy_np, value_float)\n",
        "    predict_batch_fn: callable(list[state_np]) -> list[(policy, value)]\n",
        "    progress_cb: callable(move_count, score, length) opcional para telemetria.\n",
        "    Retorna: (examples, is_win, score, num_moves)\n",
        "    \"\"\"\n",
        "    env = SnakeEnv(cfg.board_size, cfg.max_steps)\n",
        "    mcts = MCTS(predict_fn, cfg, predict_batch_fn)\n",
        "    env.reset()\n",
        "\n",
        "    states = []\n",
        "    policies = []\n",
        "    move_count = 0\n",
        "\n",
        "    while not env.done:\n",
        "        # Temperatura: 1.0 hasta move temp_decay_move, luego 0.0\n",
        "        temp = cfg.temp_init if move_count < cfg.temp_decay_move else cfg.temp_final\n",
        "\n",
        "        # Busqueda MCTS\n",
        "        pi = mcts.search(env, temperature=temp)\n",
        "\n",
        "        # Guardar datos de entrenamiento\n",
        "        states.append(env.get_state().copy())\n",
        "        policies.append(pi.copy())\n",
        "\n",
        "        # Seleccionar accion\n",
        "        if temp == 0:\n",
        "            action = int(np.argmax(pi))\n",
        "        else:\n",
        "            action = int(np.random.choice(4, p=pi))\n",
        "\n",
        "        # Ejecutar\n",
        "        env.step(action)\n",
        "        move_count += 1\n",
        "        if progress_cb is not None:\n",
        "            tick_every = max(25, int(getattr(cfg, \"game_tick_moves\", 100)))\n",
        "            if move_count == 1 or move_count % tick_every == 0:\n",
        "                progress_cb(move_count, env.score, len(env.snake))\n",
        "\n",
        "    # Determinar resultado: +1 si gano (lleno tablero), -1 si no\n",
        "    z = 1.0 if env.is_win() else -1.0\n",
        "\n",
        "    # Crear ejemplos de entrenamiento\n",
        "    examples = [(s, p, z) for s, p in zip(states, policies)]\n",
        "    return examples, env.is_win(), env.score, move_count, len(env.snake)\n",
        "\n",
        "\n",
        "def run_n_games(n, request_queue, response_queue, progress_queue, worker_id, cfg):\n",
        "    \"\"\"\n",
        "    Ejecutar n partidas de self-play en un proceso worker.\n",
        "    Usa request_queue/response_queue para obtener (policy, value) del proceso principal.\n",
        "    Necesario para multiprocessing: cada worker usa su propio nucleo (sin GIL).\n",
        "    \"\"\"\n",
        "    def predict(state_np):\n",
        "        request_queue.put((worker_id, state_np))\n",
        "        return response_queue.get()\n",
        "\n",
        "    def predict_batch(states_list):\n",
        "        return [predict(s) for s in states_list]\n",
        "\n",
        "    all_examples = []\n",
        "    wins = 0\n",
        "    scores_list = []\n",
        "    moves_list = []\n",
        "    lengths_list = []\n",
        "\n",
        "    if progress_queue is not None:\n",
        "        progress_queue.put({\"type\": \"worker_start\", \"worker\": worker_id, \"games\": n})\n",
        "\n",
        "    for game_idx in range(n):\n",
        "        if progress_queue is not None:\n",
        "            progress_queue.put({\"type\": \"game_start\", \"worker\": worker_id, \"game_local\": game_idx + 1})\n",
        "\n",
        "        def _progress_cb(move_count, score_now, len_now):\n",
        "            if progress_queue is not None:\n",
        "                progress_queue.put({\n",
        "                    \"type\": \"game_tick\",\n",
        "                    \"worker\": worker_id,\n",
        "                    \"game_local\": game_idx + 1,\n",
        "                    \"moves\": move_count,\n",
        "                    \"score\": score_now,\n",
        "                    \"length\": len_now,\n",
        "                })\n",
        "\n",
        "        examples, won, score, moves, length = self_play_game(predict, cfg, predict_batch, progress_cb=_progress_cb)\n",
        "        all_examples.extend(examples)\n",
        "        if won:\n",
        "            wins += 1\n",
        "        scores_list.append(score)\n",
        "        moves_list.append(moves)\n",
        "        lengths_list.append(length)\n",
        "\n",
        "        if progress_queue is not None:\n",
        "            progress_queue.put({\n",
        "                \"type\": \"game_end\",\n",
        "                \"worker\": worker_id,\n",
        "                \"game_local\": game_idx + 1,\n",
        "                \"won\": won,\n",
        "                \"score\": score,\n",
        "                \"moves\": moves,\n",
        "                \"length\": length,\n",
        "                \"examples\": len(examples),\n",
        "            })\n",
        "\n",
        "    return all_examples, wins, scores_list, moves_list, lengths_list\n",
        "\n",
        "\n",
        "print(\"Self-play implementado.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8484df3d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 7: Replay Buffer y Training\n",
        "# ============================================================\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Buffer circular para almacenar ejemplos de self-play.\"\"\"\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "\n",
        "    def add(self, examples):\n",
        "        \"\"\"Agregar lista de (state, policy, value) al buffer.\"\"\"\n",
        "        self.buffer.extend(examples)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Samplear un batch aleatorio.\"\"\"\n",
        "        n = min(batch_size, len(self.buffer))\n",
        "        indices = np.random.choice(len(self.buffer), size=n, replace=False)\n",
        "        batch = [self.buffer[i] for i in indices]\n",
        "        states = np.array([b[0] for b in batch])\n",
        "        policies = np.array([b[1] for b in batch])\n",
        "        values = np.array([b[2] for b in batch], dtype=np.float32)\n",
        "        return states, policies, values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def save(self, path):\n",
        "        with open(path, 'wb') as f:\n",
        "            pickle.dump(list(self.buffer), f)\n",
        "\n",
        "    def load(self, path):\n",
        "        if os.path.exists(path):\n",
        "            with open(path, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "            self.buffer = deque(data, maxlen=self.buffer.maxlen)\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "\n",
        "def train_epoch(net, optimizer, buffer, cfg, device, scaler=None):\n",
        "    \"\"\"\n",
        "    Entrenar una epoca sobre el replay buffer.\n",
        "    Loss = (z - v)^2 - pi * log(p)\n",
        "    (weight_decay ya esta en el optimizer como L2)\n",
        "    \"\"\"\n",
        "    net.train()\n",
        "    total_loss = 0.0\n",
        "    total_p_loss = 0.0\n",
        "    total_v_loss = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    num_batches = max(1, len(buffer) // cfg.batch_size)\n",
        "    use_amp = bool(getattr(cfg, \"use_amp\", True) and device.type == \"cuda\")\n",
        "\n",
        "    for _ in range(num_batches):\n",
        "        states, policies, values = buffer.sample(cfg.batch_size)\n",
        "\n",
        "        states_t = torch.from_numpy(states)\n",
        "        policies_t = torch.from_numpy(policies)\n",
        "        values_t = torch.from_numpy(values).unsqueeze(1)\n",
        "\n",
        "        if device.type == \"cuda\":\n",
        "            states_t = states_t.pin_memory().to(device, non_blocking=True)\n",
        "            policies_t = policies_t.pin_memory().to(device, non_blocking=True)\n",
        "            values_t = values_t.pin_memory().to(device, non_blocking=True)\n",
        "        else:\n",
        "            states_t = states_t.to(device)\n",
        "            policies_t = policies_t.to(device)\n",
        "            values_t = values_t.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=use_amp):\n",
        "            pred_p, pred_v = net(states_t)\n",
        "\n",
        "            # Value loss: MSE\n",
        "            v_loss = F.mse_loss(pred_v, values_t)\n",
        "\n",
        "            # Policy loss: cross-entropy (pi * log(p))\n",
        "            p_loss = -torch.mean(torch.sum(policies_t * torch.log(pred_p + 1e-8), dim=1))\n",
        "\n",
        "            loss = v_loss + p_loss\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        if use_amp and scaler is not None:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_p_loss += p_loss.item()\n",
        "        total_v_loss += v_loss.item()\n",
        "        n_batches += 1\n",
        "\n",
        "    if n_batches == 0:\n",
        "        return 0, 0, 0\n",
        "    return total_loss / n_batches, total_p_loss / n_batches, total_v_loss / n_batches\n",
        "\n",
        "print(\"ReplayBuffer y train_epoch implementados.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6881665",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 8: Evaluacion\n",
        "# ============================================================\n",
        "\n",
        "def evaluate_mcts(net, cfg, device, n_games=None):\n",
        "    \"\"\"\n",
        "    Evaluar modelo usando MCTS (greedy, temp=0).\n",
        "    Retorna (win_rate, avg_score, scores).\n",
        "    \"\"\"\n",
        "    if n_games is None:\n",
        "        n_games = cfg.eval_games\n",
        "    net.eval()\n",
        "\n",
        "    wins = 0\n",
        "    total_score = 0\n",
        "    scores = []\n",
        "\n",
        "    for i in tqdm(range(n_games), desc=\"Eval MCTS\", leave=False):\n",
        "        env = SnakeEnv(cfg.board_size, cfg.max_steps)\n",
        "        mcts_eval = MCTS(make_predict_fn(net, device), cfg)\n",
        "        env.reset()\n",
        "\n",
        "        while not env.done:\n",
        "            pi = mcts_eval.search(env, temperature=0)\n",
        "            action = int(np.argmax(pi))\n",
        "            env.step(action)\n",
        "\n",
        "        if env.is_win():\n",
        "            wins += 1\n",
        "        total_score += env.score\n",
        "        scores.append(env.score)\n",
        "\n",
        "    wr = wins / max(n_games, 1)\n",
        "    avg = total_score / max(n_games, 1)\n",
        "    return wr, avg, scores\n",
        "\n",
        "\n",
        "def evaluate_policy_only(net, cfg, device, n_games=200):\n",
        "    \"\"\"\n",
        "    Evaluacion rapida usando solo la policy network (sin MCTS).\n",
        "    Util para monitoreo rapido entre iteraciones.\n",
        "    \"\"\"\n",
        "    net.eval()\n",
        "    wins = 0\n",
        "    total_score = 0\n",
        "\n",
        "    for _ in tqdm(range(n_games), desc=\"Eval Policy\", leave=False):\n",
        "        env = SnakeEnv(cfg.board_size, cfg.max_steps)\n",
        "        env.reset()\n",
        "\n",
        "        while not env.done:\n",
        "            state_t = torch.from_numpy(env.get_state()).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                policy, _ = net(state_t)\n",
        "\n",
        "            # Mascara de acciones validas\n",
        "            valid = env.valid_actions()\n",
        "            mask = torch.zeros(4, device=device)\n",
        "            for a in valid:\n",
        "                mask[a] = 1.0\n",
        "            policy = policy.squeeze() * mask\n",
        "            total = policy.sum()\n",
        "            if total > 0:\n",
        "                policy = policy / total\n",
        "            else:\n",
        "                policy = mask / mask.sum()\n",
        "\n",
        "            action = int(torch.argmax(policy).item())\n",
        "            env.step(action)\n",
        "\n",
        "        if env.is_win():\n",
        "            wins += 1\n",
        "        total_score += env.score\n",
        "\n",
        "    wr = wins / max(n_games, 1)\n",
        "    avg = total_score / max(n_games, 1)\n",
        "    return wr, avg\n",
        "\n",
        "print(\"Funciones de evaluacion implementadas.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9dd4d65",
      "metadata": {},
      "source": [
        "## Loop Principal de Entrenamiento\n",
        "\n",
        "Ciclo completo: Self-play \u2192 Train \u2192 Evaluate \u2192 Champion update.\n",
        "Guarda checkpoints en `save_dir` (Colab: Drive; Vast.ai: /workspace o volumen) para recuperar de desconexiones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae4e1d90",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 9: Loop Principal de Entrenamiento\n",
        "# ============================================================\n",
        "\n",
        "def save_checkpoint(net, best_net, optimizer, iteration, best_win_rate, cfg, buffer):\n",
        "    \"\"\"Guardar checkpoint completo en save_dir.\"\"\"\n",
        "    ckpt = {\n",
        "        'iteration': iteration,\n",
        "        'net': net.state_dict(),\n",
        "        'best_net': best_net.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'best_win_rate': best_win_rate,\n",
        "        'config': dataclasses.asdict(cfg),\n",
        "    }\n",
        "    path = os.path.join(cfg.save_dir, 'latest_checkpoint.pt')\n",
        "    torch.save(ckpt, path)\n",
        "\n",
        "    # Guardar mejor modelo por separado\n",
        "    best_path = os.path.join(cfg.save_dir, 'best_model.pt')\n",
        "    torch.save(best_net.state_dict(), best_path)\n",
        "\n",
        "    # Guardar buffer\n",
        "    buffer_path = os.path.join(cfg.save_dir, 'buffer.pkl')\n",
        "    buffer.save(buffer_path)\n",
        "\n",
        "    print(f\"  Checkpoint guardado (iter {iteration})\")\n",
        "\n",
        "\n",
        "def load_checkpoint(net, best_net, optimizer, cfg, buffer):\n",
        "    \"\"\"Cargar checkpoint desde save_dir si existe.\"\"\"\n",
        "    path = os.path.join(cfg.save_dir, 'latest_checkpoint.pt')\n",
        "    if not os.path.exists(path):\n",
        "        return 0, 0.0\n",
        "\n",
        "    ckpt = torch.load(path, map_location=device)\n",
        "    net.load_state_dict(ckpt['net'])\n",
        "    best_net.load_state_dict(ckpt['best_net'])\n",
        "    optimizer.load_state_dict(ckpt['optimizer'])\n",
        "\n",
        "    buffer_path = os.path.join(cfg.save_dir, 'buffer.pkl')\n",
        "    buffer.load(buffer_path)\n",
        "\n",
        "    iteration = ckpt.get('iteration', 0)\n",
        "    best_wr = ckpt.get('best_win_rate', 0.0)\n",
        "    print(f\"Checkpoint cargado: iteracion {iteration}, best_win_rate={best_wr:.3f}\")\n",
        "    print(f\"Buffer restaurado: {len(buffer)} ejemplos\")\n",
        "    return iteration, best_wr\n",
        "\n",
        "\n",
        "def train_alphasnake(cfg):\n",
        "    \"\"\"Loop principal de entrenamiento AlphaSnake.\"\"\"\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\" AlphaSnake Training\")\n",
        "    print(f\" Board: {cfg.board_size}x{cfg.board_size}\")\n",
        "    print(f\" Simulations: {cfg.num_simulations}\")\n",
        "    print(f\" Games/iter: {cfg.games_per_iter}\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    # Crear redes\n",
        "    net = AlphaSnakeNet(4, cfg.net_blocks, cfg.net_channels, cfg.board_size).to(device)\n",
        "    best_net = AlphaSnakeNet(4, cfg.net_blocks, cfg.net_channels, cfg.board_size).to(device)\n",
        "    best_net.load_state_dict(net.state_dict())\n",
        "\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    buffer = ReplayBuffer(cfg.buffer_size)\n",
        "    scaler = torch.amp.GradScaler(device=\"cuda\", enabled=bool(getattr(cfg, \"use_amp\", True) and device.type == \"cuda\"))\n",
        "\n",
        "    # Intentar cargar checkpoint\n",
        "    start_iter, best_win_rate = load_checkpoint(net, best_net, optimizer, cfg, buffer)\n",
        "\n",
        "    training_start = time.time()\n",
        "    for iteration in range(start_iter, cfg.max_iterations):\n",
        "        iter_start = time.time()\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"  ITERACION {iteration + 1} / {cfg.max_iterations}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # ------------------------------------------------\n",
        "        # 1. SELF-PLAY PARALELO con procesos (usa todos los nucleos; evita GIL)\n",
        "        # ------------------------------------------------\n",
        "        cpu_total = os.cpu_count() or 8\n",
        "        n_workers = max(1, min(int(getattr(cfg, \"selfplay_workers\", 5)), cpu_total))\n",
        "        best_net.eval()\n",
        "        all_examples = []\n",
        "        sp_wins = 0\n",
        "        sp_scores = []\n",
        "        sp_moves = []\n",
        "        sp_lengths = []\n",
        "\n",
        "        # Manager().Queue() es pasable a procesos con spawn (Vast.ai/Jupyter); Queue() solo por herencia (fork)\n",
        "        mp_manager = multiprocessing.Manager()\n",
        "        mp_request_queue = mp_manager.Queue()\n",
        "        response_queues = [mp_manager.Queue() for _ in range(n_workers)]\n",
        "        progress_queue = mp_manager.Queue()\n",
        "\n",
        "        def inference_server():\n",
        "            batch_timeout = max(0.001, float(getattr(cfg, \"inference_timeout_ms\", 20)) / 1000.0)\n",
        "            max_batch = max(8, int(getattr(cfg, \"inference_batch_size\", 64)))\n",
        "            use_amp = bool(getattr(cfg, \"use_amp\", True) and device.type == \"cuda\")\n",
        "            while True:\n",
        "                batch = []\n",
        "                deadline = time.time() + batch_timeout\n",
        "                while len(batch) < max_batch:\n",
        "                    try:\n",
        "                        item = mp_request_queue.get(timeout=max(0.001, deadline - time.time()))\n",
        "                        if item is None:\n",
        "                            return\n",
        "                        batch.append(item)\n",
        "                    except Empty:\n",
        "                        break\n",
        "                if not batch:\n",
        "                    continue\n",
        "                wids = [b[0] for b in batch]\n",
        "                states = [b[1] for b in batch]\n",
        "                arr = np.stack(states)\n",
        "                t = torch.from_numpy(arr)\n",
        "                if device.type == \"cuda\":\n",
        "                    t = t.pin_memory().to(device, non_blocking=True)\n",
        "                else:\n",
        "                    t = t.to(device)\n",
        "                with torch.no_grad():\n",
        "                    with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=use_amp):\n",
        "                        policies, values = best_net(t)\n",
        "                p_np = policies.cpu().numpy()\n",
        "                v_np = values.cpu().numpy()\n",
        "                for i in range(len(batch)):\n",
        "                    response_queues[wids[i]].put((p_np[i], float(v_np[i, 0])))\n",
        "\n",
        "        server_thread = threading.Thread(target=inference_server, daemon=True)\n",
        "        server_thread.start()\n",
        "\n",
        "        iter_stamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        print(f\"  [Iter {iteration + 1}] Inicio confirmado: {iter_stamp}\")\n",
        "        print(f\"  [Self-play] Arranco con {n_workers} workers y {cfg.games_per_iter} juegos objetivo\")\n",
        "\n",
        "        sp_start = time.time()\n",
        "        games_per_worker = [cfg.games_per_iter // n_workers + (1 if i < cfg.games_per_iter % n_workers else 0) for i in range(n_workers)]\n",
        "\n",
        "        with concurrent.futures.ProcessPoolExecutor(max_workers=n_workers) as pool:\n",
        "            futures = [\n",
        "                pool.submit(run_n_games, games_per_worker[i], mp_request_queue, response_queues[i], progress_queue, i, cfg)\n",
        "                for i in range(n_workers)\n",
        "            ]\n",
        "            pending = set(futures)\n",
        "            completed = 0\n",
        "            games_reported = 0\n",
        "            verbose_games = bool(getattr(cfg, \"verbose_game_updates\", True))\n",
        "            log_every = max(1, int(getattr(cfg, \"game_log_interval\", 1)))\n",
        "            heartbeat_s = max(10, int(getattr(cfg, \"heartbeat_seconds\", 30)))\n",
        "            last_heartbeat = time.time()\n",
        "\n",
        "            with tqdm(total=cfg.games_per_iter, desc=f\"Self-play iter {iteration+1}\", mininterval=0.5, dynamic_ncols=True) as pbar:\n",
        "                while pending:\n",
        "                    drained = 0\n",
        "                    while True:\n",
        "                        try:\n",
        "                            msg = progress_queue.get_nowait()\n",
        "                        except Empty:\n",
        "                            break\n",
        "\n",
        "                        drained += 1\n",
        "                        msg_type = msg.get(\"type\", \"unknown\") if isinstance(msg, dict) else \"unknown\"\n",
        "\n",
        "                        if msg_type == \"worker_start\":\n",
        "                            print(f\"      [Worker {msg['worker']}] iniciado | juegos_asignados={msg['games']}\", flush=True)\n",
        "                            continue\n",
        "\n",
        "                        if msg_type == \"game_start\":\n",
        "                            print(f\"      [Worker {msg['worker']}] inicio juego local={msg['game_local']}\", flush=True)\n",
        "                            continue\n",
        "\n",
        "                        if msg_type == \"game_tick\":\n",
        "                            if verbose_games:\n",
        "                                print(\n",
        "                                    f\"      [Tick] worker={msg['worker']} juego={msg['game_local']} \"\n",
        "                                    f\"moves={msg['moves']} score={msg['score']} len={msg['length']}\",\n",
        "                                    flush=True,\n",
        "                                )\n",
        "                            continue\n",
        "\n",
        "                        if msg_type == \"game_end\":\n",
        "                            games_reported += 1\n",
        "                            pbar.update(1)\n",
        "                            if verbose_games and (games_reported == 1 or games_reported % log_every == 0):\n",
        "                                result_tag = \"WIN\" if msg[\"won\"] else \"LOSE\"\n",
        "                                print(\n",
        "                                    f\"      [Juego {games_reported}/{cfg.games_per_iter}] \"\n",
        "                                    f\"worker={msg['worker']} local={msg['game_local']} {result_tag} \"\n",
        "                                    f\"score={msg['score']} moves={msg['moves']} len={msg['length']} ex={msg['examples']}\",\n",
        "                                    flush=True,\n",
        "                                )\n",
        "                            continue\n",
        "\n",
        "                    done_now, pending = concurrent.futures.wait(\n",
        "                        pending,\n",
        "                        timeout=1.0,\n",
        "                        return_when=concurrent.futures.FIRST_COMPLETED,\n",
        "                    )\n",
        "\n",
        "                    if not done_now:\n",
        "                        now = time.time()\n",
        "                        if drained == 0 and now - last_heartbeat >= heartbeat_s:\n",
        "                            print(\n",
        "                                f\"      [Heartbeat] esperando progreso | juegos={games_reported}/{cfg.games_per_iter} | workers listos={completed}/{n_workers} (siguiente tick/juego cuando avance MCTS)\",\n",
        "                                flush=True,\n",
        "                            )\n",
        "                            last_heartbeat = now\n",
        "                        continue\n",
        "\n",
        "                    for future in done_now:\n",
        "                        ex, w, sc, mv, ln = future.result()\n",
        "                        all_examples.extend(ex)\n",
        "                        sp_wins += w\n",
        "                        sp_scores.extend(sc)\n",
        "                        sp_moves.extend(mv)\n",
        "                        sp_lengths.extend(ln)\n",
        "                        completed += 1\n",
        "                        games_done = len(sp_scores)\n",
        "                        wr_so_far = 100 * sp_wins / games_done if games_done else 0\n",
        "                        avg_sc = np.mean(sp_scores) if sp_scores else 0\n",
        "                        print(f\"      [Worker completado] {completed}/{n_workers} | juegos agregados={len(sc)} | acumulado={games_done}/{cfg.games_per_iter} | wins={sp_wins} ({wr_so_far:.1f}%) | avg_score={avg_sc:.1f} | ejemplos={len(all_examples)}\", flush=True)\n",
        "\n",
        "        for _ in range(n_workers):\n",
        "            mp_request_queue.put(None)\n",
        "        server_thread.join(timeout=10)\n",
        "\n",
        "        sp_time = time.time() - sp_start\n",
        "        sp_wr = sp_wins / cfg.games_per_iter\n",
        "        sp_scores_arr = np.array(sp_scores)\n",
        "        sp_moves_arr = np.array(sp_moves)\n",
        "        sp_lengths_arr = np.array(sp_lengths)\n",
        "\n",
        "        ejemplos_per_min = len(all_examples) / (sp_time / 60.0) if sp_time > 0 else 0\n",
        "        print(f\"  --- Self-play ({sp_time:.0f}s, {n_workers} procesos) ---\")\n",
        "        print(f\"    Juegos: {cfg.games_per_iter} | Wins: {sp_wins} ({100*sp_wr:.1f}%) | Ejemplos: {len(all_examples)} ({ejemplos_per_min:.0f}/min)\")\n",
        "        print(f\"    Score  -> min: {sp_scores_arr.min():.0f}  med: {np.median(sp_scores_arr):.0f}  max: {sp_scores_arr.max():.0f}  avg: {sp_scores_arr.mean():.1f}\")\n",
        "        print(f\"    Length -> min: {sp_lengths_arr.min():.0f}  med: {np.median(sp_lengths_arr):.0f}  max: {sp_lengths_arr.max():.0f}  (max: {cfg.board_size**2})\")\n",
        "        print(f\"    Moves  -> min: {sp_moves_arr.min():.0f}  med: {np.median(sp_moves_arr):.0f}  max: {sp_moves_arr.max():.0f}\")\n",
        "\n",
        "        buffer.add(all_examples)\n",
        "        print(f\"    Buffer total: {len(buffer)}\")\n",
        "\n",
        "        # ------------------------------------------------\n",
        "        # 2. TRAINING\n",
        "        # ------------------------------------------------\n",
        "        train_start = time.time()\n",
        "        last_loss, last_p, last_v = 0.0, 0.0, 0.0\n",
        "        for epoch in tqdm(range(cfg.epochs_per_iter), desc=f\"Training iter {iteration+1}\", leave=False):\n",
        "            last_loss, last_p, last_v = train_epoch(net, optimizer, buffer, cfg, device, scaler=scaler)\n",
        "            if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "                tqdm.write(f\"    Epoch {epoch+1}/{cfg.epochs_per_iter}: loss={last_loss:.4f} (policy={last_p:.4f}, value={last_v:.4f})\")\n",
        "        train_time = time.time() - train_start\n",
        "        print(f\"  --- Training ({train_time:.0f}s, {cfg.epochs_per_iter} epochs) ---\")\n",
        "        print(f\"    Loss: total={last_loss:.4f} | policy={last_p:.4f} | value={last_v:.4f}\")\n",
        "\n",
        "        # ------------------------------------------------\n",
        "        # 3. EVALUACION RAPIDA (policy-only para velocidad)\n",
        "        # ------------------------------------------------\n",
        "        eval_start = time.time()\n",
        "        wr_new_pol, avg_new_pol = evaluate_policy_only(net, cfg, device, n_games=50)\n",
        "        wr_best_pol, avg_best_pol = evaluate_policy_only(best_net, cfg, device, n_games=50)\n",
        "        eval_time = time.time() - eval_start\n",
        "\n",
        "        print(f\"  --- Eval policy-only (50 juegos, {eval_time:.0f}s) ---\")\n",
        "        print(f\"    Nuevo:  win_rate={100*wr_new_pol:.1f}%  avg_score={avg_new_pol:.1f}\")\n",
        "        print(f\"    Mejor:  win_rate={100*wr_best_pol:.1f}%  avg_score={avg_best_pol:.1f}\")\n",
        "\n",
        "        # ------------------------------------------------\n",
        "        # 4. CHAMPION UPDATE\n",
        "        # ------------------------------------------------\n",
        "        # Comparar por score promedio (mas granular que win rate para scores bajos)\n",
        "        if avg_new_pol > avg_best_pol * cfg.accept_threshold / 0.5:\n",
        "            print(f\"  >>> NUEVO CHAMPION aceptado! <<<\")\n",
        "            best_net.load_state_dict(net.state_dict())\n",
        "            best_win_rate = max(best_win_rate, wr_new_pol)\n",
        "        else:\n",
        "            print(f\"  Mejor modelo retenido.\")\n",
        "            net.load_state_dict(best_net.state_dict())\n",
        "\n",
        "        # ------------------------------------------------\n",
        "        # 5. RESUMEN ITERACION\n",
        "        # ------------------------------------------------\n",
        "        iter_time = time.time() - iter_start\n",
        "        total_elapsed = time.time() - training_start\n",
        "        iters_done = iteration - start_iter + 1\n",
        "        iters_left = cfg.max_iterations - iteration - 1\n",
        "        eta_sec = (iter_time * iters_left) if iters_left > 0 else 0\n",
        "        eta_min = eta_sec / 60\n",
        "\n",
        "        print(f\"  --- Resumen iter {iteration+1}/{cfg.max_iterations} ---\")\n",
        "        print(f\"    Tiempos: self-play={sp_time:.0f}s  train={train_time:.0f}s  eval={eval_time:.0f}s  total={iter_time:.0f}s\")\n",
        "        print(f\"    Acumulado: {total_elapsed/60:.1f} min | Restantes: {iters_left} iters | ETA ~{eta_min:.0f} min\")\n",
        "        print(f\"    Buffer: {len(buffer)} posiciones | Mejor win_rate (policy): {best_win_rate:.3f}\")\n",
        "        if (iteration + 1) % cfg.checkpoint_interval == 0:\n",
        "            save_checkpoint(net, best_net, optimizer, iteration + 1, best_win_rate, cfg, buffer)\n",
        "            print(f\"    Checkpoint guardado (iter {iteration+1})\")\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"  ENTRENAMIENTO COMPLETADO\")\n",
        "    print(f\"  Mejor win rate (policy): {best_win_rate:.3f}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    return best_net\n",
        "\n",
        "print(\"Loop de entrenamiento definido. Ejecutar la siguiente celda para entrenar.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aca6bd9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell X: Benchmark rapido de throughput (opcional)\n",
        "# ============================================================\n",
        "\n",
        "def benchmark_train_epoch(cfg, n_batches=80):\n",
        "    \"\"\"\n",
        "    Benchmark sintetico del throughput de entrenamiento.\n",
        "    Mide samples/s usando train_epoch sobre un buffer artificial.\n",
        "    \"\"\"\n",
        "    net_bench = AlphaSnakeNet(4, cfg.net_blocks, cfg.net_channels, cfg.board_size).to(device)\n",
        "    optimizer_bench = torch.optim.Adam(net_bench.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "    scaler_bench = torch.amp.GradScaler(device=\"cuda\", enabled=bool(getattr(cfg, \"use_amp\", True) and device.type == \"cuda\"))\n",
        "\n",
        "    # Llenar buffer con datos sinteticos para medir solo pipeline de train.\n",
        "    rb = ReplayBuffer(max(cfg.batch_size * (n_batches + 2), cfg.batch_size * 16))\n",
        "    synth_states = np.random.randn(cfg.batch_size, 4, cfg.board_size, cfg.board_size).astype(np.float32)\n",
        "    synth_policies = np.full((cfg.batch_size, 4), 0.25, dtype=np.float32)\n",
        "    synth_values = np.random.uniform(-1, 1, size=(cfg.batch_size,)).astype(np.float32)\n",
        "\n",
        "    block = list(zip(synth_states, synth_policies, synth_values))\n",
        "    for _ in range(n_batches + 4):\n",
        "        rb.add(block)\n",
        "\n",
        "    cfg_bench = dataclasses.replace(cfg)\n",
        "    cfg_bench.batch_size = cfg.batch_size\n",
        "\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "    t0 = time.time()\n",
        "\n",
        "    for _ in range(n_batches):\n",
        "        train_epoch(net_bench, optimizer_bench, rb, cfg_bench, device, scaler=scaler_bench)\n",
        "\n",
        "    if device.type == \"cuda\":\n",
        "        torch.cuda.synchronize()\n",
        "    dt = time.time() - t0\n",
        "\n",
        "    total_samples = n_batches * max(1, len(rb) // cfg_bench.batch_size) * cfg_bench.batch_size\n",
        "    samples_per_sec = total_samples / max(dt, 1e-6)\n",
        "    print(f\"[Benchmark train] {samples_per_sec:.1f} samples/s | tiempo={dt:.2f}s | batch={cfg.batch_size} | AMP={getattr(cfg, 'use_amp', True)}\")\n",
        "\n",
        "\n",
        "# Uso sugerido (opcional):\n",
        "# benchmark_train_epoch(config, n_batches=40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a46b1d40",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 10: EJECUTAR ENTRENAMIENTO\n",
        "# ============================================================\n",
        "# Esta celda lanza el entrenamiento completo.\n",
        "# Se puede interrumpir y re-ejecutar: los checkpoints se guardan en save_dir (Drive/workspace).\n",
        "\n",
        "best_model = train_alphasnake(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4424c52",
      "metadata": {},
      "source": [
        "## Evaluacion Final y Exportacion ONNX\n",
        "\n",
        "Evaluar el mejor modelo con MCTS completo y exportar a ONNX para usar en el navegador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb3eb272",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 11: Evaluacion Final con MCTS\n",
        "# ============================================================\n",
        "# Cargar el mejor modelo (en caso de re-ejecucion)\n",
        "best_model_path = os.path.join(config.save_dir, 'best_model.pt')\n",
        "if os.path.exists(best_model_path):\n",
        "    best_model = AlphaSnakeNet(4, config.net_blocks, config.net_channels, config.board_size).to(device)\n",
        "    best_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "    print(\"Mejor modelo cargado desde save_dir.\")\n",
        "\n",
        "# Evaluacion completa con MCTS\n",
        "print(\"\\nEvaluacion final con MCTS (esto puede tardar)...\")\n",
        "eval_cfg = Config(\n",
        "    board_size=config.board_size,\n",
        "    max_steps=config.max_steps,\n",
        "    num_simulations=config.num_simulations,\n",
        "    c_puct=config.c_puct,\n",
        "    dir_alpha=config.dir_alpha,\n",
        "    dir_eps=0.0,  # Sin noise en evaluacion\n",
        "    food_samples=config.food_samples,\n",
        ")\n",
        "\n",
        "wr, avg_score, scores = evaluate_mcts(best_model, eval_cfg, device, n_games=100)\n",
        "print(f\"\\nResultados finales ({100} juegos con MCTS):\")\n",
        "print(f\"  Win rate: {wr:.3f} ({int(wr*100)}/100)\")\n",
        "print(f\"  Avg score: {avg_score:.1f} / {config.board_size**2 - 3}\")\n",
        "print(f\"  Min score: {min(scores)}, Max score: {max(scores)}\")\n",
        "print(f\"  Median score: {np.median(scores):.0f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4403abbc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 12: Exportar a ONNX\n",
        "# ============================================================\n",
        "import onnx\n",
        "\n",
        "best_model.eval()\n",
        "best_model.cpu()\n",
        "\n",
        "# Dummy input: (batch=1, channels=4, h=10, w=10)\n",
        "dummy_input = torch.randn(1, 4, config.board_size, config.board_size)\n",
        "\n",
        "# Path: un solo archivo en save_dir (Colab, Vast.ai y local)\n",
        "onnx_path = os.path.join(config.save_dir, 'alphasnake.onnx')\n",
        "\n",
        "# Exportar\n",
        "torch.onnx.export(\n",
        "    best_model,\n",
        "    dummy_input,\n",
        "    onnx_path,\n",
        "    export_params=True,\n",
        "    opset_version=13,\n",
        "    do_constant_folding=True,\n",
        "    input_names=['state'],\n",
        "    output_names=['policy', 'value'],\n",
        "    dynamic_axes={\n",
        "        'state': {0: 'batch_size'},\n",
        "        'policy': {0: 'batch_size'},\n",
        "        'value': {0: 'batch_size'},\n",
        "    }\n",
        ")\n",
        "\n",
        "# Verificar\n",
        "model_onnx = onnx.load(onnx_path)\n",
        "onnx.checker.check_model(model_onnx)\n",
        "print(f\"Modelo ONNX exportado y verificado: {onnx_path}\")\n",
        "\n",
        "# Verificar que las salidas coincidan\n",
        "import onnxruntime as ort\n",
        "\n",
        "ort_session = ort.InferenceSession(onnx_path)\n",
        "test_input = np.random.randn(1, 4, 10, 10).astype(np.float32)\n",
        "\n",
        "# PyTorch\n",
        "best_model.cpu()\n",
        "with torch.no_grad():\n",
        "    pt_policy, pt_value = best_model(torch.from_numpy(test_input))\n",
        "pt_policy = pt_policy.numpy()\n",
        "pt_value = pt_value.numpy()\n",
        "\n",
        "# ONNX\n",
        "ort_inputs = {'state': test_input}\n",
        "ort_policy, ort_value = ort_session.run(None, ort_inputs)\n",
        "\n",
        "print(f\"\\nVerificacion PyTorch vs ONNX:\")\n",
        "print(f\"  Policy diff max: {np.abs(pt_policy - ort_policy).max():.8f}\")\n",
        "print(f\"  Value diff max:  {np.abs(pt_value - ort_value).max():.8f}\")\n",
        "\n",
        "# Tamanio\n",
        "size_mb = os.path.getsize(onnx_path) / (1024 * 1024)\n",
        "print(f\"Tamanio del modelo: {size_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bfe4a5e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 13: Ubicaci\u00f3n del modelo ONNX\n",
        "# ============================================================\n",
        "# El modelo est\u00e1 en config.save_dir. En Colab descomenta: from google.colab import files; files.download(onnx_path)\n",
        "# En Vast.ai: descarga por Jupyter (clic derecho en alphasnake.onnx) o por scp.\n",
        "print(f\"Modelo ONNX guardado en: {onnx_path}\")\n",
        "print(\"Descarga iniciada. Coloca el archivo alphasnake.onnx en games/snake/ai/\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
